{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load and BIO label\n",
        "\n",
        "This script loads the data and its labels from given paths/places (see configs.ini file), joins the data with BIO labeling information, and saves the resulting file(s) to respective subfolder in the folder *_data*. You do not need a gpu to do preprocessing of data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install missing libraries if any\n",
        "import sys\n",
        "!{sys.executable} -m pip install dask[dataframe] pandas pyarrow seaborn spacy nltk"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: pandas in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (2.2.2)\nRequirement already satisfied: pyarrow in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (16.1.0)\nCollecting seaborn\n  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\nCollecting spacy\n  Downloading spacy-3.7.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\nCollecting nltk\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nCollecting dask[dataframe]\n  Downloading dask-2024.6.2-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: click>=8.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from dask[dataframe]) (8.1.7)\nRequirement already satisfied: cloudpickle>=1.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from dask[dataframe]) (2.2.1)\nRequirement already satisfied: fsspec>=2021.09.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from dask[dataframe]) (2024.5.0)\nRequirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from dask[dataframe]) (24.0)\nCollecting partd>=1.2.0 (from dask[dataframe])\n  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: pyyaml>=5.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from dask[dataframe]) (6.0.1)\nCollecting toolz>=0.10.0 (from dask[dataframe])\n  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: importlib-metadata>=4.13.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from dask[dataframe]) (7.1.0)\nCollecting dask-expr<1.2,>=1.1 (from dask[dataframe])\n  Downloading dask_expr-1.1.6-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: numpy>=1.22.4 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from pandas) (1.23.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from pandas) (2.9.0)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from pandas) (2024.1)\nRequirement already satisfied: matplotlib!=3.6.1,>=3.4 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from seaborn) (3.9.0)\nCollecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\nCollecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\nCollecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n  Downloading murmurhash-1.0.10-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\nCollecting cymem<2.1.0,>=2.0.2 (from spacy)\n  Downloading cymem-2.0.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\nCollecting preshed<3.1.0,>=3.0.2 (from spacy)\n  Downloading preshed-3.0.9-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\nCollecting thinc<8.3.0,>=8.2.2 (from spacy)\n  Downloading thinc-8.2.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\nCollecting wasabi<1.2.0,>=0.9.1 (from spacy)\n  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\nCollecting srsly<3.0.0,>=2.4.3 (from spacy)\n  Downloading srsly-2.4.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nCollecting catalogue<2.1.0,>=2.0.6 (from spacy)\n  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\nCollecting weasel<0.5.0,>=0.1.0 (from spacy)\n  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\nCollecting typer<1.0.0,>=0.3.0 (from spacy)\n  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from spacy) (4.66.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from spacy) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from spacy) (2.7.3)\nRequirement already satisfied: jinja2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from spacy) (3.1.4)\nRequirement already satisfied: setuptools in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from spacy) (68.0.0)\nCollecting langcodes<4.0.0,>=3.2.0 (from spacy)\n  Downloading langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: joblib in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from nltk) (2024.5.15)\nRequirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from importlib-metadata>=4.13.0->dask[dataframe]) (3.19.2)\nCollecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\nRequirement already satisfied: pillow>=8 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\nRequirement already satisfied: importlib-resources>=3.2.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.4.0)\nCollecting locket (from partd>=1.2.0->dask[dataframe])\n  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: annotated-types>=0.4.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\nRequirement already satisfied: pydantic-core==2.18.4 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.4)\nRequirement already satisfied: typing-extensions>=4.6.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\nCollecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n  Downloading blis-0.7.11-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\nCollecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\nCollecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\nCollecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\nCollecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n  Downloading cloudpathlib-0.18.1-py3-none-any.whl.metadata (14 kB)\nCollecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n  Downloading smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from jinja2->spacy) (2.1.5)\nCollecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n  Downloading marisa_trie-1.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\nCollecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\nRequirement already satisfied: wrapt in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\nCollecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\nDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading spacy-3.7.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\nDownloading cymem-2.0.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dask_expr-1.1.6-py3-none-any.whl (206 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dask-2024.6.2-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.0/182.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading murmurhash-1.0.10-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\nDownloading partd-1.4.2-py3-none-any.whl (18 kB)\nDownloading preshed-3.0.9-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.5/157.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\nDownloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\nDownloading srsly-2.4.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.4/492.4 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading thinc-8.2.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (937 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m937.8/937.8 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading toolz-0.12.1-py3-none-any.whl (56 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading typer-0.12.3-py3-none-any.whl (47 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading wasabi-1.1.3-py3-none-any.whl (27 kB)\nDownloading weasel-0.4.1-py3-none-any.whl (50 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading blis-0.7.11-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading cloudpathlib-0.18.1-py3-none-any.whl (47 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\nDownloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\nDownloading smart_open-7.0.4-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\nDownloading marisa_trie-1.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nInstalling collected packages: cymem, wasabi, toolz, spacy-loggers, spacy-legacy, smart-open, shellingham, nltk, murmurhash, mdurl, marisa-trie, locket, cloudpathlib, catalogue, blis, srsly, preshed, partd, markdown-it-py, language-data, seaborn, rich, langcodes, dask, confection, typer, thinc, dask-expr, weasel, spacy\nSuccessfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.18.1 confection-0.1.5 cymem-2.0.8 dask-2024.6.2 dask-expr-1.1.6 langcodes-3.4.0 language-data-1.2.0 locket-1.0.0 marisa-trie-1.2.0 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.10 nltk-3.8.1 partd-1.4.2 preshed-3.0.9 rich-13.7.1 seaborn-0.13.2 shellingham-1.5.4 smart-open-7.0.4 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 toolz-0.12.1 typer-0.12.3 wasabi-1.1.3 weasel-0.4.1\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1719996149862
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"hello worls!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "hello worls!\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996150105
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import ast # for converting list as a string to a list\n",
        "# azureml-core of version 1.0.72 or higher is required\n",
        "# azureml-dataprep[pandas] of version 1.1.34 or higher is required\n",
        "from azureml.core import Workspace, Dataset\n",
        "import configparser\n",
        "config = configparser.ConfigParser() #init\n",
        "config.read('../configs.ini') # init config with values from configs.ini\n",
        "import hashlib\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "# nltk.download('punkt')\n",
        "import numpy as np\n",
        "import os\n",
        "print(os.getcwd()) # show current \"local\" folder location\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "nlp = spacy.blank('fi') # init spacy with Finnish\n",
        "import string\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/mnt/batch/tasks/shared/LS_root/mounts/clusters/ext131165225/code/Users/EXT13116522/base_ner_refactored/scripts_to_preprocess_data\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996171334
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VARIABLES\n",
        "# experiment of interest (experiment name must match the section name in configs.ini)\n",
        "#experiment = 'loneliness'\n",
        "#experiment = 'incontinence_v5'\n",
        "#experiment = 'feeding_20240515'\n",
        "#experiment = 'mobility_v5'\n",
        "#experiment = 'mobility_v5.C.original'\n",
        "#experiment = 'Falling_NER_v3_20231114'# DONE\n",
        "#experiment = 'Mobility_2404_20240619'\n",
        "#experiment = 'Loneliness_beta0_20231123'\n",
        "experiment = 'Incontinence_NER_v5_20231208'\n",
        "\n",
        "# CONSTANTS from config\n",
        "# to save data\n",
        "data_folder = config[experiment]['data_folder'] # must exist\n",
        "data_subfolder = config[experiment]['data_subfolder'] # must exist\n",
        "journals_subfolder = config[experiment]['journals_subfolder'] # the key should exist\n",
        "\n",
        "# path to save folder\n",
        "data_save_folder_path = os.path.join(data_folder, data_subfolder)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996171609
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('data_save_folder_path', data_save_folder_path)\n",
        "print('journals_subfolder', journals_subfolder)\n",
        "print('type', type(journals_subfolder))\n",
        "print('length', len(journals_subfolder))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "data_save_folder_path ../_data/data_Incontinence_NER_v5_20231208\njournals_subfolder incontinence_journals\ntype <class 'str'>\nlength 21\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996171837
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main code"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load labels and journals data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You might encounter UserErrorException\n",
        "#\"\"\"UserErrorException: UserErrorException:Message: You are currently logged-in to \n",
        "# <tenant_id> tenant. You don't have access to <supscription_id> subscription, please check \n",
        "# if it is in this tenant. All the subscriptions that you have access to in this tenant are...\"\"\"\n",
        "# \n",
        "# azureml-core of version 1.0.72 or higher is required\n",
        "# azureml-dataprep[pandas] of version 1.1.34 or higher is required\n",
        "\n",
        "workspace = Workspace(\n",
        "    config['default']['SUBSCRIPTION_ID'], \n",
        "    config['default']['RESOURCE_GROUP'], \n",
        "    config['default']['WORKSPACE_NAME'])"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996172128
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "dataset = Dataset.get_by_name(workspace, name=config[experiment]['data_name'])\n",
        "# save dataset to a TEMPORARY folder (if folder already exists and overwrite is False then throws error)\n",
        "temporary_folder_path = data_save_folder_path + '_tmp'\n",
        "try:\n",
        "    dataset.download(target_path=temporary_folder_path, overwrite=False)\n",
        "#except UserErrorException as e: #UserErrorException is not in this environment? \n",
        "except Exception as e:\n",
        "    print(f\"Please, remove the temporary folder from the folder structure manually. It is called {temporary_folder_path}.\")\n",
        "    print(\"#------------------#\")\n",
        "    print(f\"Encountered an exception. Error was: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'infer_column_types': 'False', 'activity': 'download'}\n{'infer_column_types': 'False', 'activity': 'download', 'activityApp': 'FileDataset'}\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996215116
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check contents of the tmp folder\n",
        "hasFolder=False\n",
        "dataset_directory_name = None\n",
        "for root, dirs, files in os.walk(temporary_folder_path, topdown=False):\n",
        "    if len(dirs) == 1: # is there a subfolder?\n",
        "        print(\"# we got a folder!\")\n",
        "        dataset_directory_name = dirs[0]\n",
        "        hasFolder = True\n",
        "    else: pass\n",
        "\n",
        "# Did we fail to get the subfolder name?\n",
        "if hasFolder is False and dataset_directory_name is None:\n",
        "    print(\"There was no folder!\")\n",
        "    # we get the folder name from configuration\n",
        "    dataset_directory_name = journals_subfolder\n",
        "else: pass\n",
        "\n",
        "print('dataset_directory_name', dataset_directory_name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "There was no folder!\ndataset_directory_name incontinence_journals\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996215506
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_non_empty_folder(top):\n",
        "    \"\"\"Remove folder based on the path\"\"\"\n",
        "    for root, dirs, files in os.walk(top, topdown=False):\n",
        "        for name in files: # delete files\n",
        "            os.remove(os.path.join(root, name))\n",
        "        for name in dirs: # delete directories\n",
        "            os.rmdir(os.path.join(root, name))\n",
        "    if os.path.exists(top):\n",
        "        # delete the empty tmp folder\n",
        "        os.removedirs(top)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996215788
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if there is a subfolder with some name\n",
        "if hasFolder and dataset_directory_name is not None:\n",
        "    dataset.download(target_path=data_save_folder_path, overwrite=True)\n",
        "    print(f\"downloaded to: {data_save_folder_path}\")\n",
        "    # delete tmp folder    \n",
        "    delete_non_empty_folder(os.path.join(temporary_folder_path, dataset_directory_name)) # delete files from folder\n",
        "    print(f\"deleted tmp folder: {os.path.join(temporary_folder_path, dataset_directory_name)}\")\n",
        "    delete_non_empty_folder(temporary_folder_path) # delete any files outside the subfolder (tmp folder)\n",
        "    print(f\"deleted tmp folder: {os.path.join(temporary_folder_path)}\")\n",
        "else: \n",
        "    print(f\"HasFolder was {hasFolder} and dataset_directory_name was {dataset_directory_name}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "HasFolder was False and dataset_directory_name was incontinence_journals\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996216065
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if hasFolder is False and dataset_directory_name is not None:\n",
        "    # we got files without a folder, thus all journal \n",
        "    # files must be moved to specified subfolder.\n",
        "    for root, dirs, files in os.walk(temporary_folder_path, topdown=False):\n",
        "        # fetch the files again\n",
        "        if len(dirs) == 0: # there should be no folders, instead many files\n",
        "            print(f\"we got files without a folder in {temporary_folder_path}\")\n",
        "            # select journal related files by excluding possible label file.\n",
        "            files = list(filter(lambda x: 'raw' not in x.lower(), files))\n",
        "            print(f\"We got {len(files)} files.\")\n",
        "\n",
        "            # we've got the folder name from configuration previously\n",
        "            print('dataset_directory_name', dataset_directory_name)\n",
        "\n",
        "            # destination path\n",
        "            destination_folder = os.path.join(data_save_folder_path, dataset_directory_name)\n",
        "            print('destination_folder', destination_folder)\n",
        "\n",
        "            # create destination folder\n",
        "            if not os.path.exists(destination_folder):\n",
        "                os.makedirs(destination_folder) # create an empty folder\n",
        "            else:\n",
        "                # the folder already exists\n",
        "                print(\"Destination folder exists. Re-creating it!\")\n",
        "                delete_non_empty_folder(destination_folder) # delete the folder\n",
        "                os.makedirs(destination_folder) # create an empty folder\n",
        "\n",
        "            # move files from tmp folder to target folder with os library\n",
        "            for item in files:\n",
        "                # save journals to subfolder within the specified target folder \n",
        "                os.rename(\n",
        "                    os.path.join(root, item),\n",
        "                    os.path.join(destination_folder, item))\n",
        "            print(\"Files copied to final data folder\")\n",
        "\n",
        "            # remove the now unnecessary tmp folder\n",
        "            delete_non_empty_folder(temporary_folder_path)\n",
        "            print(\"DONE\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "we got files without a folder in ../_data/data_Incontinence_NER_v5_20231208_tmp\nWe got 3000 files.\ndataset_directory_name incontinence_journals\ndestination_folder ../_data/data_Incontinence_NER_v5_20231208/incontinence_journals\nFiles copied to final data folder\nDONE\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996434573
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load labels\n",
        "dataset = Dataset.get_by_name(workspace, name=config[experiment]['labels_name'])\n",
        "df = dataset.to_pandas_dataframe()\n",
        "# This method is deprecated and will no longer be supported.\n",
        "# Create a TabularDataset by calling the static methods on Dataset.Tabular and use the to_pandas_dataframe method there. For more information, see https://aka.ms/dataset-deprecation."
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'infer_column_types': 'False', 'activity': 'to_pandas_dataframe'}\n{'infer_column_types': 'False', 'activity': 'to_pandas_dataframe', 'activityApp': 'TabularDataset'}\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996435137
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save raw label data to file\n",
        "df.to_csv(os.path.join(data_save_folder_path, 'raw_LABELINGS.csv'), sep='|', encoding='utf-16le', index=False)"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996435571
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove references that are no longer needed in order to manage memory\n",
        "del df\n",
        "del dataset\n",
        "del workspace"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996435830
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process raw journals\n",
        "\n",
        "#### Labels"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_df = pd.read_csv(\n",
        "    os.path.join(data_save_folder_path, 'raw_LABELINGS.csv'), \n",
        "    sep='|', \n",
        "    encoding='utf-16le')\n",
        "labels_df.head(0)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "Empty DataFrame\nColumns: [image_url, label, label_confidence, labeler, updated_by, labeling_time_in_seconds, label_creation_time]\nIndex: []",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_url</th>\n      <th>label</th>\n      <th>label_confidence</th>\n      <th>labeler</th>\n      <th>updated_by</th>\n      <th>labeling_time_in_seconds</th>\n      <th>label_creation_time</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996436117
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_short_filename(url:str) -> str:\n",
        "    \"\"\"extract filename from url\"\"\"\n",
        "    filename = url.split('/')[-1]\n",
        "    return filename\n",
        "\n",
        "labels_df['short_filename'] = list(map(lambda x: get_short_filename(x), labels_df['image_url']))"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996436368
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the data frame to the local folder (overwrites the original file)\n",
        "#labels_df.to_csv(os.path.join(data_save_folder_path, 'raw_LABELINGS.csv'), sep='|', encoding='utf-16le', index=False)"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996436739
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def md5hash(input_string):\n",
        "    result = hashlib.md5(input_string.encode())\n",
        "    return result.hexdigest()\n",
        "\n",
        "\n",
        "def extract_filename_from_filepath(url:str) -> str:\n",
        "    # filename is the last of the string\n",
        "    return url.split('/')[-1]\n",
        "\n",
        "\n",
        "def get_start_index(text_id:str) -> str:\n",
        "    return text_id.split('_')[-2]\n",
        "\n",
        "\n",
        "def get_end_index(text_id:str) -> str:\n",
        "    end_value = text_id.split('_')[-1]\n",
        "    return end_value.split('.')[0]"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996437009
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_save_folder_path, dataset_directory_name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "../_data/data_Incontinence_NER_v5_20231208 incontinence_journals\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996437241
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the journal files' paths\n",
        "journal_filenames = []\n",
        "for root, dirs, files in os.walk(os.path.join(data_save_folder_path, dataset_directory_name), topdown=False):\n",
        "   for name in files:\n",
        "      journal_filenames.append(os.path.join(root, name))\n",
        "\n",
        "# filter out files that are not journals (only text files accepted)\n",
        "journal_filenames = list(filter(lambda x: x.endswith('.txt'), journal_filenames))\n",
        "\n",
        "# form tuples with path and file name\n",
        "names_ids = [(journal_filename, extract_filename_from_filepath(journal_filename)) for journal_filename in journal_filenames]"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996437502
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# collect the nice data into this data frame\n",
        "journals_df = pd.DataFrame(data=[], columns=['excerpt', 'filepath', 'text_id', 'start_index', 'end_index'])\n",
        "\n",
        "# iterate the filenames and the content of the files \n",
        "objs = []\n",
        "for name_id in names_ids:\n",
        "    with open(name_id[0]) as f_in:\n",
        "        text = f_in.readline()\n",
        "        obj = {}\n",
        "        obj['excerpt'] = text\n",
        "        obj['filepath'] = name_id[0]\n",
        "        file_name = str(name_id[1])\n",
        "        obj['text_id'] = file_name.split('_')[0]\n",
        "        obj['start_index'] = get_start_index(file_name)\n",
        "        obj['end_index'] = get_end_index(file_name)\n",
        "        obj['short_filename'] = name_id[1]\n",
        "        objs.append(obj)\n",
        "\n",
        "# add data to the data frame\n",
        "journals_df = pd.concat([journals_df, pd.DataFrame.from_dict(objs)], ignore_index=True)"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996516159
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# process excerpts by removing pipe ('|') characters\n",
        "journals_df['excerpt'] = list(map(lambda x: x.replace('|',';;;'), journals_df['excerpt']))"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996516462
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Write valid lines to compressed csv\n",
        "# journals_df.to_csv(\n",
        "#     os.path.join(data_save_folder_path,'tmp_TEXTS.csv'),\n",
        "#     encoding='utf-8',\n",
        "#     sep='|',\n",
        "#     quoting=0,\n",
        "#     header=True,\n",
        "#     index=False\n",
        "# )"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996516833
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"journals: {journals_df.shape[0]}, labels: {labels_df.shape[0]}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "journals: 3000, labels: 2561\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996517149
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Match the label records with journal records"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# merge data frames \n",
        "dataset = pd.merge(left=labels_df, right=journals_df, how='inner', on='short_filename')\n",
        "dataset.head(0) # display column headers"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "Empty DataFrame\nColumns: [image_url, label, label_confidence, labeler, updated_by, labeling_time_in_seconds, label_creation_time, short_filename, excerpt, filepath, text_id, start_index, end_index]\nIndex: []",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_url</th>\n      <th>label</th>\n      <th>label_confidence</th>\n      <th>labeler</th>\n      <th>updated_by</th>\n      <th>labeling_time_in_seconds</th>\n      <th>label_creation_time</th>\n      <th>short_filename</th>\n      <th>excerpt</th>\n      <th>filepath</th>\n      <th>text_id</th>\n      <th>start_index</th>\n      <th>end_index</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996517426
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.info() # display statistics of separate columns"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2561 entries, 0 to 2560\nData columns (total 13 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   image_url                 2561 non-null   object \n 1   label                     2561 non-null   object \n 2   label_confidence          2561 non-null   object \n 3   labeler                   2561 non-null   object \n 4   updated_by                349 non-null    object \n 5   labeling_time_in_seconds  2561 non-null   float64\n 6   label_creation_time       2561 non-null   object \n 7   short_filename            2561 non-null   object \n 8   excerpt                   2561 non-null   object \n 9   filepath                  2561 non-null   object \n 10  text_id                   2561 non-null   object \n 11  start_index               2561 non-null   object \n 12  end_index                 2561 non-null   object \ndtypes: float64(1), object(12)\nmemory usage: 260.2+ KB\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996517665
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Write valid lines to compressed csv\n",
        "# dataset.to_csv(\n",
        "#     'tmp_RECORDS.csv',\n",
        "#     encoding='utf-8',\n",
        "#     sep='|',\n",
        "#     quoting=0,\n",
        "#     header=True,\n",
        "#     index=False\n",
        "# )"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996517879
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove unnecessary dataframes for memory management\n",
        "del labels_df\n",
        "del journals_df"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996518101
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BIO labeling"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform semicolons back to pipes\n",
        "Previously we changed pipe character ('|') to three semicolons characters (';;;'). Now we replace them again with the pipe character."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['data'] = list(map(lambda x: x.replace(';;;', '|'), dataset['excerpt']))"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996518314
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add modified_labels and only_labels columns"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modified_labels(labels_array:str):\n",
        "    \"\"\"The modified labels are a list with single dictionary. \n",
        "    However, they seem to be strings in the dataset. Let's \n",
        "    transform label_array back to list and dictionary.\"\"\"\n",
        "    # There might be more than one label per each labels_array.\n",
        "    # Those are separated by new line character ('\\n'). We must \n",
        "    # replace '\\n' with comma (',') to match list format.\n",
        "    labels_array = labels_array.replace('\\n', ',')\n",
        "    # not the most secure thing to do but we expect not to \n",
        "    # encounter anything malicious in the labels\n",
        "    return ast.literal_eval(labels_array)\n",
        "\n",
        "# get modified labels\n",
        "dataset['modified_labels'] = list(map(lambda x: modified_labels(x), dataset['label']))"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996518546
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['only_labels'] = list(map(lambda x: [x[0]['label']], dataset['modified_labels']))\n",
        "\n",
        "dataset['only_labels'] # display"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/plain": "0          [Ongelmia]\n1       [Ei ongelmia]\n2          [Ongelmia]\n3          [Ongelmia]\n4          [Ongelmia]\n            ...      \n2556    [Ei ongelmia]\n2557       [Ongelmia]\n2558       [Ongelmia]\n2559       [Ongelmia]\n2560       [Ongelmia]\nName: only_labels, Length: 2561, dtype: object"
          },
          "metadata": {}
        }
      ],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996518821
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(set(list(map(lambda x: x[0], dataset['only_labels']))))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 32,
          "data": {
            "text/plain": "['Ei ongelmia', 'Ongelmia']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996519137
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make BIO labels\n",
        "\n",
        "BIO labels are tags for each word/term in text that indicate whether the term begins the labeled sentence or is in the middle of the labeled sentence or ends the labeled sentence.\n",
        "Here's an example of BIO labels:\n",
        "\n",
        "bio_label_list = [\n",
        "\"O\", \n",
        "\"B-LONELINESS_LONELY\",\n",
        "\"I-LONELINESS_LONELY\",\n",
        "\"B-LONELINESS_NOTLONELY\",\n",
        "\"I-LONELINESS_NOTLONELY\",\n",
        "] where \n",
        "\n",
        "'O' indicates a word or term that is not significant to the labeling of the piece of text. Most of the terms and words in text excerpts are marked as such. 'B' marks the beginning and 'I' marks any term intermitted in the piece of sentence that labeler has deemed to be enough to signify that the sentence matches the given label. We have not used the ending tag. In addition to the indicative starting character, the label is given (in cases 'B' and 'I') preseced by a dash character ('-'). Label itself can have underscore in its name as it has been used in this example. Please, do not deviate away from the labeling format \"<Indicative character\\>-<Dataset\\>\\_<specific label\\>\". \n",
        "\n",
        "The BIO labels get added to the text excerpts in this part of the code; however, instead of having them in the code I recommend that they are read from a configuration file specific to each dataset case (including different versions)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_BIO_label_list(experiment:str):\n",
        "    \"\"\"Get the BIO label list as a list from the configs.ini file for \n",
        "        the given experiment (experiment must match section name). Returns\n",
        "        BIO_label_list or throws an error.\"\"\"\n",
        "    if \"bio_label_list\" in config[experiment]:\n",
        "        # get config values as string\n",
        "        s:str = config[experiment][\"bio_label_list\"]\n",
        "        \n",
        "        # check length of s\n",
        "        if len(s) < 3:\n",
        "            raise ValueError(\"bio_label_list in config file is too short.\")\n",
        "        else:pass\n",
        "\n",
        "        # remove starting and closing brackets\n",
        "        if s.startswith('['):\n",
        "            s = s[1:]\n",
        "        else:\n",
        "            pass\n",
        "        if s.endswith(']'):\n",
        "            s = s[:-1]\n",
        "        else: pass\n",
        "\n",
        "        # we use the values of bio_label_list variable in section \"BIO labeling\" in this file\n",
        "        bio_label_list = None\n",
        "        if ',' in s:\n",
        "            # split text and remove extra spaces from values\n",
        "            s = s.split(',')\n",
        "            bio_label_list = [x.replace(' ', '') for x in s]\n",
        "        \n",
        "        return bio_label_list\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Could not find {experiment} in config\")\n",
        "\n",
        "\n",
        "bio_label_list = get_BIO_label_list(experiment)\n",
        "print(bio_label_list)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['O', 'B-Ongelmia', 'I-Ongelmia', 'B-Eiongelmia', 'I-Eiongelmia', '']\n"
        }
      ],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996519387
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text):\n",
        "    # The regular expression matches word characters or punctuation marks\n",
        "    return re.findall(r'\\w+|[-.,!?;]', text)"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996519645
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation_list = set(string.punctuation)\n",
        "def bio_tagging(text, labeled_data):\n",
        "\n",
        "    words = split_text(text)\n",
        "\n",
        "    \n",
        "    bio_tag_list = []\n",
        "    dict_labeled_data = {}\n",
        "    label_array = []\n",
        "    for label_data in labeled_data:\n",
        "        \n",
        "        if label_data[\"label\"] != 'O':\n",
        "            \n",
        "            #label = mapping_labels_to_english[label_data[\"label\"]]\n",
        "            label = label_data[\"label\"]\n",
        "            label_array.append(label)\n",
        "\n",
        "            offsetStart = label_data[\"offsetStart\"]\n",
        "            offsetEnd = label_data[\"offsetEnd\"]\n",
        "            sample_text = text[offsetStart:offsetEnd]\n",
        "            words_sample = split_text(sample_text)\n",
        "\n",
        "\n",
        "            for i, word_sample in enumerate(words_sample):\n",
        "                if word_sample not in punctuation_list: #if word len is < 3, omit\n",
        "                    if i == 0:\n",
        "                        dict_labeled_data[word_sample + \"_\" + label] = 'B-' + label   \n",
        "                    else:\n",
        "                        dict_labeled_data[word_sample + \"_\" + label] = 'I-' + label\n",
        "\n",
        "\n",
        "    bio_tag = {}\n",
        "    bio_tag_list = []\n",
        "    \n",
        "    for j, word in enumerate(words):\n",
        "        mapped = False\n",
        "        for label in label_array:\n",
        "\n",
        "            if (word + \"_\" + label) in dict_labeled_data and dict_labeled_data[word + \"_\" + label]!=\"used\":\n",
        "                bio_tag_list.append(dict_labeled_data[word + \"_\" + label])\n",
        "                bio_tag[word] = dict_labeled_data[word + \"_\" + label]\n",
        "                dict_labeled_data[word + \"_\" + label] = \"used\"\n",
        "\n",
        "                mapped = True\n",
        "            \n",
        "            if mapped:\n",
        "                break\n",
        "\n",
        "        if not mapped:\n",
        "            bio_tag[word] = \"O\"\n",
        "            bio_tag_list.append(\"O\")\n",
        "\n",
        "\n",
        "    return bio_tag_list, words\n"
      ],
      "outputs": [],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996519918
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[[\"bi_tags\", \"words\"]] = dataset.apply(\n",
        "    lambda row: pd.Series(bio_tagging(row['data'], row['modified_labels'])),\n",
        "    axis=1\n",
        ")\n",
        "df = dataset.copy()\n",
        "# Words is a list of the excerpt text separated by each word/punctuation. For example, ['Hello', ',', 'world', '!', 'How', 'are', 'you', '?']\n",
        "# Bi_tags is a list of items found in bio_label_list matching the words list. For example, [O,O,O,O,O,O,O,O].\n",
        "#print(df[[\"words\", \"bi_tags\"]].head(10))"
      ],
      "outputs": [],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996520361
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_bio_labeling(data, i):\n",
        "    print(data[\"modified_labels\"][i])\n",
        "    print(data[\"words\"][i])\n",
        "    print(data[\"bi_tags\"][i])\n",
        "\n",
        "# a quick test to see whats in some record for the tags\n",
        "#test_bio_labeling(df, 100)"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996520645
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns.values.tolist()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 38,
          "data": {
            "text/plain": "['image_url',\n 'label',\n 'label_confidence',\n 'labeler',\n 'updated_by',\n 'labeling_time_in_seconds',\n 'label_creation_time',\n 'short_filename',\n 'excerpt',\n 'filepath',\n 'text_id',\n 'start_index',\n 'end_index',\n 'data',\n 'modified_labels',\n 'only_labels',\n 'bi_tags',\n 'words']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 38,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996520921
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Saving the BIO labeled data to file(s)\n",
        "#Rename file to match the context of the data!\n",
        "\n",
        "# Write valid lines to parquet\n",
        "df.to_parquet(\n",
        "    os.path.join(data_save_folder_path, 'tmp_BIO_labeled_data.parquet'),\n",
        "    index=True,\n",
        "    use_dictionary=False\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 39,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719996521236
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.19",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}