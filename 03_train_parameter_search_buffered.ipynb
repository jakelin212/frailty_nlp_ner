{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install nervaluate transformers[torch] datasets evaluate seqeval torch jupyter ipywidgets"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: nervaluate in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (0.2.0)\nRequirement already satisfied: datasets in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (2.20.0)\nRequirement already satisfied: evaluate in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (0.4.2)\nRequirement already satisfied: seqeval in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: torch in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (2.3.1)\nRequirement already satisfied: jupyter in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (1.0.0)\nRequirement already satisfied: ipywidgets in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (8.1.3)\nRequirement already satisfied: transformers[torch] in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers[torch]) (3.14.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers[torch]) (0.23.4)\nRequirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers[torch]) (24.0)\nRequirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers[torch]) (2024.5.15)\nRequirement already satisfied: requests in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers[torch]) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers[torch]) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers[torch]) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers[torch]) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers[torch]) (0.31.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2023.10.0)\nRequirement already satisfied: aiohttp in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: scikit-learn>=0.21.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from seqeval) (1.5.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (1.12.1)\nRequirement already satisfied: networkx in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: triton==2.3.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (2.3.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\nRequirement already satisfied: notebook in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyter) (7.2.1)\nRequirement already satisfied: qtconsole in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyter) (5.5.2)\nRequirement already satisfied: jupyter-console in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyter) (6.6.3)\nRequirement already satisfied: nbconvert in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyter) (7.16.4)\nRequirement already satisfied: ipykernel in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyter) (6.29.4)\nRequirement already satisfied: comm>=0.1.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\nRequirement already satisfied: ipython>=6.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipywidgets) (8.25.0)\nRequirement already satisfied: traitlets>=4.3.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\nRequirement already satisfied: widgetsnbextension~=4.0.11 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipywidgets) (4.0.11)\nRequirement already satisfied: jupyterlab-widgets~=3.0.11 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipywidgets) (3.0.11)\nRequirement already satisfied: psutil in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\nRequirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: decorator in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.46)\nRequirement already satisfied: pygments>=2.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\nRequirement already satisfied: stack-data in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\nRequirement already satisfied: exceptiongroup in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers[torch]) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers[torch]) (2.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.6.2)\nRequirement already satisfied: scipy>=1.6.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\nRequirement already satisfied: joblib>=1.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\nRequirement already satisfied: debugpy>=1.6.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipykernel->jupyter) (1.6.7)\nRequirement already satisfied: jupyter-client>=6.1.12 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipykernel->jupyter) (8.6.2)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipykernel->jupyter) (5.7.2)\nRequirement already satisfied: nest-asyncio in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipykernel->jupyter) (1.6.0)\nRequirement already satisfied: pyzmq>=24 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipykernel->jupyter) (25.1.2)\nRequirement already satisfied: tornado>=6.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipykernel->jupyter) (6.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: beautifulsoup4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nbconvert->jupyter) (4.12.3)\nRequirement already satisfied: bleach!=5.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nbconvert->jupyter) (6.1.0)\nRequirement already satisfied: defusedxml in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nbconvert->jupyter) (0.7.1)\nRequirement already satisfied: jupyterlab-pygments in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nbconvert->jupyter) (0.3.0)\nRequirement already satisfied: mistune<4,>=2.0.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nbconvert->jupyter) (3.0.2)\nRequirement already satisfied: nbclient>=0.5.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nbconvert->jupyter) (0.10.0)\nRequirement already satisfied: nbformat>=5.7 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nbconvert->jupyter) (5.10.4)\nRequirement already satisfied: pandocfilters>=1.4.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nbconvert->jupyter) (1.5.1)\nRequirement already satisfied: tinycss2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nbconvert->jupyter) (1.3.0)\nRequirement already satisfied: jupyter-server<3,>=2.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from notebook->jupyter) (2.14.1)\nRequirement already satisfied: jupyterlab-server<3,>=2.27.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from notebook->jupyter) (2.27.2)\nRequirement already satisfied: jupyterlab<4.3,>=4.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from notebook->jupyter) (4.2.3)\nRequirement already satisfied: notebook-shim<0.3,>=0.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from notebook->jupyter) (0.2.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: qtpy>=2.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from qtconsole->jupyter) (2.4.1)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.9.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert->jupyter) (1.16.0)\nRequirement already satisfied: webencodings in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert->jupyter) (0.5.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\nRequirement already satisfied: platformdirs>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\nRequirement already satisfied: anyio>=3.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (4.4.0)\nRequirement already satisfied: argon2-cffi>=21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (23.1.0)\nRequirement already satisfied: jupyter-events>=0.9.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (0.10.0)\nRequirement already satisfied: jupyter-server-terminals>=0.4.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (0.5.3)\nRequirement already satisfied: overrides>=5.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (7.7.0)\nRequirement already satisfied: prometheus-client>=0.9 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (0.20.0)\nRequirement already satisfied: send2trash>=1.8.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (1.8.3)\nRequirement already satisfied: terminado>=0.8.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (0.18.1)\nRequirement already satisfied: websocket-client>=1.7 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (1.8.0)\nRequirement already satisfied: async-lru>=1.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyterlab<4.3,>=4.2.0->notebook->jupyter) (2.0.4)\nRequirement already satisfied: httpx>=0.25.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyterlab<4.3,>=4.2.0->notebook->jupyter) (0.27.0)\nRequirement already satisfied: jupyter-lsp>=2.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyterlab<4.3,>=4.2.0->notebook->jupyter) (2.2.5)\nRequirement already satisfied: setuptools>=40.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyterlab<4.3,>=4.2.0->notebook->jupyter) (69.5.1)\nRequirement already satisfied: tomli>=1.2.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyterlab<4.3,>=4.2.0->notebook->jupyter) (2.0.1)\nRequirement already satisfied: babel>=2.10 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->notebook->jupyter) (2.15.0)\nRequirement already satisfied: json5>=0.9.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->notebook->jupyter) (0.9.25)\nRequirement already satisfied: jsonschema>=4.18.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->notebook->jupyter) (4.22.0)\nRequirement already satisfied: fastjsonschema>=2.15 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert->jupyter) (2.20.0)\nRequirement already satisfied: ptyprocess>=0.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\nRequirement already satisfied: soupsieve>1.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter) (2.5)\nRequirement already satisfied: executing>=1.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\nRequirement already satisfied: pure-eval in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\nRequirement already satisfied: sniffio>=1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.3.1)\nRequirement already satisfied: argon2-cffi-bindings in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter) (21.2.0)\nRequirement already satisfied: httpcore==1.* in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook->jupyter) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook->jupyter) (0.14.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->jupyter) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->jupyter) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->jupyter) (0.18.1)\nRequirement already satisfied: python-json-logger>=2.0.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (2.0.7)\nRequirement already satisfied: rfc3339-validator in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (0.1.4)\nRequirement already satisfied: rfc3986-validator>=0.1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (0.1.1)\nRequirement already satisfied: fqdn in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.5.1)\nRequirement already satisfied: isoduration in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (20.11.0)\nRequirement already satisfied: jsonpointer>1.13 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (3.0.0)\nRequirement already satisfied: uri-template in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.3.0)\nRequirement already satisfied: webcolors>=1.11 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (24.6.0)\nRequirement already satisfied: cffi>=1.0.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.16.0)\nRequirement already satisfied: pycparser in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter) (2.22)\nRequirement already satisfied: arrow>=0.15.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.3.0)\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (2.9.0.20240316)\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313589906
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import configparser\n",
        "config = configparser.ConfigParser() #init\n",
        "config.read('../configs.ini') # init config with values from configs.ini\n",
        "from datetime import date\n",
        "import datetime\n",
        "import datasets\n",
        "from datasets import Dataset, DatasetDict\n",
        "from functools import partial\n",
        "import inspect\n",
        "import os\n",
        "import pandas as pd\n",
        "print(pd.__version__)\n",
        "import torch\n",
        "torch.cuda.empty_cache() # for memory management and especially to avoid OutOfMemoryError\n",
        "import torch.nn as nn\n",
        "from transformers import AutoConfig, AutoModel\n",
        "from transformers import TrainerCallback\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "import transformers\n",
        "from typing import List, Dict\n",
        "import uuid\n",
        "import logging\n",
        "from myLogging import get_logger, my_function_inputs_logger, pre_defined_function_inputs_logger\n",
        "from myModels import ExtraLastLayerModel, ModelWrapper\n",
        "from myUtilities import get_ner_map, read_convert_config_training_values, get_short_uuid, read_config_values_as_dict, extract_config_training_values\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2.2.2\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313595470
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VARIABLES\n",
        "# Only thing to change in this file (in ideal case)\n",
        "#experiment = 'loneliness'\n",
        "#experiment = 'incontinence_v5'\n",
        "#experiment = 'loneliness.03_train_with_comparison_parameters.original'\n",
        "#experiment = 'loneliness.C.original'\n",
        "#experiment = 'loneliness.03_train_with_comparison_parameters.extralayer'\n",
        "#experiment = 'mobility_v5.C.original'\n",
        "#experiment = 'mobility_v5.C.extralayer'\n",
        "\n",
        "#experiment = 'Falling_NER_v3_20231114_orig_par' # Done\n",
        "#experiment = 'Mobility_2404_20240619_orig_par' # Done\n",
        "#experiment = 'Loneliness_beta0_20231123_orig_par' #Done\n",
        "#experiment = 'Incontinence_NER_v5_20231208_orig_par' # Done\n",
        "\n",
        "#experiment = 'Falling_NER_v3_20231114_extra_par'\n",
        "#experiment = 'Mobility_2404_20240619_extra_par'\n",
        "#experiment = 'Loneliness_beta0_20231123_extra_par'\n",
        "#experiment = 'Incontinence_NER_v5_20231208_extra_par'\n",
        "\n",
        "#experiment = 'Falling_NER_v3_20231114_orig_str' # Done\n",
        "#experiment = 'Mobility_2404_20240619_orig_str' # Done\n",
        "#experiment = 'Loneliness_beta0_20231123_orig_str' #Done\n",
        "#experiment = 'Incontinence_NER_v5_20231208_orig_str' #Done\n",
        "\n",
        "#experiment = 'Falling_NER_v3_20231114_orig_par_opt' # Done\n",
        "#experiment = 'Mobility_2404_20240619_orig_par_opt' #Done\n",
        "#experiment = 'Loneliness_beta0_20231123_orig_par_opt' #Done??\n",
        "experiment = 'Incontinence_NER_v5_20231208_orig_par_opt'\n",
        "\n",
        "print(experiment)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Incontinence_NER_v5_20231208_orig_par_opt\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313595652
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = config[experiment]['experiment_name'] # dataset\n",
        "experiment_identifier = config[experiment]['experiment_file'] # training file\n",
        "experiment_model = config[experiment]['model'] # original or extralayer\n",
        "\n",
        "print('experiment_name', experiment_name)\n",
        "print('experiment_identifier', experiment_identifier)\n",
        "print('experiment_model', experiment_model)\n",
        "\n",
        "run_id = get_short_uuid() # get random uuid for each run or select it manually (for logger, model saving)\n",
        "dataset_identifier = experiment_name # for logger\n",
        "today = date.today()\n",
        "today = today.strftime(\"%Y_%m_%d\")\n",
        "\n",
        "# CONSTANTS from config\n",
        "# to read data from\n",
        "data_folder = config[experiment]['data_folder'] # must exist\n",
        "data_subfolder = config[experiment]['data_subfolder'] # must exist\n",
        "# path to DATA save folder\n",
        "data_save_folder_path = os.path.join(data_folder, data_subfolder)\n",
        "print(\"data_save_folder_path\", data_save_folder_path)\n",
        "\n",
        "# to read/write models\n",
        "model_folder = config[experiment]['model_folder'] # must exist\n",
        "model_subfolder = config[experiment]['model_subfolder'] # must exist\n",
        "# path to MODEL save folder\n",
        "model_save_folder_path = os.path.join(model_folder, model_subfolder)\n",
        "model_save_folder_path = os.path.join(model_save_folder_path, experiment_identifier)\n",
        "model_save_folder_path = os.path.join(model_save_folder_path, f\"{today}_{run_id}\")\n",
        "print(\"model_save_folder_path\", model_save_folder_path)\n",
        "\n",
        "logger = get_logger('train.log', run_id=run_id, experiment_identifier=experiment_identifier, dataset_identifier=dataset_identifier)\n",
        "logger.info(f\"\\n--------Starting training--------\\n\")\n",
        "logger.info(f\"\\nModel: {experiment_model}\\n\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "experiment_name Incontinence_NER_v5_20231208_orig_par_opt\nexperiment_identifier C\nexperiment_model original\ndata_save_folder_path ../_data/data_Incontinence_NER_v5_20231208\nmodel_save_folder_path ../_trained_models/Incontinence_NER_v5_20231208_orig_par_opt/C/2024_07_18_cbe7f211\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313595832
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"\\nTRAIN DATA, TEST DATA, and VAL DATA\\n\")\n",
        "train_data = pre_defined_function_inputs_logger(logger, pd.read_parquet, os.path.join(data_save_folder_path, \"train_data.parquet\"))\n",
        "test_data = pre_defined_function_inputs_logger(logger, pd.read_parquet, os.path.join(data_save_folder_path, \"test_data.parquet\"))\n",
        "val_data = pre_defined_function_inputs_logger(logger, pd.read_parquet, os.path.join(data_save_folder_path, \"val_data.parquet\"))\n",
        "\n",
        "test_data = test_data[test_data['words'].apply(len) <= 512]\n",
        "test_data.reset_index(drop=False, inplace=True)\n",
        "\n",
        "train_data = train_data[train_data['words'].apply(len) <= 512]\n",
        "train_data.reset_index(drop=False, inplace=True)\n",
        "\n",
        "val_data = val_data[val_data['words'].apply(len) <= 512]\n",
        "val_data.reset_index(drop=False, inplace=True)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313596034
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "(2048, 19)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313596241
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ner_map = get_ner_map(config, experiment) # from myUtilities\n",
        "ner_map = {'O': 0, 'B-Ongelmia': 1, 'I-Ongelmia': 2, 'B-Ei ongelmia': 3, 'I-Ei ongelmia': 4} # Only for incontinence!\n",
        "print(\"ner_map\", ner_map)\n",
        "logger.info(f\"NER MAP: {ner_map}\")\n",
        "\n",
        "label_list = list(ner_map.keys())\n",
        "label2id = ner_map\n",
        "id2label = {v: k for k,v in label2id.items()}\n",
        "logger.info(f\"id2label: {id2label}\")\n",
        "\n",
        "# Function to convert a dataframe row to the desired format\n",
        "def row_to_dict(row, idx):\n",
        "    return {\n",
        "        'id': str(idx),\n",
        "        'ner_tags': [ner_map[tag] for tag in row['bi_tags']],\n",
        "        'tokens': list(row['words'])\n",
        "    }\n",
        "\n",
        "# Transforming the DataFrame\n",
        "transformed_data_train = [row_to_dict(row, idx) for idx, row in train_data.iterrows()]\n",
        "transformed_data_test = [row_to_dict(row, idx) for idx, row in test_data.iterrows()]\n",
        "transformed_data_val = [row_to_dict(row, idx) for idx, row in val_data.iterrows()]\n",
        "\n",
        "transformed_data = {\"train\": transformed_data_train, \"test\": transformed_data_test, \"val\": transformed_data_val}\n",
        "# Printing out the first entry as a sample\n",
        "#print(transformed_data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "ner_map {'O': 0, 'B-Ongelmia': 1, 'I-Ongelmia': 2, 'B-Ei ongelmia': 3, 'I-Ei ongelmia': 4}\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313596650
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert each split separately\n",
        "def split_to_dataset(data_split):\n",
        "    # Convert the list of dictionaries to separate lists for each column\n",
        "    ids = [entry[\"id\"] for entry in data_split]\n",
        "    ner_tags = [entry[\"ner_tags\"] for entry in data_split]\n",
        "    tokens = [entry[\"tokens\"] for entry in data_split]\n",
        "    \n",
        "    # Construct the dictionary format that Dataset.from_dict() expects\n",
        "    formatted_data = {\n",
        "        \"id\": ids,\n",
        "        \"ner_tags\": ner_tags,\n",
        "        \"tokens\": tokens\n",
        "    }\n",
        "    \n",
        "    # Convert to Dataset\n",
        "    return Dataset.from_dict(formatted_data)\n",
        "\n",
        "# Convert each dataset split\n",
        "train_dataset = split_to_dataset(transformed_data[\"train\"])\n",
        "test_dataset = split_to_dataset(transformed_data[\"test\"])\n",
        "validation_dataset = split_to_dataset(transformed_data[\"val\"])\n",
        "\n",
        "# Combine into DatasetDict\n",
        "dataset_dict = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"test\": test_dataset,\n",
        "    \"val\": validation_dataset\n",
        "})\n"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313596836
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing tools"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get tokenizer directory path from config\n",
        "tokenizer_type = config[experiment]['tokenizer_path']\n",
        "print(tokenizer_type)\n",
        "logger.info(f\"\\nTOKENIZER: {tokenizer_type}\\n\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "../_bert_bases/bert-base-finnish-cased-transformers-v1\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313597016
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get tokenizer\n",
        "# Juho removed add_special_tokens below\n",
        "tokenizer = pre_defined_function_inputs_logger(\n",
        "    logger, \n",
        "    transformers.AutoTokenizer.from_pretrained, \n",
        "    tokenizer_type, \n",
        "    is_split_into_words=True, truncation=True, padding=True, max_length=1024)\n",
        "\n",
        "# the same but without logging\n",
        "#tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-finnish-cased-transformers-v1\", is_split_into_words=True, truncation=True, padding=True, max_length=1024, add_special_tokens=True)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313597205
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    #tokenized_inputs = tokenizer(examples[\"tokens\"], truncation = True, is_split_into_words=True)\n",
        "    # Juho removed add_special_tokens=True below\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], is_split_into_words=True, truncation=True, max_length=512)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
        "        #print(label)\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        #print(\"word_ids\", word_ids)\n",
        "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
        "            # print(\"word_idx: \", word_idx)\n",
        "\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(label[word_idx])\n",
        "                \"\"\"\n",
        "                label_text = id2label[label[word_idx]]\n",
        "                if label_text.startswith('B-'):\n",
        "                    label_ids.append(label2id[label_text.replace('B-', 'I-')])\n",
        "                else:\n",
        "                    label_ids.append(label[word_idx])\n",
        "                \"\"\"\n",
        "\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "\n",
        "tokenized_data = dataset_dict.map(tokenize_and_align_labels, batched=True)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/2048 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4149843e04a848598e46b88022fbc5bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/256 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "884c1834f06b4f45be700bc3d8bf8457"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/257 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d0adba73db94daa9b60669a85fdc561"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313598262
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data collator creates batch. You can also do modifications to data, such as padding text, with this.\n",
        "data_collator = transformers.DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313598428
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label2id"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "{'O': 0,\n 'B-Ongelmia': 1,\n 'I-Ongelmia': 2,\n 'B-Ei ongelmia': 3,\n 'I-Ei ongelmia': 4}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313598618
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "logger.info(f\"\\nDEVICE: {device}\\n\") # just in case it somehow changed\n",
        "\n",
        "logger.info(\"\\nMODEL LOAD\\n\")\n",
        "# model wrapper\n",
        "mw = None \n",
        "if experiment_model == 'original':\n",
        "    # original model instantiation\n",
        "    # model = transformers.AutoModelForTokenClassification.from_pretrained(\n",
        "    #     \"bert-base-finnish-cased-transformers-v1\", num_labels=len(id2label), id2label=id2label, label2id=label2id\n",
        "    # )\n",
        "    model = pre_defined_function_inputs_logger(\n",
        "        logger, \n",
        "        transformers.AutoModelForTokenClassification.from_pretrained,\n",
        "        tokenizer_type, \n",
        "        num_labels=len(id2label), \n",
        "        id2label=id2label, \n",
        "        label2id=label2id)\n",
        "\n",
        "    from sklearn.metrics import classification_report\n",
        "    import numpy as np\n",
        "    import seqeval\n",
        "    from seqeval.metrics import classification_report, accuracy_score, f1_score\n",
        "    from seqeval.scheme import IOB2\n",
        "\n",
        "    #!{sys.executable} -m pip install nervaluate\n",
        "\n",
        "    from nervaluate import Evaluator \n",
        "    #from sklearn.metrics import classification_report\n",
        "\n",
        "    def compute_metrics(p, strictness='partial'):\n",
        "        predictions, labels = p\n",
        "        predictions = np.argmax(predictions, axis=-1)\n",
        "\n",
        "        true_predictions = [\n",
        "            [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "        true_labels = [\n",
        "            [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "\n",
        "        evaluator = Evaluator(true_labels, true_predictions, stripped_label_list, loader='list')\n",
        "        \n",
        "        # JUHO added result_indices and result_indices_by_tag below, (this returns 4 things sometimes)\n",
        "        evaluation_results = evaluator.evaluate()\n",
        "        if len(evaluation_results) == 4:\n",
        "            results, results_by_tag, result_indices, result_indices_by_tag = evaluation_results\n",
        "        else:\n",
        "            # there are only two values to unpack\n",
        "            results, results_by_tag = evaluation_results\n",
        "        #strictness = 'partial' # see https://pypi.org/project/nervaluate/ for available settings\n",
        "        print(\"\\t\\t\\t\\t Precision \\t Recall \\t F1 score\")\n",
        "        for label in results_by_tag:\n",
        "            if label != \"O\":\n",
        "                precision_label = results_by_tag[label][strictness]['precision']\n",
        "                recall_label = results_by_tag[label][strictness]['recall']\n",
        "                fscore_label = results_by_tag[label][strictness]['f1']\n",
        "                \n",
        "                print(\" {:<25} \\t {:.2f} \\t\\t {:.2f}\\t\\t {:.2f}\\t\".format(label, precision_label, recall_label, fscore_label))\n",
        "        results_compute = {}\n",
        "        \n",
        "        logger.info(\"\\nCHECKPOIT SCORES:\")\n",
        "        logger.info(\"\\nCheckpointU F1-Score: {:.2f}, Recall: {:.2f}, Precision: {:.2f}\\n\".format(\n",
        "            results[strictness]['f1'],\n",
        "            results[strictness]['recall'],\n",
        "            results[strictness]['precision']))\n",
        "        print(f\"Strictness: {strictness}\")\n",
        "        print(\"CheckpointU F1-Score: {:.2f} \\n\".format(results[strictness]['f1']))\n",
        "        #print(\"CheckpointU Recall: {:.2f} \\n\".format(results[strictness]['recall']))\n",
        "        #print(\"CheckpointU Precision: {:.2f} \\n\".format(results[strictness]['precision']))\n",
        "        results_compute[\"f1_score\"] = results[strictness]['f1']\n",
        "        results_compute[\"recall_score\"] = results[strictness]['recall']\n",
        "        results_compute[\"precision\"] = results[strictness]['precision']\n",
        "        return results_compute\n",
        "\n",
        "    mw = ModelWrapper(model=model, compute_metrics=compute_metrics)\n",
        "elif experiment_model == 'extralayer':\n",
        "    # extralayer model instantiation\n",
        "    model = AutoModel.from_pretrained(tokenizer_type,config=AutoConfig.from_pretrained(tokenizer_type, output_attentions=True, output_hidden_states=True))\n",
        "    # the new model getting instantiated\n",
        "    model = ExtraLastLayerModel(model=model, num_labels=len(id2label), id2label=id2label, label2id=label2id)\n",
        "\n",
        "    from sklearn.metrics import classification_report\n",
        "    import numpy as np\n",
        "    import seqeval\n",
        "    from seqeval.metrics import classification_report, accuracy_score, f1_score\n",
        "    from seqeval.scheme import IOB2\n",
        "\n",
        "    #!{sys.executable} -m pip install nervaluate\n",
        "\n",
        "    from nervaluate import Evaluator \n",
        "    #from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "    def compute_metrics(p, strictness='partial'):\n",
        "        predictions, labels = p\n",
        "\n",
        "        #----------------\n",
        "        # My change to predictions & labels\n",
        "        if len(predictions) == 2:\n",
        "            # predictions probably carries labels in its second batch item\n",
        "            predictions, labels = predictions\n",
        "        else: pass\n",
        "        #----------------\n",
        "        predictions = np.argmax(predictions, axis=-1)\n",
        "\n",
        "        true_predictions = [\n",
        "            [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "        true_labels = [\n",
        "            [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "\n",
        "        evaluator = Evaluator(true_labels, true_predictions, stripped_label_list, loader='list')\n",
        "        # Juho added result_indices and result_indices_by_tag below (but we don't use them)\n",
        "        evaluation_results = evaluator.evaluate()\n",
        "        if len(evaluation_results) == 4:\n",
        "            results, results_by_tag, result_indices, result_indices_by_tag = evaluation_results\n",
        "        else:\n",
        "            # there are only two values to unpack\n",
        "            results, results_by_tag = evaluation_results\n",
        "            \n",
        "        #strictness = 'partial' # see https://pypi.org/project/nervaluate/ for available settings\n",
        "        print(\"\\t\\t\\t\\t Precision \\t Recall \\t F1 score\")\n",
        "        for label in results_by_tag:\n",
        "            if label != \"O\":\n",
        "                precision_label = results_by_tag[label][strictness]['precision']\n",
        "                recall_label = results_by_tag[label][strictness]['recall']\n",
        "                fscore_label = results_by_tag[label][strictness]['f1']\n",
        "                \n",
        "                print(\" {:<25} \\t {:.2f} \\t\\t {:.2f}\\t\\t {:.2f}\\t\".format(label, precision_label, recall_label, fscore_label))\n",
        "        results_compute = {}\n",
        "        \n",
        "        logger.info(\"\\nCHECKPOIT SCORES:\")\n",
        "        logger.info(\"\\nCheckpointU F1-Score: {:.2f}, Recall: {:.2f}, Precision: {:.2f}\\n\".format(\n",
        "            results[strictness]['f1'],\n",
        "            results[strictness]['recall'],\n",
        "            results[strictness]['precision']))\n",
        "        print(\"CheckpointU F1-Score: {:.2f} \\n\".format(results[strictness]['f1']))\n",
        "        results_compute[\"f1_score\"] = results[strictness]['f1']\n",
        "        results_compute[\"recall_score\"] = results[strictness]['recall']\n",
        "        results_compute[\"precision\"] = results[strictness]['precision']\n",
        "        return results_compute\n",
        "\n",
        "    mw = ModelWrapper(model=model, compute_metrics=compute_metrics)\n",
        "else:\n",
        "    # inform that model is not baked into the pipeline \n",
        "    raise NotImplementedError('Given model not found. It is probably not implemented in pipeline.')\n",
        "\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "if mw.model is not None:\n",
        "    logger.info(f\"\\nExperiment model: {experiment_model}\\n\")\n",
        "    # send model to device\n",
        "    mw.model.to(device)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../_bert_bases/bert-base-finnish-cased-transformers-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313603344
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get BIO label list from our configuration file\n",
        "label_list = config[experiment]['bio_label_list']\n",
        "label_list = label_list[1:-1].split(',') # str to list\n",
        "label_list = list(filter(lambda x: len(x) > 0, label_list)) # include non-empty values\n",
        "label_list = [x.replace(' ', '') for x in label_list] # remove white spaces if any\n",
        "#print(label_list)\n",
        "\n",
        "stripped_label_list = list(set([x[2:] if x.startswith('B-') or x.startswith('I-') else x for x in label_list]))\n",
        "print(stripped_label_list)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['', 'Ongelmia', 'O', 'Eiongelmia']\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313603553
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do training \n",
        "Do training with the specific parameters, model, and save them and results to file.  "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# demonstrate the function\n",
        "print(read_convert_config_training_values(config, experiment, verbose=False))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'evaluation_strategy': 'steps', 'learning_rate': 9e-05, 'load_best_model_at_end': True, 'logging_steps': 100, 'num_train_epochs': 10, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 16, 'save_strategy': 'steps', 'seed': 42, 'weight_decay': 0.19998}\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313603733
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_arguments(output_dir=model_save_folder_path, **kwargs):\n",
        "    # The arguments to train the transformer model.\n",
        "    # Changing these could inprove the results with our metrics but do so in the configuration file\n",
        "    training_args = pre_defined_function_inputs_logger(logger, transformers.TrainingArguments,\n",
        "        output_dir=output_dir, # path is also from configuration\n",
        "        **kwargs\n",
        "        )\n",
        "    return training_args"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313603912
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new way reads even strictness from configuration file\n",
        "print(get_training_arguments(output_dir=model_save_folder_path, **extract_config_training_values(read_config_values_as_dict(config, experiment))))\n",
        "print(type(get_training_arguments(output_dir=model_save_folder_path, **extract_config_training_values(read_config_values_as_dict(config, experiment)))))\n",
        "\n",
        "# Old way\n",
        "# Fails to change strictness even if it's given in configs.ini\n",
        "#print(get_training_arguments(**read_convert_config_training_values(config, experiment, verbose=False)))\n",
        "#print(type(get_training_arguments(**read_convert_config_training_values(config, experiment, verbose=False))))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "TrainingArguments(\n_n_gpu=2,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_steps=100,\neval_strategy=steps,\nevaluation_strategy=steps,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=9e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=../_trained_models/Incontinence_NER_v5_20231208_orig_par_opt/C/2024_07_18_cbe7f211/runs/Jul18_14-40-03_ext131165225,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=100,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=10,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=../_trained_models/Incontinence_NER_v5_20231208_orig_par_opt/C/2024_07_18_cbe7f211,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=16,\nper_device_train_batch_size=16,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['mlflow'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=../_trained_models/Incontinence_NER_v5_20231208_orig_par_opt/C/2024_07_18_cbe7f211,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=True,\nsave_steps=500,\nsave_strategy=steps,\nsave_total_limit=None,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=0,\nweight_decay=0.19998,\n)\n<class 'transformers.training_args.TrainingArguments'>\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313604114
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SavingTrainingResultsCallback(TrainerCallback):\n",
        "    \"A callback that saves the training metrics history at the end of training.\"\n",
        "\n",
        "    # def on_epoch_end(self, args, state, control, **kwargs):\n",
        "    #     print(\"Training callback called!!\")\n",
        "    #     #print('args', args)\n",
        "    #     #print('state', state)\n",
        "    #     #print('control', control)\n",
        "    #     #print('kwargs', kwargs)\n",
        "    #     print(\"state['log_history']\", state.log_history)\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        print(\"Training callback called!!\")\n",
        "        print(\"state['log_history']\", state.log_history)\n",
        "        logger.info(f\"\\nSaving results to: {os.path.join(model_save_folder_path, 'training_results.txt')}\")\n",
        "        with open(os.path.join(model_save_folder_path, 'training_results.txt'), 'a') as f:\n",
        "            for elem in state.log_history:\n",
        "                f.write(str(elem)+\"\\n\")\n"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313604309
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Given a default learning rate and weight decay, \n",
        "    run the model and capture the F1, \n",
        "    then keep the weight decay constant, \n",
        "    iterate on training model while increasing the LR (with default delta .00001), \n",
        "    keep capturing the F1 as it improves, \n",
        "    once it stops improving, then, the code stops. \n",
        "    \n",
        "    You only want to run the decreasing of LR if the F1 never improves from increasing the LR, \n",
        "    then check by decreasing the LR (same delta), \n",
        "    if the F1 does not improve, then you can say that the default LR is in effect optimal. \n",
        " \n",
        "    Once you have the optimal LR, keep it constant, \n",
        "    then do the same for weight decay as described above for LR, say default 0.20 \n",
        "    and increment it.\"\"\"\n",
        "\n",
        "\n",
        "# # given starting parameters\n",
        "# lr = float(\"5e-5\")\n",
        "# delta = float(\"0.00001\")\n",
        "# iteration = 3\n",
        "# # init positive and negative search directions\n",
        "# iteration_pos, iteration_neg = iteration, iteration\n",
        "\n",
        "# print(lr, delta, iteration)\n",
        "# current_parameter_settings = read_convert_config_training_values(config, experiment, verbose=False)\n",
        "# print(current_parameter_settings['learning_rate'])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "'Given a default learning rate and weight decay, \\n    run the model and capture the F1, \\n    then keep the weight decay constant, \\n    iterate on training model while increasing the LR (with default delta .00001), \\n    keep capturing the F1 as it improves, \\n    once it stops improving, then, the code stops. \\n    \\n    You only want to run the decreasing of LR if the F1 never improves from increasing the LR, \\n    then check by decreasing the LR (same delta), \\n    if the F1 does not improve, then you can say that the default LR is in effect optimal.\\xa0\\n \\n    Once you have the optimal LR, keep it constant, \\n    then do the same for weight decay as described above for LR, say default 0.20 \\n    and increment it.'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721313604509
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_trainer(current_parameters, mw, tokenized_data, data_collator, tokenizer, strictness):\n",
        "    logger.info(\"\\ni:{i} TRAINER INIT\\n\")\n",
        "    trainer = transformers.Trainer(\n",
        "        model=mw.model,\n",
        "        args=get_training_arguments(**current_parameters),\n",
        "        # extract training related arguments from config values\n",
        "        #args=get_training_arguments(output_dir=model_save_folder_path, **extract_config_training_values(config_values)),\n",
        "        train_dataset=tokenized_data[\"train\"],\n",
        "        eval_dataset=tokenized_data[\"val\"],\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "        #compute_metrics = compute_metrics\n",
        "        compute_metrics = partial(mw.compute_metrics, strictness=strictness)\n",
        "    )\n",
        "\n",
        "    for cb in trainer.callback_handler.callbacks:\n",
        "        if isinstance(cb, transformers.integrations.MLflowCallback):\n",
        "            trainer.callback_handler.remove_callback(cb)\n",
        "\n",
        "    trainer.add_callback(SavingTrainingResultsCallback)\n",
        "    return trainer\n",
        "\n",
        "def train_save_model(trainer, i, current_parameters, model_save_folder_path, run_id, experiment):\n",
        "    \"\"\"Training and saving the model in one function\"\"\"\n",
        "    # train the model\n",
        "    logger.info(\"\\nTRAINING...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # evaluate the model & save it\n",
        "    logger.info(\"\\nEVALUATING\")\n",
        "    # results contain keys\n",
        "    # 'eval_loss', 'eval_f1_score', 'eval_recall_score', \n",
        "    # 'eval_precision','eval_runtime', 'eval_samples_per_second', \n",
        "    # 'eval_steps_per_second', and 'epoch'\n",
        "    results = trainer.evaluate()\n",
        "\n",
        "    # add iteration's info to results\n",
        "    results['experiment'] = experiment\n",
        "    results['run_id'] = run_id\n",
        "    model_uuid = str(uuid.uuid4())\n",
        "    results['uuid'] = model_uuid\n",
        "    results['today'] = today\n",
        "    time_stamp = datetime.datetime.now().timestamp()\n",
        "    results['timestamp'] = time_stamp\n",
        "    results['learning_rate'] = current_parameters['learning_rate'] # the real results\n",
        "    results['weight_decay'] = current_parameters['weight_decay']\n",
        "\n",
        "    # save each result to file\n",
        "    logger.info(f\"\\nRESULTS: {results}\")\n",
        "    logger.info(f\"\\nSaving results to: {os.path.join(model_save_folder_path, 'eval_results.txt')}\")\n",
        "    with open(os.path.join(model_save_folder_path, 'eval_results.txt'), 'a') as f:\n",
        "        f.write(str(results)+\"\\n\")\n",
        "\n",
        "    logger.info(f\"\\nMODEL AND TOKENIZER SAVE INFORMATION:\")\n",
        "    # saving the model and its tokenizer \n",
        "    logger.info(f\"\\nsave model to {model_save_folder_path}/model_training_file_{experiment_identifier}_{today}_RUN_{run_id}_timestamp_{time_stamp}.pt\")\n",
        "    logger.info(f\"\\nsave tokenizer to f{model_save_folder_path}/tokenizer_{today}_RUN_{run_id}_timestamp_{time_stamp}/\")\n",
        "    pre_defined_function_inputs_logger(\n",
        "        logger, \n",
        "        trainer.save_model, \n",
        "        f\"{model_save_folder_path}/model_training_file_{experiment_identifier}_{today}_RUN_{run_id}_timestamp_{time_stamp}.pt\")\n",
        "    pre_defined_function_inputs_logger(\n",
        "        logger, \n",
        "        tokenizer.save_pretrained, \n",
        "        f\"{model_save_folder_path}/tokenizer_{today}_RUN_{run_id}_timestamp_{time_stamp}/\")\n",
        "\n",
        "    logger.info(f\"\\ni:{i}, done\")\n",
        "    print(f\"\\ni:{i}, done\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "def monotonously_growing_function(trainer, i, current_parameters, model_save_folder_path, run_id, experiment):\n",
        "    \"\"\"Returns the next element of the increasing values list. The values represent 'F1 scores'.\n",
        "    Other values do not change but are set here to avoid KeyError.\n",
        "    \"\"\"\n",
        "    positive_values_list = [0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.875, 0.9, 0.925, 0.95] # 10 values in the list\n",
        "    print('i', i, positive_values_list[i])\n",
        "    return {\n",
        "        'eval_loss':0.9999, \n",
        "        'eval_f1_score':positive_values_list[i], \n",
        "        'eval_recall_score':0.9999,\n",
        "        'eval_precision':0.9999,\n",
        "        'eval_runtime': 100.0, \n",
        "        'eval_samples_per_second':12.0, \n",
        "        'eval_steps_per_second':10.0,\n",
        "        'epoch': 1.0,\n",
        "        'learning_rate': 0.00001,\n",
        "        'weight_decay':0.2\n",
        "    }\n",
        "    \n",
        "\n",
        "def decreasing_function(trainer, i, current_parameters, model_save_folder_path, run_id, experiment):\n",
        "    \"\"\"Returns values that are in decreasing order.\"\"\"\n",
        "    positive_values_list = [0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.875, 0.9, 0.925, 0.95] # 10 values in the list\n",
        "    reversed_values_list = [positive_values_list[x] for x in range(len(positive_values_list) -1, 0, -1)]\n",
        "    print('i', i, reversed_values_list[i])\n",
        "    return {\n",
        "        'eval_loss':0.9999, \n",
        "        'eval_f1_score':reversed_values_list[i], \n",
        "        'eval_recall_score':0.9999,\n",
        "        'eval_precision':0.9999,\n",
        "        'eval_runtime': 100.0, \n",
        "        'eval_samples_per_second':12.0, \n",
        "        'eval_steps_per_second':10.0,\n",
        "        'epoch': 1.0,\n",
        "        'learning_rate': 0.00001,\n",
        "        'weight_decay':0.2\n",
        "    }\n",
        "\n",
        "def altering_function(trainer, i, current_parameters, model_save_folder_path, run_id, experiment):\n",
        "    \"\"\"Returns values that are in preselected order.\n",
        "    Idea is that the 4th F1 score is lower than 3rd value and 7th value is lower than 6th value.\n",
        "    \"\"\"\n",
        "    #               0    1    2     3     4    5     6     7    8    9      10  11      12\n",
        "    values_list = [0.5, 0.6, 0.61, 0.59, 0.7, 0.7, 0.68, 0.8, 0.85, 0.875, 0.9, 0.925, 0.95] # 10 values in the list\n",
        "    print('i', i, values_list[i])\n",
        "    return {\n",
        "        'eval_loss':0.9999, \n",
        "        'eval_f1_score':values_list[i], \n",
        "        'eval_recall_score':0.9999,\n",
        "        'eval_precision':0.9999,\n",
        "        'eval_runtime': 100.0, \n",
        "        'eval_samples_per_second':12.0, \n",
        "        'eval_steps_per_second':10.0,\n",
        "        'epoch': 1.0,\n",
        "        'learning_rate': 0.00001,\n",
        "        'weight_decay':0.2\n",
        "    }\n",
        "\n",
        "def make_history(\n",
        "    k:int, \n",
        "    results, \n",
        "    target_parameter:str, \n",
        "    current_parameters:transformers.training_args.TrainingArguments,\n",
        "    strictness:str):\n",
        "    \"\"\"Makes a dictionary record of the values with current parameters\"\"\"\n",
        "    if isinstance(current_parameters, transformers.training_args.TrainingArguments):\n",
        "        # print(\"Current parameters before transforming them to dict\")\n",
        "        # print(f\"\\n{type(current_parameters)}\")\n",
        "        # print(f\"\\n{current_parameters}\")\n",
        "        current_parameters = current_parameters.to_dict()\n",
        "        # print(\"\\nCurrent parameters AFTER transforming them to dict\")\n",
        "        # print(f\"\\n{type(current_parameters)}\")\n",
        "        # print(f\"\\n{current_parameters}\")\n",
        "    else: \n",
        "        pass\n",
        "        #print(\"current_parameters was not of type 'transformers.training_args.TrainingArguments'\\n\")\n",
        "    \n",
        "    #print(f\"learning_rate from current parameters: {current_parameters['learning_rate']}\")\n",
        "    #print(f\"weight_decay from current parameters: {current_parameters['weight_decay']}\")\n",
        "\n",
        "    return {\n",
        "        'f1_score': results['eval_f1_score'],\n",
        "        'eval_f1_score': results['eval_f1_score'],\n",
        "        'target': target_parameter,\n",
        "        'parameters': current_parameters,\n",
        "        #'eval_results': results['eval_results'],\n",
        "        'eval_results': results,\n",
        "        'strictness': strictness,\n",
        "        'k': k\n",
        "        }\n",
        "\n",
        "def save_history(model_save_folder_path, history, experiment, run_id):\n",
        "    \"\"\"Save the history to file together in the folder\"\"\"\n",
        "    # save each result to file\n",
        "    history['experiment'] = experiment\n",
        "    history['run_id'] = run_id\n",
        "\n",
        "    logger.info(f\"\\nSaving history to: {os.path.join(model_save_folder_path, 'history.txt')}\")\n",
        "    with open(os.path.join(model_save_folder_path, 'history.txt'), 'w') as f:\n",
        "        \n",
        "        f.writelines(f\"EXPERIMENT: {history['experiment']}\\n\")\n",
        "        f.writelines(f\"RUN_ID: {history['run_id']}\\n\")\n",
        "        for k in history.keys():\n",
        "            if str(k).isnumeric():\n",
        "                \n",
        "                f.writelines('\\nHISTORY\\n')\n",
        "                for key, v in history[k]['history'].items():\n",
        "                    f.writelines(f\"{key}: {v}\")\n",
        "\n",
        "                f.writelines('\\nBEST\\n')\n",
        "                for key, v in history[k]['best'].items():\n",
        "                    f.writelines(f\"{key}: {v}\")\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "\n",
        "def update_search_direction_parameters(positive_training, i, current_parameters, start_parameters, target_parameter, delta):\n",
        "    \"\"\"Update the hyperparameters to match the direction of the search.\n",
        "    TODO:   Change this function to take optional starting config settings \n",
        "            as a parameter instead of automatically reading from configs.ini file.\n",
        "    \"\"\"\n",
        "    if positive_training is True:\n",
        "        # positive direction\n",
        "        current_parameters[target_parameter] = current_parameters[target_parameter] + delta\n",
        "    else:\n",
        "        # negative direction\n",
        "        if i <= 0:\n",
        "            # update settings back to current 'origo'\n",
        "            current_parameters = start_parameters\n",
        "            print(\"negative direction\", target_parameter, start_parameters)\n",
        "        else: pass\n",
        "\n",
        "        candidate_value = current_parameters[target_parameter] - delta\n",
        "\n",
        "        # update the hyperparameter\n",
        "        if candidate_value < 0.0:\n",
        "            raise ValueError(\"Negative value\")\n",
        "            #current_parameters[target_parameter] = 0.0\n",
        "        else:\n",
        "            current_parameters[target_parameter] = candidate_value\n",
        "\n",
        "    return current_parameters\n",
        "\n",
        "def get_hyperparameter_settings_from_history(history):\n",
        "    \"\"\"Get best settings based on F1 score from the history dict\"\"\"\n",
        "    f1_prev, f1_ind = 0.0, 0\n",
        "    for k, v in history.items():\n",
        "        if v['f1_score'] > f1_prev:\n",
        "            f1_ind = k\n",
        "            f1_prev = v['f1_score']\n",
        "        else: pass\n",
        "    return history[f1_ind]\n",
        "\n",
        "# LOGIC\n",
        "def search_single_parameter(current_parameters, start_parameters, target_parameter, delta, mw, run_id, experiment, strictness, max_buffer):\n",
        "    \"\"\"The logic for 'optimizing' one hyperparameter.\"\"\"\n",
        "    # we are done when continue_search is False\n",
        "    continue_search = True \n",
        "    # search the positive direction first\n",
        "    positive_training = True\n",
        "    # init f1 scores for each direction\n",
        "    prev_f1_score = 0.0\n",
        "    # init buffer\n",
        "    buffer = max_buffer\n",
        "    # iterations\n",
        "    i = -1 # iterations to one direction\n",
        "    k = -1 # all iterations\n",
        "    # all training results with parameter settings\n",
        "    history = {} \n",
        "    while continue_search:\n",
        "        i = i + 1\n",
        "        k = k + 1\n",
        "        print('k', k)\n",
        "\n",
        "        # check and set stopping condition(s)\n",
        "        if i >= iteration: # iteration ok?\n",
        "            if positive_training is True:\n",
        "                # F1 scores have been increasing \n",
        "                # -> no need to check negative range \n",
        "                # -> stop iteration\n",
        "                continue_search = False\n",
        "            elif positive_training is False:\n",
        "                continue_search = False\n",
        "            else:\n",
        "                raise Exception(\"Found unhandled condition. Stopping!\")\n",
        "\n",
        "        # do we stop altogether?\n",
        "        if continue_search is False: \n",
        "            return history\n",
        "            print(\"DONE\")\n",
        "        # or we continue\n",
        "        else: \n",
        "            # parameters for training\n",
        "            # form trainer object to train the model\n",
        "            trainer = get_trainer(current_parameters, mw, tokenized_data, data_collator, tokenizer, strictness)\n",
        "\n",
        "            # train and save the models -> get results\n",
        "            results = train_save_model(trainer, i, current_parameters, model_save_folder_path, run_id, experiment)\n",
        "\n",
        "            # TEST: only increasing scores, only decreasing scores, first increases then lowers and then grows again.\n",
        "            #results = monotonously_growing_function(trainer, i, current_parameters, model_save_folder_path, run_id, experiment) # \n",
        "            # this function is meant to test the buffer/shield\n",
        "            #results = decreasing_function(trainer, i, current_parameters, model_save_folder_path, run_id, experiment) # \n",
        "            #results = altering_function(trainer, i, current_parameters, model_save_folder_path, run_id, experiment)\n",
        "            \n",
        "            # get f1_scores\n",
        "            if k < 1: \n",
        "                prev_f1_score = 0.0\n",
        "            else:\n",
        "                prev_f1_score = history[k-1]['f1_score']\n",
        "            \n",
        "            #current_f1_score = results['eval_results']['eval_f1_score']\n",
        "            current_f1_score = results['eval_f1_score']\n",
        "            # add results & parameters to history\n",
        "            history[k] = make_history(k, results, target_parameter, current_parameters, strictness)\n",
        "\n",
        "            # check if there is a need to stop or change calculation\n",
        "            if positive_training is True and prev_f1_score > current_f1_score and continue_search is True:\n",
        "                if buffer <= 0:\n",
        "                    # previous result is 'better' than current one \n",
        "                    # -> Skip the rest of this cycle. Try negative value range next starting with origo.\n",
        "                    positive_training = False\n",
        "                    i = -1\n",
        "                    # update buffer for the next direction\n",
        "                    buffer = max_buffer\n",
        "                    #current_parameters = start_parameters\n",
        "                    print(\"len(history):\", len(history))\n",
        "                    print(\"positive_training is True and prev_f1_score > current_f1_score\")\n",
        "                    try:\n",
        "                        current_parameters = update_search_direction_parameters(positive_training, i, current_parameters, start_parameters, target_parameter, delta)\n",
        "                    except ValueError as e:\n",
        "                        # catching intentional ValueError stops the hyperparameter search\n",
        "                        print(f\"Error was: {e}\")\n",
        "                        print(\"stopping search of current hyperparameter\")\n",
        "                        continue_search = False\n",
        "                else:\n",
        "                    # when buffer is still bigger than zero \n",
        "                    # we continue calculations to the same direction\n",
        "                    buffer = buffer - 1\n",
        "                    print(\"len(history):\", len(history))\n",
        "                    print(\"positive_training is True and prev_f1_score > current_f1_score\")\n",
        "                    try:\n",
        "                        current_parameters = update_search_direction_parameters(positive_training, i, current_parameters, start_parameters, target_parameter, delta)\n",
        "                    except ValueError as e:\n",
        "                        # catching intentional ValueError stops the hyperparameter search\n",
        "                        print(f\"Error was: {e}\")\n",
        "                        print(\"stopping search of current hyperparameter\")\n",
        "                        continue_search = False\n",
        "\n",
        "            elif positive_training is False and prev_f1_score > current_f1_score and continue_search is True:\n",
        "                if buffer <= 0:\n",
        "                    # we stop searching better values for our hyperparameter\n",
        "                    continue_search = False\n",
        "                    print(\"len(history):\", len(history))\n",
        "                    print(\"positive_training is False and prev_f1_score > current_f1_score\")\n",
        "                else:\n",
        "                    # remember to update\n",
        "                    buffer = buffer - 1\n",
        "                    print(\"len(history):\", len(history))\n",
        "                    print(\"positive_training is True and prev_f1_score > current_f1_score\")\n",
        "                    try:\n",
        "                        current_parameters = update_search_direction_parameters(positive_training, i, current_parameters, start_parameters, target_parameter, delta)\n",
        "                    except ValueError as e:\n",
        "                        # catching intentional ValueError stops the hyperparameter search\n",
        "                        print(f\"Error was: {e}\")\n",
        "                        print(\"stopping search of current hyperparameter\")\n",
        "                        continue_search = False\n",
        "            else:\n",
        "                # cases where previous F1 score is smaller than the current F1 score\n",
        "                # -> training is progressing as intended\n",
        "                print(\"training is progressing as intended\")\n",
        "                try:\n",
        "                    current_parameters = update_search_direction_parameters(positive_training, i, current_parameters, start_parameters, target_parameter, delta)\n",
        "                except ValueError as e:\n",
        "                    # catching intentional ValueError stops the hyperparameter search\n",
        "                    print(f\"Error was: {e}\")\n",
        "                    print(\"stopping search of current hyperparameter\")\n",
        "                    continue_search = False\n",
        "\n",
        "            #print(\"learning_rate: \", current_parameters['learning_rate'])\n",
        "            #print(\"weight_decay: \", current_parameters['weight_decay'])\n",
        "            #print(\"delta: \", delta)\n",
        "            #print(\"prev_f1_score: \", prev_f1_score)\n",
        "            #print(\"current_f1_score: \", current_f1_score)\n",
        "\n",
        "    return history\n",
        "\n",
        "# MAIN SEARCH LOOP for 'optimal' hyperparameters\n",
        "search_parameters:list = ['learning_rate', 'weight_decay'] # list of parameters to search the 'best' values\n",
        "# old way\n",
        "#current_parameters = read_convert_config_training_values(config, experiment, verbose=False)\n",
        "# new way\n",
        "config_values = read_config_values_as_dict(config, experiment)\n",
        "current_parameters = extract_config_training_values(config_values)\n",
        "strictness = config_values['strictness']\n",
        "iteration = 10\n",
        "delta = float(\"0.00001\")\n",
        "# can survive one low F1 score, 2nd causes to switch mode\n",
        "max_buffer:int = 2 \n",
        "\n",
        "all_history = {x:{} for x in range(len(search_parameters))} # dict to hold all tried hyperparameter settings and the respective results\n",
        "for i, target_parameter in enumerate(search_parameters):\n",
        "    start_parameters = current_parameters.copy() # origo\n",
        "    history = search_single_parameter(current_parameters, start_parameters, target_parameter, delta, mw, run_id, experiment, strictness, max_buffer) # train models\n",
        "    historical_settings = get_hyperparameter_settings_from_history(history) # find best settings from history\n",
        "    all_history[i]['history'] = history\n",
        "    all_history[i]['best'] = historical_settings\n",
        "    print('historical_settings', historical_settings)\n",
        "    # update current parameters\n",
        "    current_parameters = historical_settings['parameters']\n",
        "    # update current parameters with the best hyperparameter value\n",
        "    current_parameters[search_parameters[i]] = historical_settings['eval_results'][search_parameters[i]]\n",
        "\n",
        "save_history(model_save_folder_path, history=all_history, experiment=experiment, run_id=run_id)\n",
        "\n",
        "# save settings and results to .csv file\n",
        "df = pd.DataFrame(all_history)\n",
        "df.to_csv(os.path.join(model_save_folder_path, 'history.csv'), sep=';', encoding='utf-8', index=False)\n",
        "\n",
        "\n",
        "# search_parameters\n",
        "# current_parameters = get_lr_wd(tiedostonimi)\n",
        "# optimize(search_parameters, current_parameters)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "k 0\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:05, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.269900</td>\n      <td>0.149037</td>\n      <td>0.572485</td>\n      <td>0.661162</td>\n      <td>0.504783</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.125800</td>\n      <td>0.120871</td>\n      <td>0.655980</td>\n      <td>0.734055</td>\n      <td>0.592916</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.100100</td>\n      <td>0.112813</td>\n      <td>0.690270</td>\n      <td>0.771640</td>\n      <td>0.624424</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.079800</td>\n      <td>0.107857</td>\n      <td>0.689949</td>\n      <td>0.691913</td>\n      <td>0.687995</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.060800</td>\n      <td>0.108071</td>\n      <td>0.695581</td>\n      <td>0.726082</td>\n      <td>0.667539</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.048000</td>\n      <td>0.113315</td>\n      <td>0.718114</td>\n      <td>0.763098</td>\n      <td>0.678138</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.49 \t\t 0.68\t\t 0.57\t\n Eiongelmia                \t 0.75 \t\t 0.47\t\t 0.58\t\nStrictness: partial\nCheckpointU F1-Score: 0.57 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.59 \t\t 0.76\t\t 0.67\t\n Eiongelmia                \t 0.63 \t\t 0.48\t\t 0.54\t\nStrictness: partial\nCheckpointU F1-Score: 0.66 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.62 \t\t 0.80\t\t 0.70\t\n Eiongelmia                \t 0.66 \t\t 0.50\t\t 0.57\t\nStrictness: partial\nCheckpointU F1-Score: 0.69 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.71 \t\t 0.72\t\t 0.72\t\n Eiongelmia                \t 0.42 \t\t 0.39\t\t 0.41\t\nStrictness: partial\nCheckpointU F1-Score: 0.69 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.70 \t\t 0.76\t\t 0.73\t\n Eiongelmia                \t 0.39 \t\t 0.42\t\t 0.41\t\nStrictness: partial\nCheckpointU F1-Score: 0.70 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.70 \t\t 0.80\t\t 0.75\t\n Eiongelmia                \t 0.43 \t\t 0.43\t\t 0.43\t\nStrictness: partial\nCheckpointU F1-Score: 0.72 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.2699, 'grad_norm': 0.6479988694190979, 'learning_rate': 7.593750000000001e-05, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.14903677999973297, 'eval_f1_score': 0.5724852071005917, 'eval_recall_score': 0.6611617312072893, 'eval_precision': 0.5047826086956522, 'eval_runtime': 1.2585, 'eval_samples_per_second': 204.204, 'eval_steps_per_second': 7.151, 'epoch': 1.5625, 'step': 100}, {'loss': 0.1258, 'grad_norm': 0.5246321558952332, 'learning_rate': 6.1875e-05, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.12087132781744003, 'eval_f1_score': 0.6559796437659033, 'eval_recall_score': 0.7340546697038725, 'eval_precision': 0.5929162833486661, 'eval_runtime': 1.2259, 'eval_samples_per_second': 209.644, 'eval_steps_per_second': 7.342, 'epoch': 3.125, 'step': 200}, {'loss': 0.1001, 'grad_norm': 0.6485699415206909, 'learning_rate': 4.7812500000000003e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.11281336098909378, 'eval_f1_score': 0.6902699949057565, 'eval_recall_score': 0.7716400911161732, 'eval_precision': 0.6244239631336406, 'eval_runtime': 1.2197, 'eval_samples_per_second': 210.715, 'eval_steps_per_second': 7.379, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0798, 'grad_norm': 0.5967851877212524, 'learning_rate': 3.375e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.10785666108131409, 'eval_f1_score': 0.6899488926746167, 'eval_recall_score': 0.6919134396355353, 'eval_precision': 0.687995469988675, 'eval_runtime': 1.2249, 'eval_samples_per_second': 209.818, 'eval_steps_per_second': 7.348, 'epoch': 6.25, 'step': 400}, {'loss': 0.0608, 'grad_norm': 0.47271281480789185, 'learning_rate': 1.96875e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.10807117819786072, 'eval_f1_score': 0.6955810147299508, 'eval_recall_score': 0.7260820045558086, 'eval_precision': 0.6675392670157068, 'eval_runtime': 1.2631, 'eval_samples_per_second': 203.469, 'eval_steps_per_second': 7.125, 'epoch': 7.8125, 'step': 500}, {'loss': 0.048, 'grad_norm': 0.9098544716835022, 'learning_rate': 5.625e-06, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.11331544816493988, 'eval_f1_score': 0.7181136120042871, 'eval_recall_score': 0.7630979498861048, 'eval_precision': 0.6781376518218624, 'eval_runtime': 1.2258, 'eval_samples_per_second': 209.665, 'eval_steps_per_second': 7.342, 'epoch': 9.375, 'step': 600}, {'train_runtime': 247.5716, 'train_samples_per_second': 82.724, 'train_steps_per_second': 2.585, 'total_flos': 2687412044952960.0, 'train_loss': 0.10963313654065132, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.70 \t\t 0.76\t\t 0.73\t\n Eiongelmia                \t 0.39 \t\t 0.42\t\t 0.41\t\nStrictness: partial\nCheckpointU F1-Score: 0.70 \n\n\ni:0, done\ntraining is progressing as intended\nk 1\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:04, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.075400</td>\n      <td>0.112770</td>\n      <td>0.713080</td>\n      <td>0.769932</td>\n      <td>0.664047</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.051500</td>\n      <td>0.121736</td>\n      <td>0.715645</td>\n      <td>0.771071</td>\n      <td>0.667653</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.038100</td>\n      <td>0.144512</td>\n      <td>0.722904</td>\n      <td>0.771071</td>\n      <td>0.680402</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.027500</td>\n      <td>0.148539</td>\n      <td>0.703991</td>\n      <td>0.723235</td>\n      <td>0.685745</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.019000</td>\n      <td>0.152233</td>\n      <td>0.729412</td>\n      <td>0.776765</td>\n      <td>0.687500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.014400</td>\n      <td>0.160235</td>\n      <td>0.741057</td>\n      <td>0.790433</td>\n      <td>0.697487</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.69 \t\t 0.80\t\t 0.74\t\n Eiongelmia                \t 0.43 \t\t 0.52\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.71 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.68 \t\t 0.80\t\t 0.74\t\n Eiongelmia                \t 0.49 \t\t 0.46\t\t 0.48\t\nStrictness: partial\nCheckpointU F1-Score: 0.72 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.70 \t\t 0.80\t\t 0.75\t\n Eiongelmia                \t 0.46 \t\t 0.47\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.72 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.73 \t\t 0.76\t\t 0.74\t\n Eiongelmia                \t 0.34 \t\t 0.42\t\t 0.38\t\nStrictness: partial\nCheckpointU F1-Score: 0.70 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.73 \t\t 0.81\t\t 0.77\t\n Eiongelmia                \t 0.36 \t\t 0.44\t\t 0.39\t\nStrictness: partial\nCheckpointU F1-Score: 0.73 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.73 \t\t 0.83\t\t 0.77\t\n Eiongelmia                \t 0.41 \t\t 0.45\t\t 0.43\t\nStrictness: partial\nCheckpointU F1-Score: 0.74 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0754, 'grad_norm': 0.5478309988975525, 'learning_rate': 8.4375e-05, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.11276951432228088, 'eval_f1_score': 0.7130801687763713, 'eval_recall_score': 0.7699316628701595, 'eval_precision': 0.6640471512770137, 'eval_runtime': 1.2389, 'eval_samples_per_second': 207.449, 'eval_steps_per_second': 7.265, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0515, 'grad_norm': 0.7744783163070679, 'learning_rate': 6.875e-05, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.12173589318990707, 'eval_f1_score': 0.715644820295983, 'eval_recall_score': 0.7710706150341685, 'eval_precision': 0.6676528599605522, 'eval_runtime': 1.2441, 'eval_samples_per_second': 206.578, 'eval_steps_per_second': 7.234, 'epoch': 3.125, 'step': 200}, {'loss': 0.0381, 'grad_norm': 0.8900099396705627, 'learning_rate': 5.3125000000000004e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.14451240003108978, 'eval_f1_score': 0.7229044313934864, 'eval_recall_score': 0.7710706150341685, 'eval_precision': 0.6804020100502512, 'eval_runtime': 1.2348, 'eval_samples_per_second': 208.131, 'eval_steps_per_second': 7.289, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0275, 'grad_norm': 1.0431101322174072, 'learning_rate': 3.7500000000000003e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.14853860437870026, 'eval_f1_score': 0.7039911308203991, 'eval_recall_score': 0.7232346241457859, 'eval_precision': 0.6857451403887689, 'eval_runtime': 1.2268, 'eval_samples_per_second': 209.483, 'eval_steps_per_second': 7.336, 'epoch': 6.25, 'step': 400}, {'loss': 0.019, 'grad_norm': 0.3043144643306732, 'learning_rate': 2.1875e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.15223300457000732, 'eval_f1_score': 0.7294117647058823, 'eval_recall_score': 0.7767653758542141, 'eval_precision': 0.6875, 'eval_runtime': 1.2347, 'eval_samples_per_second': 208.14, 'eval_steps_per_second': 7.289, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0144, 'grad_norm': 0.28337445855140686, 'learning_rate': 6.25e-06, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.16023457050323486, 'eval_f1_score': 0.7410571276027763, 'eval_recall_score': 0.7904328018223234, 'eval_precision': 0.6974874371859296, 'eval_runtime': 1.22, 'eval_samples_per_second': 210.652, 'eval_steps_per_second': 7.377, 'epoch': 9.375, 'step': 600}, {'train_runtime': 244.5404, 'train_samples_per_second': 83.749, 'train_steps_per_second': 2.617, 'total_flos': 2687412044952960.0, 'train_loss': 0.036022381763905285, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:01]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.73 \t\t 0.81\t\t 0.77\t\n Eiongelmia                \t 0.36 \t\t 0.44\t\t 0.39\t\nStrictness: partial\nCheckpointU F1-Score: 0.73 \n\n\ni:1, done\ntraining is progressing as intended\nk 2\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:03, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.037500</td>\n      <td>0.140252</td>\n      <td>0.692901</td>\n      <td>0.767084</td>\n      <td>0.631801</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.021500</td>\n      <td>0.157164</td>\n      <td>0.743776</td>\n      <td>0.816629</td>\n      <td>0.682857</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.016300</td>\n      <td>0.158444</td>\n      <td>0.752864</td>\n      <td>0.785877</td>\n      <td>0.722513</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.010100</td>\n      <td>0.164168</td>\n      <td>0.768489</td>\n      <td>0.816629</td>\n      <td>0.725709</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.005000</td>\n      <td>0.177246</td>\n      <td>0.763458</td>\n      <td>0.799544</td>\n      <td>0.730489</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.004600</td>\n      <td>0.173056</td>\n      <td>0.758861</td>\n      <td>0.804670</td>\n      <td>0.717988</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.71 \t\t 0.79\t\t 0.75\t\n Eiongelmia                \t 0.26 \t\t 0.54\t\t 0.35\t\nStrictness: partial\nCheckpointU F1-Score: 0.69 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.70 \t\t 0.85\t\t 0.77\t\n Eiongelmia                \t 0.49 \t\t 0.52\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.74 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.74 \t\t 0.82\t\t 0.78\t\n Eiongelmia                \t 0.49 \t\t 0.45\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.75 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.85\t\t 0.79\t\n Eiongelmia                \t 0.51 \t\t 0.48\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.84\t\t 0.79\t\n Eiongelmia                \t 0.48 \t\t 0.45\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.84\t\t 0.80\t\n Eiongelmia                \t 0.39 \t\t 0.44\t\t 0.41\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0375, 'grad_norm': 0.447226345539093, 'learning_rate': 9.28125e-05, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.14025232195854187, 'eval_f1_score': 0.6929012345679012, 'eval_recall_score': 0.7670842824601367, 'eval_precision': 0.6318011257035647, 'eval_runtime': 1.2244, 'eval_samples_per_second': 209.892, 'eval_steps_per_second': 7.35, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0215, 'grad_norm': 0.9345725178718567, 'learning_rate': 7.562500000000001e-05, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.15716443955898285, 'eval_f1_score': 0.7437759336099585, 'eval_recall_score': 0.816628701594533, 'eval_precision': 0.6828571428571428, 'eval_runtime': 1.2347, 'eval_samples_per_second': 208.156, 'eval_steps_per_second': 7.29, 'epoch': 3.125, 'step': 200}, {'loss': 0.0163, 'grad_norm': 0.5973076224327087, 'learning_rate': 5.8437500000000004e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.1584441363811493, 'eval_f1_score': 0.7528641571194762, 'eval_recall_score': 0.785876993166287, 'eval_precision': 0.7225130890052356, 'eval_runtime': 1.2413, 'eval_samples_per_second': 207.034, 'eval_steps_per_second': 7.25, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0101, 'grad_norm': 0.4786902070045471, 'learning_rate': 4.125e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.16416825354099274, 'eval_f1_score': 0.7684887459807074, 'eval_recall_score': 0.816628701594533, 'eval_precision': 0.7257085020242915, 'eval_runtime': 1.2544, 'eval_samples_per_second': 204.877, 'eval_steps_per_second': 7.175, 'epoch': 6.25, 'step': 400}, {'loss': 0.005, 'grad_norm': 0.19492299854755402, 'learning_rate': 2.4062500000000002e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.17724579572677612, 'eval_f1_score': 0.763458401305057, 'eval_recall_score': 0.7995444191343963, 'eval_precision': 0.7304890738813735, 'eval_runtime': 1.2149, 'eval_samples_per_second': 211.535, 'eval_steps_per_second': 7.408, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0046, 'grad_norm': 0.19895629584789276, 'learning_rate': 6.875e-06, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.1730562150478363, 'eval_f1_score': 0.7588614393125672, 'eval_recall_score': 0.8046697038724373, 'eval_precision': 0.7179878048780488, 'eval_runtime': 1.2311, 'eval_samples_per_second': 208.76, 'eval_steps_per_second': 7.311, 'epoch': 9.375, 'step': 600}, {'train_runtime': 244.1563, 'train_samples_per_second': 83.881, 'train_steps_per_second': 2.621, 'total_flos': 2687412044952960.0, 'train_loss': 0.015031911991536617, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.84\t\t 0.79\t\n Eiongelmia                \t 0.48 \t\t 0.45\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\ni:2, done\ntraining is progressing as intended\nk 3\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:03, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.018500</td>\n      <td>0.166553</td>\n      <td>0.730102</td>\n      <td>0.814920</td>\n      <td>0.661275</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.011800</td>\n      <td>0.169488</td>\n      <td>0.758602</td>\n      <td>0.816059</td>\n      <td>0.708704</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.009400</td>\n      <td>0.178557</td>\n      <td>0.749340</td>\n      <td>0.808656</td>\n      <td>0.698132</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.005100</td>\n      <td>0.185918</td>\n      <td>0.747831</td>\n      <td>0.785308</td>\n      <td>0.713768</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.002400</td>\n      <td>0.194180</td>\n      <td>0.757867</td>\n      <td>0.809226</td>\n      <td>0.712638</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.002100</td>\n      <td>0.190766</td>\n      <td>0.763102</td>\n      <td>0.812642</td>\n      <td>0.719254</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.69 \t\t 0.85\t\t 0.76\t\n Eiongelmia                \t 0.41 \t\t 0.53\t\t 0.46\t\nStrictness: partial\nCheckpointU F1-Score: 0.73 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.73 \t\t 0.85\t\t 0.78\t\n Eiongelmia                \t 0.48 \t\t 0.54\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.72 \t\t 0.85\t\t 0.78\t\n Eiongelmia                \t 0.44 \t\t 0.46\t\t 0.45\t\nStrictness: partial\nCheckpointU F1-Score: 0.75 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.74 \t\t 0.82\t\t 0.78\t\n Eiongelmia                \t 0.46 \t\t 0.43\t\t 0.45\t\nStrictness: partial\nCheckpointU F1-Score: 0.75 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.84\t\t 0.80\t\n Eiongelmia                \t 0.38 \t\t 0.48\t\t 0.42\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.85\t\t 0.80\t\n Eiongelmia                \t 0.43 \t\t 0.46\t\t 0.44\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0185, 'grad_norm': 0.37596118450164795, 'learning_rate': 0.00010125000000000001, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.16655264794826508, 'eval_f1_score': 0.7301020408163265, 'eval_recall_score': 0.8149202733485194, 'eval_precision': 0.661275415896488, 'eval_runtime': 1.2989, 'eval_samples_per_second': 197.862, 'eval_steps_per_second': 6.929, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0118, 'grad_norm': 1.2821751832962036, 'learning_rate': 8.25e-05, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.1694883108139038, 'eval_f1_score': 0.7586024351508736, 'eval_recall_score': 0.8160592255125285, 'eval_precision': 0.7087042532146389, 'eval_runtime': 1.2285, 'eval_samples_per_second': 209.204, 'eval_steps_per_second': 7.326, 'epoch': 3.125, 'step': 200}, {'loss': 0.0094, 'grad_norm': 0.5195663571357727, 'learning_rate': 6.375e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.17855699360370636, 'eval_f1_score': 0.7493403693931399, 'eval_recall_score': 0.8086560364464692, 'eval_precision': 0.6981317600786627, 'eval_runtime': 1.2179, 'eval_samples_per_second': 211.018, 'eval_steps_per_second': 7.39, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0051, 'grad_norm': 0.3787708878517151, 'learning_rate': 4.5e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.18591761589050293, 'eval_f1_score': 0.747830802603037, 'eval_recall_score': 0.7853075170842825, 'eval_precision': 0.7137681159420289, 'eval_runtime': 1.2297, 'eval_samples_per_second': 208.99, 'eval_steps_per_second': 7.319, 'epoch': 6.25, 'step': 400}, {'loss': 0.0024, 'grad_norm': 0.19411009550094604, 'learning_rate': 2.625e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.1941801756620407, 'eval_f1_score': 0.7578666666666667, 'eval_recall_score': 0.8092255125284739, 'eval_precision': 0.7126379137412236, 'eval_runtime': 1.2423, 'eval_samples_per_second': 206.866, 'eval_steps_per_second': 7.244, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0021, 'grad_norm': 0.2841031849384308, 'learning_rate': 7.5e-06, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.19076576828956604, 'eval_f1_score': 0.7631016042780749, 'eval_recall_score': 0.8126423690205011, 'eval_precision': 0.7192540322580645, 'eval_runtime': 1.2311, 'eval_samples_per_second': 208.75, 'eval_steps_per_second': 7.31, 'epoch': 9.375, 'step': 600}, {'train_runtime': 243.5453, 'train_samples_per_second': 84.091, 'train_steps_per_second': 2.628, 'total_flos': 2687412044952960.0, 'train_loss': 0.0077827996865380555, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.84\t\t 0.80\t\n Eiongelmia                \t 0.38 \t\t 0.48\t\t 0.42\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\ni:3, done\nlen(history): 4\npositive_training is True and prev_f1_score > current_f1_score\nk 4\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:02, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.014600</td>\n      <td>0.167622</td>\n      <td>0.727952</td>\n      <td>0.832005</td>\n      <td>0.647033</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.009600</td>\n      <td>0.177846</td>\n      <td>0.738083</td>\n      <td>0.802392</td>\n      <td>0.683317</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.006800</td>\n      <td>0.169088</td>\n      <td>0.770632</td>\n      <td>0.818907</td>\n      <td>0.727733</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.002900</td>\n      <td>0.186034</td>\n      <td>0.768325</td>\n      <td>0.817768</td>\n      <td>0.724521</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.001600</td>\n      <td>0.204267</td>\n      <td>0.770800</td>\n      <td>0.817768</td>\n      <td>0.728934</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.001100</td>\n      <td>0.195715</td>\n      <td>0.772679</td>\n      <td>0.824601</td>\n      <td>0.726908</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.66 \t\t 0.87\t\t 0.75\t\n Eiongelmia                \t 0.47 \t\t 0.43\t\t 0.45\t\nStrictness: partial\nCheckpointU F1-Score: 0.73 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.73 \t\t 0.84\t\t 0.78\t\n Eiongelmia                \t 0.32 \t\t 0.48\t\t 0.38\t\nStrictness: partial\nCheckpointU F1-Score: 0.74 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.86\t\t 0.80\t\n Eiongelmia                \t 0.49 \t\t 0.47\t\t 0.48\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.85\t\t 0.80\t\n Eiongelmia                \t 0.43 \t\t 0.51\t\t 0.46\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.85\t\t 0.80\t\n Eiongelmia                \t 0.51 \t\t 0.47\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.86\t\t 0.80\t\n Eiongelmia                \t 0.49 \t\t 0.47\t\t 0.48\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0146, 'grad_norm': 0.2821207046508789, 'learning_rate': 0.00010968750000000002, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.16762156784534454, 'eval_f1_score': 0.727952167414051, 'eval_recall_score': 0.8320045558086561, 'eval_precision': 0.6470327723649247, 'eval_runtime': 1.2153, 'eval_samples_per_second': 211.478, 'eval_steps_per_second': 7.406, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0096, 'grad_norm': 0.24570360779762268, 'learning_rate': 8.937500000000002e-05, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.17784611880779266, 'eval_f1_score': 0.7380827658459925, 'eval_recall_score': 0.8023917995444191, 'eval_precision': 0.6833171677982541, 'eval_runtime': 1.2199, 'eval_samples_per_second': 210.669, 'eval_steps_per_second': 7.378, 'epoch': 3.125, 'step': 200}, {'loss': 0.0068, 'grad_norm': 0.2842920124530792, 'learning_rate': 6.90625e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.16908769309520721, 'eval_f1_score': 0.7706323687031083, 'eval_recall_score': 0.8189066059225513, 'eval_precision': 0.7277327935222672, 'eval_runtime': 1.2425, 'eval_samples_per_second': 206.843, 'eval_steps_per_second': 7.244, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0029, 'grad_norm': 0.5145641565322876, 'learning_rate': 4.8750000000000006e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.18603412806987762, 'eval_f1_score': 0.7683253076511504, 'eval_recall_score': 0.8177676537585421, 'eval_precision': 0.7245206861755802, 'eval_runtime': 1.225, 'eval_samples_per_second': 209.8, 'eval_steps_per_second': 7.347, 'epoch': 6.25, 'step': 400}, {'loss': 0.0016, 'grad_norm': 0.19450265169143677, 'learning_rate': 2.8437500000000003e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.20426659286022186, 'eval_f1_score': 0.7707997852925389, 'eval_recall_score': 0.8177676537585421, 'eval_precision': 0.7289340101522843, 'eval_runtime': 1.2244, 'eval_samples_per_second': 209.896, 'eval_steps_per_second': 7.35, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0011, 'grad_norm': 0.15908759832382202, 'learning_rate': 8.125000000000001e-06, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.19571508467197418, 'eval_f1_score': 0.7726787620064035, 'eval_recall_score': 0.8246013667425968, 'eval_precision': 0.7269076305220884, 'eval_runtime': 1.2451, 'eval_samples_per_second': 206.406, 'eval_steps_per_second': 7.228, 'epoch': 9.375, 'step': 600}, {'train_runtime': 242.937, 'train_samples_per_second': 84.302, 'train_steps_per_second': 2.634, 'total_flos': 2687412044952960.0, 'train_loss': 0.005779967745183967, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:01]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.85\t\t 0.80\t\n Eiongelmia                \t 0.51 \t\t 0.47\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\ni:4, done\ntraining is progressing as intended\nk 5\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:04, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.010200</td>\n      <td>0.174234</td>\n      <td>0.750777</td>\n      <td>0.825171</td>\n      <td>0.688688</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.008500</td>\n      <td>0.189560</td>\n      <td>0.758470</td>\n      <td>0.790433</td>\n      <td>0.728992</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.005000</td>\n      <td>0.196989</td>\n      <td>0.761304</td>\n      <td>0.824601</td>\n      <td>0.707031</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.001900</td>\n      <td>0.184922</td>\n      <td>0.759595</td>\n      <td>0.811503</td>\n      <td>0.713928</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.000800</td>\n      <td>0.209674</td>\n      <td>0.760824</td>\n      <td>0.820615</td>\n      <td>0.709154</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000800</td>\n      <td>0.198751</td>\n      <td>0.768697</td>\n      <td>0.819476</td>\n      <td>0.723843</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.71 \t\t 0.86\t\t 0.78\t\n Eiongelmia                \t 0.45 \t\t 0.51\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.75 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.83\t\t 0.79\t\n Eiongelmia                \t 0.49 \t\t 0.46\t\t 0.48\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.72 \t\t 0.86\t\t 0.78\t\n Eiongelmia                \t 0.53 \t\t 0.49\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.74 \t\t 0.84\t\t 0.79\t\n Eiongelmia                \t 0.45 \t\t 0.50\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.72 \t\t 0.86\t\t 0.79\t\n Eiongelmia                \t 0.51 \t\t 0.45\t\t 0.48\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.86\t\t 0.80\t\n Eiongelmia                \t 0.49 \t\t 0.48\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0102, 'grad_norm': 0.5064897537231445, 'learning_rate': 0.00011812500000000001, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.17423449456691742, 'eval_f1_score': 0.750777202072539, 'eval_recall_score': 0.8251708428246014, 'eval_precision': 0.6886882129277566, 'eval_runtime': 1.2214, 'eval_samples_per_second': 210.412, 'eval_steps_per_second': 7.369, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0085, 'grad_norm': 0.6524461507797241, 'learning_rate': 9.625000000000001e-05, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.18956029415130615, 'eval_f1_score': 0.7584699453551913, 'eval_recall_score': 0.7904328018223234, 'eval_precision': 0.7289915966386554, 'eval_runtime': 1.2151, 'eval_samples_per_second': 211.508, 'eval_steps_per_second': 7.407, 'epoch': 3.125, 'step': 200}, {'loss': 0.005, 'grad_norm': 0.33212029933929443, 'learning_rate': 7.4375e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.19698895514011383, 'eval_f1_score': 0.76130389064143, 'eval_recall_score': 0.8246013667425968, 'eval_precision': 0.70703125, 'eval_runtime': 1.2221, 'eval_samples_per_second': 210.295, 'eval_steps_per_second': 7.364, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0019, 'grad_norm': 0.02213549241423607, 'learning_rate': 5.25e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.18492180109024048, 'eval_f1_score': 0.759594882729211, 'eval_recall_score': 0.8115034168564921, 'eval_precision': 0.7139278557114228, 'eval_runtime': 1.2265, 'eval_samples_per_second': 209.536, 'eval_steps_per_second': 7.338, 'epoch': 6.25, 'step': 400}, {'loss': 0.0008, 'grad_norm': 0.08436500281095505, 'learning_rate': 3.0625000000000006e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.2096741795539856, 'eval_f1_score': 0.7608236536430834, 'eval_recall_score': 0.820615034168565, 'eval_precision': 0.7091535433070866, 'eval_runtime': 1.2191, 'eval_samples_per_second': 210.819, 'eval_steps_per_second': 7.383, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0008, 'grad_norm': 0.15312084555625916, 'learning_rate': 8.750000000000001e-06, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.19875134527683258, 'eval_f1_score': 0.7686965811965812, 'eval_recall_score': 0.8194760820045558, 'eval_precision': 0.7238430583501007, 'eval_runtime': 1.2342, 'eval_samples_per_second': 208.24, 'eval_steps_per_second': 7.292, 'epoch': 9.375, 'step': 600}, {'train_runtime': 244.9821, 'train_samples_per_second': 83.598, 'train_steps_per_second': 2.612, 'total_flos': 2687412044952960.0, 'train_loss': 0.0043006382678868246, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.72 \t\t 0.86\t\t 0.79\t\n Eiongelmia                \t 0.51 \t\t 0.45\t\t 0.48\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\ni:5, done\nlen(history): 6\npositive_training is True and prev_f1_score > current_f1_score\nk 6\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:04, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.008700</td>\n      <td>0.174666</td>\n      <td>0.723754</td>\n      <td>0.777335</td>\n      <td>0.677083</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.006700</td>\n      <td>0.182486</td>\n      <td>0.751678</td>\n      <td>0.829157</td>\n      <td>0.687441</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.005400</td>\n      <td>0.165765</td>\n      <td>0.769430</td>\n      <td>0.845672</td>\n      <td>0.705798</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.002500</td>\n      <td>0.187756</td>\n      <td>0.773256</td>\n      <td>0.833144</td>\n      <td>0.721400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.000800</td>\n      <td>0.204853</td>\n      <td>0.768770</td>\n      <td>0.804670</td>\n      <td>0.735938</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000700</td>\n      <td>0.195622</td>\n      <td>0.759554</td>\n      <td>0.814920</td>\n      <td>0.711233</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.80\t\t 0.78\t\n Eiongelmia                \t 0.26 \t\t 0.52\t\t 0.34\t\nStrictness: partial\nCheckpointU F1-Score: 0.72 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.70 \t\t 0.87\t\t 0.78\t\n Eiongelmia                \t 0.51 \t\t 0.45\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.75 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.72 \t\t 0.88\t\t 0.80\t\n Eiongelmia                \t 0.49 \t\t 0.49\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.74 \t\t 0.88\t\t 0.80\t\n Eiongelmia                \t 0.46 \t\t 0.43\t\t 0.45\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.84\t\t 0.80\t\n Eiongelmia                \t 0.49 \t\t 0.45\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.74 \t\t 0.85\t\t 0.79\t\n Eiongelmia                \t 0.43 \t\t 0.45\t\t 0.44\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0087, 'grad_norm': 0.3712764382362366, 'learning_rate': 0.00012656250000000002, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.17466631531715393, 'eval_f1_score': 0.7237539766702014, 'eval_recall_score': 0.7773348519362187, 'eval_precision': 0.6770833333333334, 'eval_runtime': 1.2548, 'eval_samples_per_second': 204.814, 'eval_steps_per_second': 7.172, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0067, 'grad_norm': 0.5289633274078369, 'learning_rate': 0.00010312500000000001, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.18248635530471802, 'eval_f1_score': 0.7516778523489933, 'eval_recall_score': 0.8291571753986332, 'eval_precision': 0.6874409820585458, 'eval_runtime': 1.2822, 'eval_samples_per_second': 200.433, 'eval_steps_per_second': 7.019, 'epoch': 3.125, 'step': 200}, {'loss': 0.0054, 'grad_norm': 0.19768616557121277, 'learning_rate': 7.96875e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.1657651662826538, 'eval_f1_score': 0.7694300518134715, 'eval_recall_score': 0.8456719817767654, 'eval_precision': 0.7057984790874525, 'eval_runtime': 1.2325, 'eval_samples_per_second': 208.528, 'eval_steps_per_second': 7.303, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0025, 'grad_norm': 0.05775407701730728, 'learning_rate': 5.6250000000000005e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.1877560168504715, 'eval_f1_score': 0.7732558139534883, 'eval_recall_score': 0.8331435079726651, 'eval_precision': 0.7214003944773175, 'eval_runtime': 1.2244, 'eval_samples_per_second': 209.891, 'eval_steps_per_second': 7.35, 'epoch': 6.25, 'step': 400}, {'loss': 0.0008, 'grad_norm': 0.036521948873996735, 'learning_rate': 3.2812500000000005e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.204852893948555, 'eval_f1_score': 0.7687704026115343, 'eval_recall_score': 0.8046697038724373, 'eval_precision': 0.7359375, 'eval_runtime': 1.2326, 'eval_samples_per_second': 208.502, 'eval_steps_per_second': 7.302, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0007, 'grad_norm': 0.6160938143730164, 'learning_rate': 9.375000000000001e-06, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.19562171399593353, 'eval_f1_score': 0.7595541401273885, 'eval_recall_score': 0.8149202733485194, 'eval_precision': 0.7112326043737575, 'eval_runtime': 1.2345, 'eval_samples_per_second': 208.174, 'eval_steps_per_second': 7.29, 'epoch': 9.375, 'step': 600}, {'train_runtime': 244.3264, 'train_samples_per_second': 83.822, 'train_steps_per_second': 2.619, 'total_flos': 2687412044952960.0, 'train_loss': 0.003903126259683631, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.84\t\t 0.80\t\n Eiongelmia                \t 0.49 \t\t 0.45\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\ni:6, done\ntraining is progressing as intended\nk 7\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:03, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.010000</td>\n      <td>0.191516</td>\n      <td>0.745046</td>\n      <td>0.792141</td>\n      <td>0.703236</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.007600</td>\n      <td>0.153187</td>\n      <td>0.758692</td>\n      <td>0.832574</td>\n      <td>0.696854</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.004100</td>\n      <td>0.175343</td>\n      <td>0.777300</td>\n      <td>0.822893</td>\n      <td>0.736493</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.001200</td>\n      <td>0.199196</td>\n      <td>0.784501</td>\n      <td>0.841686</td>\n      <td>0.734592</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.000600</td>\n      <td>0.193269</td>\n      <td>0.787017</td>\n      <td>0.835421</td>\n      <td>0.743915</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000400</td>\n      <td>0.198018</td>\n      <td>0.781535</td>\n      <td>0.829157</td>\n      <td>0.739086</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.72 \t\t 0.82\t\t 0.77\t\n Eiongelmia                \t 0.50 \t\t 0.50\t\t 0.50\t\nStrictness: partial\nCheckpointU F1-Score: 0.75 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.72 \t\t 0.87\t\t 0.79\t\n Eiongelmia                \t 0.45 \t\t 0.49\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.86\t\t 0.80\t\n Eiongelmia                \t 0.56 \t\t 0.47\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.88\t\t 0.81\t\n Eiongelmia                \t 0.54 \t\t 0.43\t\t 0.48\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.88\t\t 0.81\t\n Eiongelmia                \t 0.53 \t\t 0.46\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.87\t\t 0.81\t\n Eiongelmia                \t 0.47 \t\t 0.45\t\t 0.46\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.01, 'grad_norm': 0.34767210483551025, 'learning_rate': 0.000135, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.1915162056684494, 'eval_f1_score': 0.74504552758436, 'eval_recall_score': 0.7921412300683371, 'eval_precision': 0.7032355915065723, 'eval_runtime': 1.2249, 'eval_samples_per_second': 209.807, 'eval_steps_per_second': 7.347, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0076, 'grad_norm': 0.5561515092849731, 'learning_rate': 0.00011, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.15318746864795685, 'eval_f1_score': 0.7586922677737415, 'eval_recall_score': 0.8325740318906606, 'eval_precision': 0.6968541468064824, 'eval_runtime': 1.2825, 'eval_samples_per_second': 200.387, 'eval_steps_per_second': 7.017, 'epoch': 3.125, 'step': 200}, {'loss': 0.0041, 'grad_norm': 0.06597922742366791, 'learning_rate': 8.5e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.17534299194812775, 'eval_f1_score': 0.7772996234534696, 'eval_recall_score': 0.8228929384965832, 'eval_precision': 0.736493374108053, 'eval_runtime': 1.2395, 'eval_samples_per_second': 207.341, 'eval_steps_per_second': 7.261, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0012, 'grad_norm': 0.20652586221694946, 'learning_rate': 6.000000000000001e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.19919608533382416, 'eval_f1_score': 0.7845010615711253, 'eval_recall_score': 0.8416856492027335, 'eval_precision': 0.7345924453280318, 'eval_runtime': 1.2273, 'eval_samples_per_second': 209.406, 'eval_steps_per_second': 7.333, 'epoch': 6.25, 'step': 400}, {'loss': 0.0006, 'grad_norm': 0.006180952303111553, 'learning_rate': 3.5000000000000004e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.19326943159103394, 'eval_f1_score': 0.7870171673819742, 'eval_recall_score': 0.8354214123006833, 'eval_precision': 0.7439148073022313, 'eval_runtime': 1.2267, 'eval_samples_per_second': 209.507, 'eval_steps_per_second': 7.337, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0004, 'grad_norm': 0.05813618376851082, 'learning_rate': 1e-05, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.19801786541938782, 'eval_f1_score': 0.7815351583467526, 'eval_recall_score': 0.8291571753986332, 'eval_precision': 0.7390862944162436, 'eval_runtime': 1.2175, 'eval_samples_per_second': 211.091, 'eval_steps_per_second': 7.392, 'epoch': 9.375, 'step': 600}, {'train_runtime': 243.3037, 'train_samples_per_second': 84.175, 'train_steps_per_second': 2.63, 'total_flos': 2687412044952960.0, 'train_loss': 0.0037586342863505707, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.88\t\t 0.81\t\n Eiongelmia                \t 0.53 \t\t 0.46\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\ni:7, done\ntraining is progressing as intended\nk 8\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:04, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.008800</td>\n      <td>0.167820</td>\n      <td>0.759577</td>\n      <td>0.858200</td>\n      <td>0.681284</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.007400</td>\n      <td>0.180117</td>\n      <td>0.750391</td>\n      <td>0.820046</td>\n      <td>0.691643</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.002700</td>\n      <td>0.187974</td>\n      <td>0.777959</td>\n      <td>0.816059</td>\n      <td>0.743257</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.001300</td>\n      <td>0.191764</td>\n      <td>0.781553</td>\n      <td>0.825171</td>\n      <td>0.742316</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.000600</td>\n      <td>0.204188</td>\n      <td>0.791101</td>\n      <td>0.830296</td>\n      <td>0.755440</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000600</td>\n      <td>0.198705</td>\n      <td>0.789587</td>\n      <td>0.837699</td>\n      <td>0.746701</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.70 \t\t 0.89\t\t 0.79\t\n Eiongelmia                \t 0.47 \t\t 0.52\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.73 \t\t 0.86\t\t 0.79\t\n Eiongelmia                \t 0.36 \t\t 0.48\t\t 0.41\t\nStrictness: partial\nCheckpointU F1-Score: 0.75 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.78 \t\t 0.85\t\t 0.82\t\n Eiongelmia                \t 0.39 \t\t 0.46\t\t 0.42\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.86\t\t 0.81\t\n Eiongelmia                \t 0.45 \t\t 0.46\t\t 0.46\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.87\t\t 0.82\t\n Eiongelmia                \t 0.53 \t\t 0.45\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.88\t\t 0.82\t\n Eiongelmia                \t 0.53 \t\t 0.48\t\t 0.50\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0088, 'grad_norm': 0.24977600574493408, 'learning_rate': 0.0001434375, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.16782048344612122, 'eval_f1_score': 0.7595766129032258, 'eval_recall_score': 0.8582004555808656, 'eval_precision': 0.6812839059674503, 'eval_runtime': 1.2323, 'eval_samples_per_second': 208.549, 'eval_steps_per_second': 7.303, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0074, 'grad_norm': 0.3067910671234131, 'learning_rate': 0.00011687500000000001, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.18011656403541565, 'eval_f1_score': 0.7503908285565398, 'eval_recall_score': 0.8200455580865603, 'eval_precision': 0.69164265129683, 'eval_runtime': 1.2215, 'eval_samples_per_second': 210.402, 'eval_steps_per_second': 7.368, 'epoch': 3.125, 'step': 200}, {'loss': 0.0027, 'grad_norm': 0.35401540994644165, 'learning_rate': 9.03125e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.18797436356544495, 'eval_f1_score': 0.7779587404994571, 'eval_recall_score': 0.8160592255125285, 'eval_precision': 0.7432572614107884, 'eval_runtime': 1.2435, 'eval_samples_per_second': 206.67, 'eval_steps_per_second': 7.237, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0013, 'grad_norm': 0.019000699743628502, 'learning_rate': 6.375e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.19176365435123444, 'eval_f1_score': 0.7815533980582524, 'eval_recall_score': 0.8251708428246014, 'eval_precision': 0.7423155737704918, 'eval_runtime': 1.2532, 'eval_samples_per_second': 205.077, 'eval_steps_per_second': 7.182, 'epoch': 6.25, 'step': 400}, {'loss': 0.0006, 'grad_norm': 0.010363627225160599, 'learning_rate': 3.71875e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.2041882425546646, 'eval_f1_score': 0.791101465002713, 'eval_recall_score': 0.8302961275626424, 'eval_precision': 0.755440414507772, 'eval_runtime': 1.2272, 'eval_samples_per_second': 209.424, 'eval_steps_per_second': 7.334, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0006, 'grad_norm': 0.24673306941986084, 'learning_rate': 1.0625e-05, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.1987050473690033, 'eval_f1_score': 0.7895866881374128, 'eval_recall_score': 0.8376993166287016, 'eval_precision': 0.7467005076142132, 'eval_runtime': 1.2318, 'eval_samples_per_second': 208.638, 'eval_steps_per_second': 7.306, 'epoch': 9.375, 'step': 600}, {'train_runtime': 244.7893, 'train_samples_per_second': 83.664, 'train_steps_per_second': 2.614, 'total_flos': 2687412044952960.0, 'train_loss': 0.0033698537648888306, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.87\t\t 0.82\t\n Eiongelmia                \t 0.53 \t\t 0.45\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\ni:8, done\ntraining is progressing as intended\nk 9\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:03, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.007300</td>\n      <td>0.225971</td>\n      <td>0.766316</td>\n      <td>0.829157</td>\n      <td>0.712329</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.009200</td>\n      <td>0.187173</td>\n      <td>0.756111</td>\n      <td>0.810364</td>\n      <td>0.708665</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.003100</td>\n      <td>0.184896</td>\n      <td>0.798061</td>\n      <td>0.843964</td>\n      <td>0.756895</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.001800</td>\n      <td>0.170831</td>\n      <td>0.778256</td>\n      <td>0.823462</td>\n      <td>0.737755</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.000800</td>\n      <td>0.198575</td>\n      <td>0.792250</td>\n      <td>0.838269</td>\n      <td>0.751020</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000600</td>\n      <td>0.176549</td>\n      <td>0.800760</td>\n      <td>0.839977</td>\n      <td>0.765041</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.72 \t\t 0.87\t\t 0.78\t\n Eiongelmia                \t 0.63 \t\t 0.48\t\t 0.55\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.73 \t\t 0.84\t\t 0.79\t\n Eiongelmia                \t 0.45 \t\t 0.49\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.88\t\t 0.82\t\n Eiongelmia                \t 0.55 \t\t 0.49\t\t 0.52\t\nStrictness: partial\nCheckpointU F1-Score: 0.80 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.86\t\t 0.80\t\n Eiongelmia                \t 0.51 \t\t 0.52\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.78 \t\t 0.87\t\t 0.82\t\n Eiongelmia                \t 0.50 \t\t 0.52\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.79 \t\t 0.87\t\t 0.83\t\n Eiongelmia                \t 0.52 \t\t 0.53\t\t 0.52\t\nStrictness: partial\nCheckpointU F1-Score: 0.80 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0073, 'grad_norm': 0.8188667893409729, 'learning_rate': 0.00015187500000000002, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.22597147524356842, 'eval_f1_score': 0.7663157894736843, 'eval_recall_score': 0.8291571753986332, 'eval_precision': 0.7123287671232876, 'eval_runtime': 1.2197, 'eval_samples_per_second': 210.71, 'eval_steps_per_second': 7.379, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0092, 'grad_norm': 0.25518569350242615, 'learning_rate': 0.00012375, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.18717294931411743, 'eval_f1_score': 0.7561105207226355, 'eval_recall_score': 0.8103644646924829, 'eval_precision': 0.7086653386454184, 'eval_runtime': 1.2312, 'eval_samples_per_second': 208.747, 'eval_steps_per_second': 7.31, 'epoch': 3.125, 'step': 200}, {'loss': 0.0031, 'grad_norm': 0.3028273284435272, 'learning_rate': 9.562500000000001e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.18489564955234528, 'eval_f1_score': 0.7980613893376414, 'eval_recall_score': 0.8439635535307517, 'eval_precision': 0.7568947906026557, 'eval_runtime': 1.2252, 'eval_samples_per_second': 209.758, 'eval_steps_per_second': 7.346, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0018, 'grad_norm': 0.10448214411735535, 'learning_rate': 6.75e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.17083057761192322, 'eval_f1_score': 0.7782561894510226, 'eval_recall_score': 0.8234624145785877, 'eval_precision': 0.7377551020408163, 'eval_runtime': 1.2425, 'eval_samples_per_second': 206.846, 'eval_steps_per_second': 7.244, 'epoch': 6.25, 'step': 400}, {'loss': 0.0008, 'grad_norm': 0.03161607310175896, 'learning_rate': 3.9375e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.1985751986503601, 'eval_f1_score': 0.7922497308934339, 'eval_recall_score': 0.8382687927107062, 'eval_precision': 0.7510204081632653, 'eval_runtime': 1.2298, 'eval_samples_per_second': 208.984, 'eval_steps_per_second': 7.319, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0006, 'grad_norm': 0.004547903314232826, 'learning_rate': 1.125e-05, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.17654861509799957, 'eval_f1_score': 0.8007600434310531, 'eval_recall_score': 0.8399772209567198, 'eval_precision': 0.7650414937759336, 'eval_runtime': 1.2115, 'eval_samples_per_second': 212.13, 'eval_steps_per_second': 7.429, 'epoch': 9.375, 'step': 600}, {'train_runtime': 243.6245, 'train_samples_per_second': 84.064, 'train_steps_per_second': 2.627, 'total_flos': 2687412044952960.0, 'train_loss': 0.0035828300096909516, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:01]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.78 \t\t 0.87\t\t 0.82\t\n Eiongelmia                \t 0.50 \t\t 0.52\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\ni:9, done\ntraining is progressing as intended\nk 10\nhistorical_settings {'f1_score': 0.7922497308934339, 'eval_f1_score': 0.7922497308934339, 'target': 'learning_rate', 'parameters': {'evaluation_strategy': 'steps', 'learning_rate': 0.00019, 'load_best_model_at_end': True, 'logging_steps': 100, 'num_train_epochs': 10, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 16, 'save_strategy': 'steps', 'seed': 42, 'weight_decay': 0.19998}, 'eval_results': {'eval_loss': 0.1985751986503601, 'eval_f1_score': 0.7922497308934339, 'eval_recall_score': 0.8382687927107062, 'eval_precision': 0.7510204081632653, 'eval_runtime': 1.4279, 'eval_samples_per_second': 179.991, 'eval_steps_per_second': 6.303, 'epoch': 10.0, 'experiment': 'Incontinence_NER_v5_20231208_orig_par_opt', 'run_id': 'cbe7f211', 'uuid': '59820cdf-8e40-4560-bb84-38e116d5268a', 'today': '2024_07_18', 'timestamp': 1721316106.43603, 'learning_rate': 0.00018, 'weight_decay': 0.19998}, 'strictness': 'partial', 'k': 9}\nk 0\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:05, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.007600</td>\n      <td>0.173277</td>\n      <td>0.765231</td>\n      <td>0.829727</td>\n      <td>0.710039</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.007600</td>\n      <td>0.160953</td>\n      <td>0.731970</td>\n      <td>0.774487</td>\n      <td>0.693878</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.003600</td>\n      <td>0.212465</td>\n      <td>0.774384</td>\n      <td>0.822893</td>\n      <td>0.731275</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.001700</td>\n      <td>0.203232</td>\n      <td>0.779670</td>\n      <td>0.834282</td>\n      <td>0.731768</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.000300</td>\n      <td>0.219091</td>\n      <td>0.778075</td>\n      <td>0.828588</td>\n      <td>0.733367</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000300</td>\n      <td>0.212347</td>\n      <td>0.775661</td>\n      <td>0.834852</td>\n      <td>0.724308</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.86\t\t 0.81\t\n Eiongelmia                \t 0.34 \t\t 0.52\t\t 0.41\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.81\t\t 0.79\t\n Eiongelmia                \t 0.27 \t\t 0.48\t\t 0.35\t\nStrictness: partial\nCheckpointU F1-Score: 0.73 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.86\t\t 0.80\t\n Eiongelmia                \t 0.54 \t\t 0.45\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.87\t\t 0.81\t\n Eiongelmia                \t 0.50 \t\t 0.48\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.87\t\t 0.81\t\n Eiongelmia                \t 0.47 \t\t 0.43\t\t 0.45\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.87\t\t 0.80\t\n Eiongelmia                \t 0.48 \t\t 0.46\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0076, 'grad_norm': 0.20995965600013733, 'learning_rate': 0.00015187500000000002, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.1732773780822754, 'eval_f1_score': 0.7652310924369748, 'eval_recall_score': 0.8297266514806378, 'eval_precision': 0.7100389863547758, 'eval_runtime': 1.2687, 'eval_samples_per_second': 202.576, 'eval_steps_per_second': 7.094, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0076, 'grad_norm': 0.3149130642414093, 'learning_rate': 0.00012375, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.160952627658844, 'eval_f1_score': 0.7319698600645855, 'eval_recall_score': 0.7744874715261959, 'eval_precision': 0.6938775510204082, 'eval_runtime': 1.22, 'eval_samples_per_second': 210.659, 'eval_steps_per_second': 7.377, 'epoch': 3.125, 'step': 200}, {'loss': 0.0036, 'grad_norm': 0.1903030127286911, 'learning_rate': 9.562500000000001e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.2124650925397873, 'eval_f1_score': 0.7743837084673096, 'eval_recall_score': 0.8228929384965832, 'eval_precision': 0.7312753036437247, 'eval_runtime': 1.2516, 'eval_samples_per_second': 205.342, 'eval_steps_per_second': 7.191, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0017, 'grad_norm': 0.19498543441295624, 'learning_rate': 6.75e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.20323196053504944, 'eval_f1_score': 0.7796700372538585, 'eval_recall_score': 0.8342824601366743, 'eval_precision': 0.7317682317682318, 'eval_runtime': 1.2348, 'eval_samples_per_second': 208.135, 'eval_steps_per_second': 7.289, 'epoch': 6.25, 'step': 400}, {'loss': 0.0003, 'grad_norm': 0.06039905548095703, 'learning_rate': 3.9375e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.2190905511379242, 'eval_f1_score': 0.7780748663101603, 'eval_recall_score': 0.8285876993166287, 'eval_precision': 0.733366935483871, 'eval_runtime': 1.231, 'eval_samples_per_second': 208.769, 'eval_steps_per_second': 7.311, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0003, 'grad_norm': 0.03509192913770676, 'learning_rate': 1.125e-05, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.21234703063964844, 'eval_f1_score': 0.7756613756613756, 'eval_recall_score': 0.8348519362186788, 'eval_precision': 0.724308300395257, 'eval_runtime': 1.2411, 'eval_samples_per_second': 207.074, 'eval_steps_per_second': 7.252, 'epoch': 9.375, 'step': 600}, {'train_runtime': 245.9476, 'train_samples_per_second': 83.27, 'train_steps_per_second': 2.602, 'total_flos': 2687412044952960.0, 'train_loss': 0.0033381745641236195, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:01]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.87\t\t 0.81\t\n Eiongelmia                \t 0.47 \t\t 0.43\t\t 0.45\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\ni:0, done\ntraining is progressing as intended\nk 1\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:03, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.006700</td>\n      <td>0.208669</td>\n      <td>0.745269</td>\n      <td>0.829727</td>\n      <td>0.676416</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.005900</td>\n      <td>0.214595</td>\n      <td>0.766794</td>\n      <td>0.799544</td>\n      <td>0.736621</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.003100</td>\n      <td>0.190571</td>\n      <td>0.778794</td>\n      <td>0.824032</td>\n      <td>0.738265</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.001600</td>\n      <td>0.212492</td>\n      <td>0.782888</td>\n      <td>0.833713</td>\n      <td>0.737903</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.000600</td>\n      <td>0.219248</td>\n      <td>0.775107</td>\n      <td>0.826310</td>\n      <td>0.729879</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000400</td>\n      <td>0.215464</td>\n      <td>0.778424</td>\n      <td>0.838269</td>\n      <td>0.726555</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.68 \t\t 0.87\t\t 0.76\t\n Eiongelmia                \t 0.63 \t\t 0.49\t\t 0.55\t\nStrictness: partial\nCheckpointU F1-Score: 0.75 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.83\t\t 0.79\t\n Eiongelmia                \t 0.55 \t\t 0.49\t\t 0.52\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.86\t\t 0.81\t\n Eiongelmia                \t 0.48 \t\t 0.51\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.87\t\t 0.81\t\n Eiongelmia                \t 0.58 \t\t 0.45\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.74 \t\t 0.86\t\t 0.80\t\n Eiongelmia                \t 0.55 \t\t 0.47\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.74 \t\t 0.88\t\t 0.80\t\n Eiongelmia                \t 0.54 \t\t 0.45\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0067, 'grad_norm': 0.13287732005119324, 'learning_rate': 0.00015187500000000002, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.2086687535047531, 'eval_f1_score': 0.7452685421994885, 'eval_recall_score': 0.8297266514806378, 'eval_precision': 0.6764159702878366, 'eval_runtime': 1.2303, 'eval_samples_per_second': 208.897, 'eval_steps_per_second': 7.315, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0059, 'grad_norm': 0.06731844693422318, 'learning_rate': 0.00012375, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.21459521353244781, 'eval_f1_score': 0.7667941015838339, 'eval_recall_score': 0.7995444191343963, 'eval_precision': 0.7366211962224554, 'eval_runtime': 1.2227, 'eval_samples_per_second': 210.191, 'eval_steps_per_second': 7.361, 'epoch': 3.125, 'step': 200}, {'loss': 0.0031, 'grad_norm': 0.37678077816963196, 'learning_rate': 9.562500000000001e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.19057074189186096, 'eval_f1_score': 0.778794402583423, 'eval_recall_score': 0.8240318906605922, 'eval_precision': 0.738265306122449, 'eval_runtime': 1.2245, 'eval_samples_per_second': 209.885, 'eval_steps_per_second': 7.35, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0016, 'grad_norm': 0.007782734930515289, 'learning_rate': 6.75e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.2124917060136795, 'eval_f1_score': 0.7828877005347594, 'eval_recall_score': 0.8337129840546698, 'eval_precision': 0.7379032258064516, 'eval_runtime': 1.2988, 'eval_samples_per_second': 197.868, 'eval_steps_per_second': 6.929, 'epoch': 6.25, 'step': 400}, {'loss': 0.0006, 'grad_norm': 0.008382146246731281, 'learning_rate': 3.9375e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.21924760937690735, 'eval_f1_score': 0.7751068376068377, 'eval_recall_score': 0.8263097949886105, 'eval_precision': 0.7298792756539235, 'eval_runtime': 1.2344, 'eval_samples_per_second': 208.204, 'eval_steps_per_second': 7.291, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0004, 'grad_norm': 0.009975806809961796, 'learning_rate': 1.125e-05, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.2154635190963745, 'eval_f1_score': 0.7784241142252776, 'eval_recall_score': 0.8382687927107062, 'eval_precision': 0.7265547877591313, 'eval_runtime': 1.239, 'eval_samples_per_second': 207.432, 'eval_steps_per_second': 7.264, 'epoch': 9.375, 'step': 600}, {'train_runtime': 243.5062, 'train_samples_per_second': 84.105, 'train_steps_per_second': 2.628, 'total_flos': 2687412044952960.0, 'train_loss': 0.002862895849102642, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.74 \t\t 0.86\t\t 0.80\t\n Eiongelmia                \t 0.55 \t\t 0.47\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\ni:1, done\nlen(history): 2\npositive_training is True and prev_f1_score > current_f1_score\nk 2\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:02, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.005900</td>\n      <td>0.177976</td>\n      <td>0.777137</td>\n      <td>0.843964</td>\n      <td>0.720117</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.004000</td>\n      <td>0.173371</td>\n      <td>0.775323</td>\n      <td>0.819476</td>\n      <td>0.735685</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.003400</td>\n      <td>0.184026</td>\n      <td>0.782656</td>\n      <td>0.812073</td>\n      <td>0.755297</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.002000</td>\n      <td>0.192427</td>\n      <td>0.785564</td>\n      <td>0.849089</td>\n      <td>0.730882</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.000700</td>\n      <td>0.208347</td>\n      <td>0.789809</td>\n      <td>0.847380</td>\n      <td>0.739563</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000300</td>\n      <td>0.197575</td>\n      <td>0.795109</td>\n      <td>0.833144</td>\n      <td>0.760395</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.88\t\t 0.81\t\n Eiongelmia                \t 0.46 \t\t 0.51\t\t 0.48\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.86\t\t 0.81\t\n Eiongelmia                \t 0.48 \t\t 0.42\t\t 0.45\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.79 \t\t 0.85\t\t 0.82\t\n Eiongelmia                \t 0.40 \t\t 0.43\t\t 0.41\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.74 \t\t 0.89\t\t 0.81\t\n Eiongelmia                \t 0.62 \t\t 0.48\t\t 0.54\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.89\t\t 0.82\t\n Eiongelmia                \t 0.53 \t\t 0.46\t\t 0.50\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.78 \t\t 0.88\t\t 0.82\t\n Eiongelmia                \t 0.53 \t\t 0.42\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.80 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0059, 'grad_norm': 0.16679929196834564, 'learning_rate': 0.00015187500000000002, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.17797572910785675, 'eval_f1_score': 0.7771368641845829, 'eval_recall_score': 0.8439635535307517, 'eval_precision': 0.7201166180758017, 'eval_runtime': 1.2192, 'eval_samples_per_second': 210.787, 'eval_steps_per_second': 7.382, 'epoch': 1.5625, 'step': 100}, {'loss': 0.004, 'grad_norm': 0.21720558404922485, 'learning_rate': 0.00012375, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.17337080836296082, 'eval_f1_score': 0.775323275862069, 'eval_recall_score': 0.8194760820045558, 'eval_precision': 0.7356850715746421, 'eval_runtime': 1.221, 'eval_samples_per_second': 210.484, 'eval_steps_per_second': 7.371, 'epoch': 3.125, 'step': 200}, {'loss': 0.0034, 'grad_norm': 0.3832034170627594, 'learning_rate': 9.562500000000001e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.18402589857578278, 'eval_f1_score': 0.782656421514819, 'eval_recall_score': 0.8120728929384966, 'eval_precision': 0.7552966101694916, 'eval_runtime': 1.2285, 'eval_samples_per_second': 209.19, 'eval_steps_per_second': 7.326, 'epoch': 4.6875, 'step': 300}, {'loss': 0.002, 'grad_norm': 0.21602675318717957, 'learning_rate': 6.75e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.19242672622203827, 'eval_f1_score': 0.7855637513171759, 'eval_recall_score': 0.8490888382687927, 'eval_precision': 0.7308823529411764, 'eval_runtime': 1.2185, 'eval_samples_per_second': 210.914, 'eval_steps_per_second': 7.386, 'epoch': 6.25, 'step': 400}, {'loss': 0.0007, 'grad_norm': 0.0004937996272929013, 'learning_rate': 3.9375e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.20834718644618988, 'eval_f1_score': 0.7898089171974522, 'eval_recall_score': 0.8473804100227791, 'eval_precision': 0.7395626242544732, 'eval_runtime': 1.2241, 'eval_samples_per_second': 209.942, 'eval_steps_per_second': 7.352, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0003, 'grad_norm': 0.00992810633033514, 'learning_rate': 1.125e-05, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.19757454097270966, 'eval_f1_score': 0.7951086956521738, 'eval_recall_score': 0.8331435079726651, 'eval_precision': 0.7603950103950103, 'eval_runtime': 1.2357, 'eval_samples_per_second': 207.987, 'eval_steps_per_second': 7.284, 'epoch': 9.375, 'step': 600}, {'train_runtime': 243.2583, 'train_samples_per_second': 84.19, 'train_steps_per_second': 2.631, 'total_flos': 2687412044952960.0, 'train_loss': 0.002565423597116023, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:01]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.89\t\t 0.82\t\n Eiongelmia                \t 0.53 \t\t 0.46\t\t 0.50\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\ni:2, done\ntraining is progressing as intended\nk 3\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:04, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.005500</td>\n      <td>0.177142</td>\n      <td>0.780065</td>\n      <td>0.820046</td>\n      <td>0.743802</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.005700</td>\n      <td>0.177256</td>\n      <td>0.784239</td>\n      <td>0.821754</td>\n      <td>0.750000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.003800</td>\n      <td>0.175459</td>\n      <td>0.796909</td>\n      <td>0.822323</td>\n      <td>0.773019</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.001700</td>\n      <td>0.177062</td>\n      <td>0.770509</td>\n      <td>0.845103</td>\n      <td>0.708015</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.000900</td>\n      <td>0.192461</td>\n      <td>0.790210</td>\n      <td>0.836560</td>\n      <td>0.748726</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000600</td>\n      <td>0.177620</td>\n      <td>0.792453</td>\n      <td>0.837130</td>\n      <td>0.752303</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.86\t\t 0.81\t\n Eiongelmia                \t 0.51 \t\t 0.45\t\t 0.48\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.86\t\t 0.81\t\n Eiongelmia                \t 0.56 \t\t 0.43\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.78 \t\t 0.86\t\t 0.82\t\n Eiongelmia                \t 0.62 \t\t 0.49\t\t 0.55\t\nStrictness: partial\nCheckpointU F1-Score: 0.80 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.72 \t\t 0.88\t\t 0.79\t\n Eiongelmia                \t 0.54 \t\t 0.50\t\t 0.52\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.88\t\t 0.81\t\n Eiongelmia                \t 0.59 \t\t 0.47\t\t 0.52\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.88\t\t 0.82\t\n Eiongelmia                \t 0.55 \t\t 0.46\t\t 0.50\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0055, 'grad_norm': 0.39367759227752686, 'learning_rate': 0.00015187500000000002, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.17714175581932068, 'eval_f1_score': 0.7800650054171181, 'eval_recall_score': 0.8200455580865603, 'eval_precision': 0.743801652892562, 'eval_runtime': 1.245, 'eval_samples_per_second': 206.432, 'eval_steps_per_second': 7.229, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0057, 'grad_norm': 0.0074446010403335094, 'learning_rate': 0.00012375, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.17725560069084167, 'eval_f1_score': 0.7842391304347825, 'eval_recall_score': 0.821753986332574, 'eval_precision': 0.75, 'eval_runtime': 1.467, 'eval_samples_per_second': 175.187, 'eval_steps_per_second': 6.135, 'epoch': 3.125, 'step': 200}, {'loss': 0.0038, 'grad_norm': 0.0875597894191742, 'learning_rate': 9.562500000000001e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.17545853555202484, 'eval_f1_score': 0.7969094922737305, 'eval_recall_score': 0.8223234624145785, 'eval_precision': 0.7730192719486081, 'eval_runtime': 1.2317, 'eval_samples_per_second': 208.652, 'eval_steps_per_second': 7.307, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0017, 'grad_norm': 0.030561672523617744, 'learning_rate': 6.75e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.1770620346069336, 'eval_f1_score': 0.770508826583593, 'eval_recall_score': 0.8451025056947609, 'eval_precision': 0.7080152671755725, 'eval_runtime': 1.2277, 'eval_samples_per_second': 209.343, 'eval_steps_per_second': 7.331, 'epoch': 6.25, 'step': 400}, {'loss': 0.0009, 'grad_norm': 0.052173834294080734, 'learning_rate': 3.9375e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.19246119260787964, 'eval_f1_score': 0.7902097902097902, 'eval_recall_score': 0.8365603644646925, 'eval_precision': 0.7487257900101937, 'eval_runtime': 1.217, 'eval_samples_per_second': 211.177, 'eval_steps_per_second': 7.395, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0006, 'grad_norm': 0.06525629013776779, 'learning_rate': 1.125e-05, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.17761975526809692, 'eval_f1_score': 0.7924528301886792, 'eval_recall_score': 0.837129840546697, 'eval_precision': 0.7523029682702149, 'eval_runtime': 1.2446, 'eval_samples_per_second': 206.499, 'eval_steps_per_second': 7.231, 'epoch': 9.375, 'step': 600}, {'train_runtime': 244.5187, 'train_samples_per_second': 83.756, 'train_steps_per_second': 2.617, 'total_flos': 2687412044952960.0, 'train_loss': 0.002851884177653119, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.88\t\t 0.81\t\n Eiongelmia                \t 0.59 \t\t 0.47\t\t 0.52\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\ni:3, done\ntraining is progressing as intended\nk 4\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:04, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.004800</td>\n      <td>0.204123</td>\n      <td>0.757873</td>\n      <td>0.835991</td>\n      <td>0.693107</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.006200</td>\n      <td>0.213199</td>\n      <td>0.759240</td>\n      <td>0.818907</td>\n      <td>0.707677</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.003500</td>\n      <td>0.190287</td>\n      <td>0.782420</td>\n      <td>0.821185</td>\n      <td>0.747150</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.000900</td>\n      <td>0.190266</td>\n      <td>0.778671</td>\n      <td>0.827449</td>\n      <td>0.735324</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.000500</td>\n      <td>0.202494</td>\n      <td>0.788357</td>\n      <td>0.825171</td>\n      <td>0.754687</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000200</td>\n      <td>0.204249</td>\n      <td>0.789163</td>\n      <td>0.837699</td>\n      <td>0.745943</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.70 \t\t 0.88\t\t 0.78\t\n Eiongelmia                \t 0.53 \t\t 0.43\t\t 0.48\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.72 \t\t 0.86\t\t 0.78\t\n Eiongelmia                \t 0.52 \t\t 0.45\t\t 0.48\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.86\t\t 0.81\t\n Eiongelmia                \t 0.57 \t\t 0.45\t\t 0.50\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.86\t\t 0.81\t\n Eiongelmia                \t 0.48 \t\t 0.48\t\t 0.48\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.86\t\t 0.81\t\n Eiongelmia                \t 0.60 \t\t 0.48\t\t 0.53\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.87\t\t 0.81\t\n Eiongelmia                \t 0.59 \t\t 0.53\t\t 0.56\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0048, 'grad_norm': 0.4517931044101715, 'learning_rate': 0.00015187500000000002, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.20412279665470123, 'eval_f1_score': 0.7578729994837378, 'eval_recall_score': 0.835990888382688, 'eval_precision': 0.6931067044381491, 'eval_runtime': 1.2132, 'eval_samples_per_second': 211.843, 'eval_steps_per_second': 7.419, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0062, 'grad_norm': 0.4862651526927948, 'learning_rate': 0.00012375, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.2131994515657425, 'eval_f1_score': 0.7592397043294614, 'eval_recall_score': 0.8189066059225513, 'eval_precision': 0.7076771653543307, 'eval_runtime': 1.2145, 'eval_samples_per_second': 211.611, 'eval_steps_per_second': 7.411, 'epoch': 3.125, 'step': 200}, {'loss': 0.0035, 'grad_norm': 0.1897336095571518, 'learning_rate': 9.562500000000001e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.19028730690479279, 'eval_f1_score': 0.7824199674443841, 'eval_recall_score': 0.8211845102505695, 'eval_precision': 0.7471502590673575, 'eval_runtime': 1.2144, 'eval_samples_per_second': 211.631, 'eval_steps_per_second': 7.411, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0009, 'grad_norm': 0.0014211218804121017, 'learning_rate': 6.75e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.19026587903499603, 'eval_f1_score': 0.7786709539121114, 'eval_recall_score': 0.8274487471526196, 'eval_precision': 0.7353238866396761, 'eval_runtime': 1.2131, 'eval_samples_per_second': 211.848, 'eval_steps_per_second': 7.419, 'epoch': 6.25, 'step': 400}, {'loss': 0.0005, 'grad_norm': 0.53138267993927, 'learning_rate': 3.9375e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.2024943232536316, 'eval_f1_score': 0.7883569096844396, 'eval_recall_score': 0.8251708428246014, 'eval_precision': 0.7546875, 'eval_runtime': 1.2185, 'eval_samples_per_second': 210.922, 'eval_steps_per_second': 7.386, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0002, 'grad_norm': 0.009283741004765034, 'learning_rate': 1.125e-05, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.20424877107143402, 'eval_f1_score': 0.7891630901287554, 'eval_recall_score': 0.8376993166287016, 'eval_precision': 0.7459432048681541, 'eval_runtime': 1.218, 'eval_samples_per_second': 211.008, 'eval_steps_per_second': 7.389, 'epoch': 9.375, 'step': 600}, {'train_runtime': 245.0942, 'train_samples_per_second': 83.56, 'train_steps_per_second': 2.611, 'total_flos': 2687412044952960.0, 'train_loss': 0.002540990407578647, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.86\t\t 0.81\t\n Eiongelmia                \t 0.60 \t\t 0.48\t\t 0.53\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\ni:4, done\nlen(history): 5\npositive_training is True and prev_f1_score > current_f1_score\nk 5\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:03, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.005500</td>\n      <td>0.192294</td>\n      <td>0.745614</td>\n      <td>0.822893</td>\n      <td>0.681604</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.006400</td>\n      <td>0.184319</td>\n      <td>0.765306</td>\n      <td>0.811503</td>\n      <td>0.724085</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.002400</td>\n      <td>0.173507</td>\n      <td>0.794831</td>\n      <td>0.858200</td>\n      <td>0.740177</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.001500</td>\n      <td>0.198884</td>\n      <td>0.785146</td>\n      <td>0.842825</td>\n      <td>0.734856</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.000700</td>\n      <td>0.205557</td>\n      <td>0.805145</td>\n      <td>0.837699</td>\n      <td>0.775026</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000600</td>\n      <td>0.186487</td>\n      <td>0.789930</td>\n      <td>0.830866</td>\n      <td>0.752838</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.71 \t\t 0.86\t\t 0.78\t\n Eiongelmia                \t 0.41 \t\t 0.46\t\t 0.44\t\nStrictness: partial\nCheckpointU F1-Score: 0.75 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.85\t\t 0.80\t\n Eiongelmia                \t 0.40 \t\t 0.45\t\t 0.42\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.89\t\t 0.82\t\n Eiongelmia                \t 0.58 \t\t 0.54\t\t 0.56\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.88\t\t 0.81\t\n Eiongelmia                \t 0.54 \t\t 0.45\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.78 \t\t 0.88\t\t 0.83\t\n Eiongelmia                \t 0.66 \t\t 0.46\t\t 0.54\t\nStrictness: partial\nCheckpointU F1-Score: 0.81 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.87\t\t 0.82\t\n Eiongelmia                \t 0.51 \t\t 0.43\t\t 0.46\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0055, 'grad_norm': 0.21672964096069336, 'learning_rate': 0.00015187500000000002, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.19229434430599213, 'eval_f1_score': 0.7456140350877193, 'eval_recall_score': 0.8228929384965832, 'eval_precision': 0.6816037735849056, 'eval_runtime': 1.2074, 'eval_samples_per_second': 212.852, 'eval_steps_per_second': 7.454, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0064, 'grad_norm': 0.295572966337204, 'learning_rate': 0.00012375, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.18431870639324188, 'eval_f1_score': 0.7653061224489797, 'eval_recall_score': 0.8115034168564921, 'eval_precision': 0.7240853658536586, 'eval_runtime': 1.2158, 'eval_samples_per_second': 211.389, 'eval_steps_per_second': 7.403, 'epoch': 3.125, 'step': 200}, {'loss': 0.0024, 'grad_norm': 0.243552103638649, 'learning_rate': 9.562500000000001e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.1735074520111084, 'eval_f1_score': 0.794831223628692, 'eval_recall_score': 0.8582004555808656, 'eval_precision': 0.7401768172888016, 'eval_runtime': 1.2289, 'eval_samples_per_second': 209.127, 'eval_steps_per_second': 7.324, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0015, 'grad_norm': 0.0011416006600484252, 'learning_rate': 6.75e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.19888418912887573, 'eval_f1_score': 0.7851458885941646, 'eval_recall_score': 0.8428246013667426, 'eval_precision': 0.7348560079443893, 'eval_runtime': 1.2372, 'eval_samples_per_second': 207.735, 'eval_steps_per_second': 7.275, 'epoch': 6.25, 'step': 400}, {'loss': 0.0007, 'grad_norm': 0.05231534689664841, 'learning_rate': 3.9375e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.20555704832077026, 'eval_f1_score': 0.8051450465243567, 'eval_recall_score': 0.8376993166287016, 'eval_precision': 0.7750263435194942, 'eval_runtime': 1.2279, 'eval_samples_per_second': 209.293, 'eval_steps_per_second': 7.329, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0006, 'grad_norm': 0.013840533792972565, 'learning_rate': 1.125e-05, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.1864871084690094, 'eval_f1_score': 0.7899296155928532, 'eval_recall_score': 0.8308656036446469, 'eval_precision': 0.7528379772961816, 'eval_runtime': 1.2288, 'eval_samples_per_second': 209.143, 'eval_steps_per_second': 7.324, 'epoch': 9.375, 'step': 600}, {'train_runtime': 243.8635, 'train_samples_per_second': 83.981, 'train_steps_per_second': 2.624, 'total_flos': 2687412044952960.0, 'train_loss': 0.0026788987903273664, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:01]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.78 \t\t 0.88\t\t 0.83\t\n Eiongelmia                \t 0.66 \t\t 0.46\t\t 0.54\t\nStrictness: partial\nCheckpointU F1-Score: 0.81 \n\n\ni:5, done\ntraining is progressing as intended\nk 6\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:03, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.004200</td>\n      <td>0.203713</td>\n      <td>0.772319</td>\n      <td>0.832574</td>\n      <td>0.720197</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.006000</td>\n      <td>0.196171</td>\n      <td>0.759915</td>\n      <td>0.818337</td>\n      <td>0.709279</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.003500</td>\n      <td>0.195644</td>\n      <td>0.775412</td>\n      <td>0.829727</td>\n      <td>0.727772</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.003000</td>\n      <td>0.208351</td>\n      <td>0.782422</td>\n      <td>0.831435</td>\n      <td>0.738866</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.001000</td>\n      <td>0.208706</td>\n      <td>0.778361</td>\n      <td>0.843964</td>\n      <td>0.722222</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000600</td>\n      <td>0.210983</td>\n      <td>0.787460</td>\n      <td>0.843964</td>\n      <td>0.738048</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.72 \t\t 0.87\t\t 0.79\t\n Eiongelmia                \t 0.72 \t\t 0.50\t\t 0.59\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.74 \t\t 0.85\t\t 0.80\t\n Eiongelmia                \t 0.39 \t\t 0.48\t\t 0.43\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.74 \t\t 0.86\t\t 0.80\t\n Eiongelmia                \t 0.62 \t\t 0.50\t\t 0.55\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.87\t\t 0.81\t\n Eiongelmia                \t 0.56 \t\t 0.49\t\t 0.53\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.74 \t\t 0.88\t\t 0.81\t\n Eiongelmia                \t 0.50 \t\t 0.51\t\t 0.50\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.88\t\t 0.81\t\n Eiongelmia                \t 0.55 \t\t 0.51\t\t 0.53\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0042, 'grad_norm': 0.03295000270009041, 'learning_rate': 0.00015187500000000002, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.20371316373348236, 'eval_f1_score': 0.7723190702588484, 'eval_recall_score': 0.8325740318906606, 'eval_precision': 0.7201970443349753, 'eval_runtime': 1.2398, 'eval_samples_per_second': 207.289, 'eval_steps_per_second': 7.259, 'epoch': 1.5625, 'step': 100}, {'loss': 0.006, 'grad_norm': 0.15252114832401276, 'learning_rate': 0.00012375, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.19617055356502533, 'eval_f1_score': 0.7599153886832364, 'eval_recall_score': 0.8183371298405467, 'eval_precision': 0.709279368213228, 'eval_runtime': 1.2212, 'eval_samples_per_second': 210.457, 'eval_steps_per_second': 7.37, 'epoch': 3.125, 'step': 200}, {'loss': 0.0035, 'grad_norm': 0.01813359186053276, 'learning_rate': 9.562500000000001e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.19564425945281982, 'eval_f1_score': 0.7754124534326771, 'eval_recall_score': 0.8297266514806378, 'eval_precision': 0.7277722277722277, 'eval_runtime': 1.2093, 'eval_samples_per_second': 212.526, 'eval_steps_per_second': 7.443, 'epoch': 4.6875, 'step': 300}, {'loss': 0.003, 'grad_norm': 0.0030403980053961277, 'learning_rate': 6.75e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.20835119485855103, 'eval_f1_score': 0.782422293676313, 'eval_recall_score': 0.8314350797266514, 'eval_precision': 0.7388663967611336, 'eval_runtime': 1.2222, 'eval_samples_per_second': 210.281, 'eval_steps_per_second': 7.364, 'epoch': 6.25, 'step': 400}, {'loss': 0.001, 'grad_norm': 0.037228140980005264, 'learning_rate': 3.9375e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.20870593190193176, 'eval_f1_score': 0.7783613445378151, 'eval_recall_score': 0.8439635535307517, 'eval_precision': 0.7222222222222222, 'eval_runtime': 1.2164, 'eval_samples_per_second': 211.271, 'eval_steps_per_second': 7.399, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0006, 'grad_norm': 0.0368729829788208, 'learning_rate': 1.125e-05, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.21098335087299347, 'eval_f1_score': 0.7874601487778958, 'eval_recall_score': 0.8439635535307517, 'eval_precision': 0.7380478087649402, 'eval_runtime': 1.2237, 'eval_samples_per_second': 210.026, 'eval_steps_per_second': 7.355, 'epoch': 9.375, 'step': 600}, {'train_runtime': 243.525, 'train_samples_per_second': 84.098, 'train_steps_per_second': 2.628, 'total_flos': 2687412044952960.0, 'train_loss': 0.002874186099506915, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.74 \t\t 0.88\t\t 0.81\t\n Eiongelmia                \t 0.50 \t\t 0.51\t\t 0.50\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\ni:6, done\nlen(history): 7\npositive_training is True and prev_f1_score > current_f1_score\nnegative direction weight_decay {'evaluation_strategy': 'steps', 'learning_rate': 0.00018, 'load_best_model_at_end': True, 'logging_steps': 100, 'num_train_epochs': 10, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 16, 'save_strategy': 'steps', 'seed': 42, 'weight_decay': 0.19998}\nk 7\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:03, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.006600</td>\n      <td>0.162482</td>\n      <td>0.779020</td>\n      <td>0.833144</td>\n      <td>0.731500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.002600</td>\n      <td>0.184518</td>\n      <td>0.757210</td>\n      <td>0.822323</td>\n      <td>0.701652</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.002100</td>\n      <td>0.178634</td>\n      <td>0.781370</td>\n      <td>0.812073</td>\n      <td>0.752904</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.001100</td>\n      <td>0.185336</td>\n      <td>0.776786</td>\n      <td>0.842255</td>\n      <td>0.720760</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.000500</td>\n      <td>0.199008</td>\n      <td>0.784293</td>\n      <td>0.830296</td>\n      <td>0.743119</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000300</td>\n      <td>0.189356</td>\n      <td>0.794595</td>\n      <td>0.837130</td>\n      <td>0.756173</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.87\t\t 0.81\t\n Eiongelmia                \t 0.47 \t\t 0.51\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.73 \t\t 0.86\t\t 0.79\t\n Eiongelmia                \t 0.42 \t\t 0.47\t\t 0.44\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.78 \t\t 0.85\t\t 0.81\t\n Eiongelmia                \t 0.47 \t\t 0.42\t\t 0.44\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.73 \t\t 0.88\t\t 0.80\t\n Eiongelmia                \t 0.54 \t\t 0.48\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.87\t\t 0.81\t\n Eiongelmia                \t 0.55 \t\t 0.45\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.88\t\t 0.82\t\n Eiongelmia                \t 0.54 \t\t 0.46\t\t 0.50\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0066, 'grad_norm': 0.12895594537258148, 'learning_rate': 0.00015187500000000002, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.16248169541358948, 'eval_f1_score': 0.7790202342917999, 'eval_recall_score': 0.8331435079726651, 'eval_precision': 0.7315, 'eval_runtime': 1.2409, 'eval_samples_per_second': 207.115, 'eval_steps_per_second': 7.253, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0026, 'grad_norm': 0.05208127945661545, 'learning_rate': 0.00012375, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.18451818823814392, 'eval_f1_score': 0.7572102779234399, 'eval_recall_score': 0.8223234624145785, 'eval_precision': 0.7016520894071915, 'eval_runtime': 1.2286, 'eval_samples_per_second': 209.18, 'eval_steps_per_second': 7.325, 'epoch': 3.125, 'step': 200}, {'loss': 0.0021, 'grad_norm': 0.1035333052277565, 'learning_rate': 9.562500000000001e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.17863363027572632, 'eval_f1_score': 0.7813698630136985, 'eval_recall_score': 0.8120728929384966, 'eval_precision': 0.7529039070749736, 'eval_runtime': 1.2233, 'eval_samples_per_second': 210.086, 'eval_steps_per_second': 7.357, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0011, 'grad_norm': 0.11182865500450134, 'learning_rate': 6.75e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.18533571064472198, 'eval_f1_score': 0.7767857142857143, 'eval_recall_score': 0.842255125284738, 'eval_precision': 0.7207602339181286, 'eval_runtime': 1.2334, 'eval_samples_per_second': 208.361, 'eval_steps_per_second': 7.297, 'epoch': 6.25, 'step': 400}, {'loss': 0.0005, 'grad_norm': 0.05314218997955322, 'learning_rate': 3.9375e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.19900763034820557, 'eval_f1_score': 0.7842926304464766, 'eval_recall_score': 0.8302961275626424, 'eval_precision': 0.7431192660550459, 'eval_runtime': 1.2388, 'eval_samples_per_second': 207.461, 'eval_steps_per_second': 7.265, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0003, 'grad_norm': 0.01181696355342865, 'learning_rate': 1.125e-05, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.1893557757139206, 'eval_f1_score': 0.7945945945945946, 'eval_recall_score': 0.837129840546697, 'eval_precision': 0.7561728395061729, 'eval_runtime': 1.2422, 'eval_samples_per_second': 206.895, 'eval_steps_per_second': 7.245, 'epoch': 9.375, 'step': 600}, {'train_runtime': 243.8073, 'train_samples_per_second': 84.001, 'train_steps_per_second': 2.625, 'total_flos': 2687412044952960.0, 'train_loss': 0.0020532386493869125, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.87\t\t 0.81\t\n Eiongelmia                \t 0.55 \t\t 0.45\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\ni:0, done\ntraining is progressing as intended\nnegative direction weight_decay {'evaluation_strategy': 'steps', 'learning_rate': 0.00018, 'load_best_model_at_end': True, 'logging_steps': 100, 'num_train_epochs': 10, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 16, 'save_strategy': 'steps', 'seed': 42, 'weight_decay': 0.19996999999999998}\nk 8\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:07, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.004600</td>\n      <td>0.191489</td>\n      <td>0.763021</td>\n      <td>0.834282</td>\n      <td>0.702975</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.007900</td>\n      <td>0.164552</td>\n      <td>0.778127</td>\n      <td>0.846811</td>\n      <td>0.719748</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.003200</td>\n      <td>0.185169</td>\n      <td>0.775607</td>\n      <td>0.836560</td>\n      <td>0.722933</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.002900</td>\n      <td>0.192298</td>\n      <td>0.789198</td>\n      <td>0.857062</td>\n      <td>0.731293</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.001200</td>\n      <td>0.188994</td>\n      <td>0.796480</td>\n      <td>0.824601</td>\n      <td>0.770213</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000700</td>\n      <td>0.187889</td>\n      <td>0.801851</td>\n      <td>0.838838</td>\n      <td>0.767987</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.72 \t\t 0.87\t\t 0.79\t\n Eiongelmia                \t 0.48 \t\t 0.54\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.72 \t\t 0.88\t\t 0.79\t\n Eiongelmia                \t 0.65 \t\t 0.53\t\t 0.59\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.74 \t\t 0.87\t\t 0.80\t\n Eiongelmia                \t 0.54 \t\t 0.49\t\t 0.52\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.89\t\t 0.82\t\n Eiongelmia                \t 0.51 \t\t 0.52\t\t 0.52\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.78 \t\t 0.86\t\t 0.82\t\n Eiongelmia                \t 0.58 \t\t 0.45\t\t 0.50\t\nStrictness: partial\nCheckpointU F1-Score: 0.80 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.78 \t\t 0.88\t\t 0.83\t\n Eiongelmia                \t 0.57 \t\t 0.42\t\t 0.49\t\nStrictness: partial\nCheckpointU F1-Score: 0.80 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0046, 'grad_norm': 0.36062052845954895, 'learning_rate': 0.00015187500000000002, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.19148948788642883, 'eval_f1_score': 0.7630208333333334, 'eval_recall_score': 0.8342824601366743, 'eval_precision': 0.7029750479846449, 'eval_runtime': 1.2226, 'eval_samples_per_second': 210.205, 'eval_steps_per_second': 7.361, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0079, 'grad_norm': 0.19763369858264923, 'learning_rate': 0.00012375, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.16455243527889252, 'eval_f1_score': 0.7781266352694924, 'eval_recall_score': 0.8468109339407744, 'eval_precision': 0.7197483059051307, 'eval_runtime': 1.218, 'eval_samples_per_second': 210.996, 'eval_steps_per_second': 7.389, 'epoch': 3.125, 'step': 200}, {'loss': 0.0032, 'grad_norm': 0.04284103587269783, 'learning_rate': 9.562500000000001e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.1851692497730255, 'eval_f1_score': 0.7756071805702217, 'eval_recall_score': 0.8365603644646925, 'eval_precision': 0.7229330708661418, 'eval_runtime': 1.2184, 'eval_samples_per_second': 210.939, 'eval_steps_per_second': 7.387, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0029, 'grad_norm': 0.034387070685625076, 'learning_rate': 6.75e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.19229772686958313, 'eval_f1_score': 0.7891976927110645, 'eval_recall_score': 0.8570615034168565, 'eval_precision': 0.7312925170068028, 'eval_runtime': 1.2326, 'eval_samples_per_second': 208.496, 'eval_steps_per_second': 7.301, 'epoch': 6.25, 'step': 400}, {'loss': 0.0012, 'grad_norm': 0.16786161065101624, 'learning_rate': 3.9375e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.18899407982826233, 'eval_f1_score': 0.7964796479647964, 'eval_recall_score': 0.8246013667425968, 'eval_precision': 0.7702127659574468, 'eval_runtime': 1.2119, 'eval_samples_per_second': 212.071, 'eval_steps_per_second': 7.427, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0007, 'grad_norm': 0.028033273294568062, 'learning_rate': 1.125e-05, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.18788868188858032, 'eval_f1_score': 0.8018508437670114, 'eval_recall_score': 0.8388382687927107, 'eval_precision': 0.7679874869655892, 'eval_runtime': 1.248, 'eval_samples_per_second': 205.934, 'eval_steps_per_second': 7.212, 'epoch': 9.375, 'step': 600}, {'train_runtime': 247.9458, 'train_samples_per_second': 82.599, 'train_steps_per_second': 2.581, 'total_flos': 2687412044952960.0, 'train_loss': 0.003254754564841278, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.78 \t\t 0.86\t\t 0.82\t\n Eiongelmia                \t 0.58 \t\t 0.45\t\t 0.50\t\nStrictness: partial\nCheckpointU F1-Score: 0.80 \n\n\ni:1, done\ntraining is progressing as intended\nk 9\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:02, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.004200</td>\n      <td>0.210490</td>\n      <td>0.773828</td>\n      <td>0.855353</td>\n      <td>0.706491</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.004200</td>\n      <td>0.193626</td>\n      <td>0.794720</td>\n      <td>0.839977</td>\n      <td>0.754090</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.002600</td>\n      <td>0.174347</td>\n      <td>0.753751</td>\n      <td>0.829727</td>\n      <td>0.690521</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.001900</td>\n      <td>0.179903</td>\n      <td>0.784737</td>\n      <td>0.849089</td>\n      <td>0.729452</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.000700</td>\n      <td>0.192549</td>\n      <td>0.784030</td>\n      <td>0.833144</td>\n      <td>0.740385</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000400</td>\n      <td>0.200231</td>\n      <td>0.784048</td>\n      <td>0.822893</td>\n      <td>0.748705</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.71 \t\t 0.89\t\t 0.79\t\n Eiongelmia                \t 0.65 \t\t 0.48\t\t 0.55\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.88\t\t 0.82\t\n Eiongelmia                \t 0.68 \t\t 0.43\t\t 0.53\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.86\t\t 0.81\t\n Eiongelmia                \t 0.29 \t\t 0.51\t\t 0.37\t\nStrictness: partial\nCheckpointU F1-Score: 0.75 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.74 \t\t 0.88\t\t 0.81\t\n Eiongelmia                \t 0.58 \t\t 0.54\t\t 0.56\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.86\t\t 0.81\t\n Eiongelmia                \t 0.53 \t\t 0.54\t\t 0.54\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.86\t\t 0.81\t\n Eiongelmia                \t 0.52 \t\t 0.51\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0042, 'grad_norm': 0.505865752696991, 'learning_rate': 0.00015187500000000002, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.21049048006534576, 'eval_f1_score': 0.773827923750644, 'eval_recall_score': 0.8553530751708428, 'eval_precision': 0.7064910630291628, 'eval_runtime': 1.2198, 'eval_samples_per_second': 210.685, 'eval_steps_per_second': 7.378, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0042, 'grad_norm': 0.09569644927978516, 'learning_rate': 0.00012375, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.19362635910511017, 'eval_f1_score': 0.7947198275862069, 'eval_recall_score': 0.8399772209567198, 'eval_precision': 0.7540899795501023, 'eval_runtime': 1.2237, 'eval_samples_per_second': 210.021, 'eval_steps_per_second': 7.355, 'epoch': 3.125, 'step': 200}, {'loss': 0.0026, 'grad_norm': 0.09980767965316772, 'learning_rate': 9.562500000000001e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.1743471622467041, 'eval_f1_score': 0.7537506466632179, 'eval_recall_score': 0.8297266514806378, 'eval_precision': 0.690521327014218, 'eval_runtime': 1.229, 'eval_samples_per_second': 209.121, 'eval_steps_per_second': 7.323, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0019, 'grad_norm': 0.05549447610974312, 'learning_rate': 6.75e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.179902583360672, 'eval_f1_score': 0.7847368421052631, 'eval_recall_score': 0.8490888382687927, 'eval_precision': 0.7294520547945206, 'eval_runtime': 1.2162, 'eval_samples_per_second': 211.321, 'eval_steps_per_second': 7.4, 'epoch': 6.25, 'step': 400}, {'loss': 0.0007, 'grad_norm': 0.0063791777938604355, 'learning_rate': 3.9375e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.19254939258098602, 'eval_f1_score': 0.7840300107181136, 'eval_recall_score': 0.8331435079726651, 'eval_precision': 0.7403846153846154, 'eval_runtime': 1.2214, 'eval_samples_per_second': 210.41, 'eval_steps_per_second': 7.368, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0004, 'grad_norm': 1.2052170038223267, 'learning_rate': 1.125e-05, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.20023107528686523, 'eval_f1_score': 0.7840477482365709, 'eval_recall_score': 0.8228929384965832, 'eval_precision': 0.7487046632124352, 'eval_runtime': 1.2117, 'eval_samples_per_second': 212.107, 'eval_steps_per_second': 7.428, 'epoch': 9.375, 'step': 600}, {'train_runtime': 243.0722, 'train_samples_per_second': 84.255, 'train_steps_per_second': 2.633, 'total_flos': 2687412044952960.0, 'train_loss': 0.002199114765971899, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:01]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.86\t\t 0.81\t\n Eiongelmia                \t 0.53 \t\t 0.54\t\t 0.54\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\ni:2, done\nlen(history): 10\npositive_training is True and prev_f1_score > current_f1_score\nk 10\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:03, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.005800</td>\n      <td>0.189509</td>\n      <td>0.771310</td>\n      <td>0.845103</td>\n      <td>0.709369</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.007000</td>\n      <td>0.195879</td>\n      <td>0.784699</td>\n      <td>0.817768</td>\n      <td>0.754202</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.003000</td>\n      <td>0.196160</td>\n      <td>0.794338</td>\n      <td>0.846811</td>\n      <td>0.747988</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.002700</td>\n      <td>0.184662</td>\n      <td>0.787037</td>\n      <td>0.822893</td>\n      <td>0.754175</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.001200</td>\n      <td>0.194591</td>\n      <td>0.787121</td>\n      <td>0.842255</td>\n      <td>0.738761</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.001100</td>\n      <td>0.180584</td>\n      <td>0.790262</td>\n      <td>0.841116</td>\n      <td>0.745207</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.74 \t\t 0.89\t\t 0.80\t\n Eiongelmia                \t 0.42 \t\t 0.46\t\t 0.44\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.86\t\t 0.81\t\n Eiongelmia                \t 0.61 \t\t 0.44\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.88\t\t 0.82\t\n Eiongelmia                \t 0.53 \t\t 0.49\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.78 \t\t 0.86\t\t 0.82\t\n Eiongelmia                \t 0.47 \t\t 0.43\t\t 0.45\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.88\t\t 0.82\t\n Eiongelmia                \t 0.46 \t\t 0.48\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.88\t\t 0.82\t\n Eiongelmia                \t 0.50 \t\t 0.47\t\t 0.48\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0058, 'grad_norm': 0.327741801738739, 'learning_rate': 0.00015187500000000002, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.18950919806957245, 'eval_f1_score': 0.7713097713097713, 'eval_recall_score': 0.8451025056947609, 'eval_precision': 0.7093690248565966, 'eval_runtime': 1.231, 'eval_samples_per_second': 208.77, 'eval_steps_per_second': 7.311, 'epoch': 1.5625, 'step': 100}, {'loss': 0.007, 'grad_norm': 0.10146234184503555, 'learning_rate': 0.00012375, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.1958785057067871, 'eval_f1_score': 0.7846994535519126, 'eval_recall_score': 0.8177676537585421, 'eval_precision': 0.7542016806722689, 'eval_runtime': 1.2137, 'eval_samples_per_second': 211.749, 'eval_steps_per_second': 7.415, 'epoch': 3.125, 'step': 200}, {'loss': 0.003, 'grad_norm': 0.2398531436920166, 'learning_rate': 9.562500000000001e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.19616007804870605, 'eval_f1_score': 0.7943376068376069, 'eval_recall_score': 0.8468109339407744, 'eval_precision': 0.7479879275653923, 'eval_runtime': 1.2089, 'eval_samples_per_second': 212.599, 'eval_steps_per_second': 7.445, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0027, 'grad_norm': 0.24407127499580383, 'learning_rate': 6.75e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.18466168642044067, 'eval_f1_score': 0.7870370370370371, 'eval_recall_score': 0.8228929384965832, 'eval_precision': 0.7541753653444676, 'eval_runtime': 1.344, 'eval_samples_per_second': 191.227, 'eval_steps_per_second': 6.697, 'epoch': 6.25, 'step': 400}, {'loss': 0.0012, 'grad_norm': 0.001396610983647406, 'learning_rate': 3.9375e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.19459080696105957, 'eval_f1_score': 0.787120808940926, 'eval_recall_score': 0.842255125284738, 'eval_precision': 0.7387612387612388, 'eval_runtime': 1.2304, 'eval_samples_per_second': 208.872, 'eval_steps_per_second': 7.315, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0011, 'grad_norm': 0.056693803519010544, 'learning_rate': 1.125e-05, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.1805838942527771, 'eval_f1_score': 0.7902621722846441, 'eval_recall_score': 0.841116173120729, 'eval_precision': 0.7452068617558022, 'eval_runtime': 1.2323, 'eval_samples_per_second': 208.556, 'eval_steps_per_second': 7.304, 'epoch': 9.375, 'step': 600}, {'train_runtime': 244.0556, 'train_samples_per_second': 83.915, 'train_steps_per_second': 2.622, 'total_flos': 2687412044952960.0, 'train_loss': 0.003275381083949469, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.88\t\t 0.82\t\n Eiongelmia                \t 0.46 \t\t 0.48\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\ni:3, done\ntraining is progressing as intended\nk 11\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:04, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.005100</td>\n      <td>0.244541</td>\n      <td>0.750763</td>\n      <td>0.840547</td>\n      <td>0.678309</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.004200</td>\n      <td>0.167055</td>\n      <td>0.762227</td>\n      <td>0.834282</td>\n      <td>0.701628</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.003400</td>\n      <td>0.191452</td>\n      <td>0.771812</td>\n      <td>0.851367</td>\n      <td>0.705855</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.002200</td>\n      <td>0.192643</td>\n      <td>0.777261</td>\n      <td>0.856492</td>\n      <td>0.711447</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.000700</td>\n      <td>0.200471</td>\n      <td>0.784681</td>\n      <td>0.834282</td>\n      <td>0.740647</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000700</td>\n      <td>0.183873</td>\n      <td>0.787895</td>\n      <td>0.837699</td>\n      <td>0.743680</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.69 \t\t 0.88\t\t 0.77\t\n Eiongelmia                \t 0.55 \t\t 0.48\t\t 0.52\t\nStrictness: partial\nCheckpointU F1-Score: 0.75 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.73 \t\t 0.87\t\t 0.79\t\n Eiongelmia                \t 0.42 \t\t 0.50\t\t 0.46\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.72 \t\t 0.89\t\t 0.79\t\n Eiongelmia                \t 0.58 \t\t 0.52\t\t 0.55\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.72 \t\t 0.89\t\t 0.79\t\n Eiongelmia                \t 0.64 \t\t 0.53\t\t 0.58\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.87\t\t 0.81\t\n Eiongelmia                \t 0.51 \t\t 0.46\t\t 0.48\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.88\t\t 0.82\t\n Eiongelmia                \t 0.48 \t\t 0.48\t\t 0.48\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0051, 'grad_norm': 0.1365009844303131, 'learning_rate': 0.00015187500000000002, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.24454057216644287, 'eval_f1_score': 0.7507629704984742, 'eval_recall_score': 0.8405466970387244, 'eval_precision': 0.6783088235294118, 'eval_runtime': 1.2345, 'eval_samples_per_second': 208.18, 'eval_steps_per_second': 7.29, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0042, 'grad_norm': 0.08125672489404678, 'learning_rate': 0.00012375, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.1670551300048828, 'eval_f1_score': 0.7622268470343392, 'eval_recall_score': 0.8342824601366743, 'eval_precision': 0.7016283524904214, 'eval_runtime': 1.2199, 'eval_samples_per_second': 210.678, 'eval_steps_per_second': 7.378, 'epoch': 3.125, 'step': 200}, {'loss': 0.0034, 'grad_norm': 0.8440059423446655, 'learning_rate': 9.562500000000001e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.19145244359970093, 'eval_f1_score': 0.7718120805369129, 'eval_recall_score': 0.8513667425968109, 'eval_precision': 0.7058545797922569, 'eval_runtime': 1.2241, 'eval_samples_per_second': 209.948, 'eval_steps_per_second': 7.352, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0022, 'grad_norm': 0.028618892654776573, 'learning_rate': 6.75e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.19264298677444458, 'eval_f1_score': 0.7772609819121447, 'eval_recall_score': 0.856492027334852, 'eval_precision': 0.7114474929044465, 'eval_runtime': 1.2388, 'eval_samples_per_second': 207.452, 'eval_steps_per_second': 7.265, 'epoch': 6.25, 'step': 400}, {'loss': 0.0007, 'grad_norm': 0.005136735271662474, 'learning_rate': 3.9375e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.20047149062156677, 'eval_f1_score': 0.7846813069094806, 'eval_recall_score': 0.8342824601366743, 'eval_precision': 0.7406471183013145, 'eval_runtime': 1.2224, 'eval_samples_per_second': 210.242, 'eval_steps_per_second': 7.363, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0007, 'grad_norm': 0.003729035146534443, 'learning_rate': 1.125e-05, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.18387259542942047, 'eval_f1_score': 0.7878950187466522, 'eval_recall_score': 0.8376993166287016, 'eval_precision': 0.743680485338726, 'eval_runtime': 1.2324, 'eval_samples_per_second': 208.538, 'eval_steps_per_second': 7.303, 'epoch': 9.375, 'step': 600}, {'train_runtime': 245.0476, 'train_samples_per_second': 83.576, 'train_steps_per_second': 2.612, 'total_flos': 2687412044952960.0, 'train_loss': 0.002583619233337231, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.87\t\t 0.81\t\n Eiongelmia                \t 0.51 \t\t 0.46\t\t 0.48\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\ni:4, done\nlen(history): 12\npositive_training is True and prev_f1_score > current_f1_score\nk 12\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:03, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.003400</td>\n      <td>0.191115</td>\n      <td>0.742455</td>\n      <td>0.840547</td>\n      <td>0.664865</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.007300</td>\n      <td>0.187119</td>\n      <td>0.761731</td>\n      <td>0.832005</td>\n      <td>0.702404</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.009200</td>\n      <td>0.152363</td>\n      <td>0.762848</td>\n      <td>0.811503</td>\n      <td>0.719697</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.004100</td>\n      <td>0.172337</td>\n      <td>0.774904</td>\n      <td>0.801822</td>\n      <td>0.749734</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.002400</td>\n      <td>0.163330</td>\n      <td>0.788058</td>\n      <td>0.834282</td>\n      <td>0.746687</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.001400</td>\n      <td>0.167095</td>\n      <td>0.795095</td>\n      <td>0.830866</td>\n      <td>0.762278</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.70 \t\t 0.88\t\t 0.78\t\n Eiongelmia                \t 0.35 \t\t 0.50\t\t 0.41\t\nStrictness: partial\nCheckpointU F1-Score: 0.74 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.71 \t\t 0.87\t\t 0.78\t\n Eiongelmia                \t 0.59 \t\t 0.49\t\t 0.54\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.85\t\t 0.80\t\n Eiongelmia                \t 0.43 \t\t 0.48\t\t 0.45\t\nStrictness: partial\nCheckpointU F1-Score: 0.76 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.83\t\t 0.79\t\n Eiongelmia                \t 0.63 \t\t 0.49\t\t 0.55\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.87\t\t 0.81\t\n Eiongelmia                \t 0.55 \t\t 0.47\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.78 \t\t 0.87\t\t 0.82\t\n Eiongelmia                \t 0.58 \t\t 0.45\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.80 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0034, 'grad_norm': 0.15106742084026337, 'learning_rate': 0.00015187500000000002, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.1911146342754364, 'eval_f1_score': 0.7424547283702213, 'eval_recall_score': 0.8405466970387244, 'eval_precision': 0.6648648648648648, 'eval_runtime': 1.2264, 'eval_samples_per_second': 209.558, 'eval_steps_per_second': 7.339, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0073, 'grad_norm': 0.5391027927398682, 'learning_rate': 0.00012375, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.18711921572685242, 'eval_f1_score': 0.7617309697601669, 'eval_recall_score': 0.8320045558086561, 'eval_precision': 0.7024038461538461, 'eval_runtime': 1.233, 'eval_samples_per_second': 208.439, 'eval_steps_per_second': 7.299, 'epoch': 3.125, 'step': 200}, {'loss': 0.0092, 'grad_norm': 0.17047055065631866, 'learning_rate': 9.562500000000001e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.15236276388168335, 'eval_f1_score': 0.762847965738758, 'eval_recall_score': 0.8115034168564921, 'eval_precision': 0.7196969696969697, 'eval_runtime': 1.2352, 'eval_samples_per_second': 208.061, 'eval_steps_per_second': 7.286, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0041, 'grad_norm': 0.5271560549736023, 'learning_rate': 6.75e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.1723371297121048, 'eval_f1_score': 0.774903687396808, 'eval_recall_score': 0.8018223234624146, 'eval_precision': 0.7497337593184239, 'eval_runtime': 1.2202, 'eval_samples_per_second': 210.617, 'eval_steps_per_second': 7.376, 'epoch': 6.25, 'step': 400}, {'loss': 0.0024, 'grad_norm': 0.14307072758674622, 'learning_rate': 3.9375e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.16332970559597015, 'eval_f1_score': 0.7880580957504034, 'eval_recall_score': 0.8342824601366743, 'eval_precision': 0.7466870540265036, 'eval_runtime': 1.2298, 'eval_samples_per_second': 208.976, 'eval_steps_per_second': 7.318, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0014, 'grad_norm': 0.11841494590044022, 'learning_rate': 1.125e-05, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.16709543764591217, 'eval_f1_score': 0.7950953678474113, 'eval_recall_score': 0.8308656036446469, 'eval_precision': 0.7622779519331243, 'eval_runtime': 1.223, 'eval_samples_per_second': 210.147, 'eval_steps_per_second': 7.359, 'epoch': 9.375, 'step': 600}, {'train_runtime': 243.9803, 'train_samples_per_second': 83.941, 'train_steps_per_second': 2.623, 'total_flos': 2687412044952960.0, 'train_loss': 0.004434765735641122, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.76 \t\t 0.87\t\t 0.81\t\n Eiongelmia                \t 0.55 \t\t 0.47\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\ni:5, done\ntraining is progressing as intended\nk 13\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:04, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.006200</td>\n      <td>0.209861</td>\n      <td>0.770151</td>\n      <td>0.843394</td>\n      <td>0.708612</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.005900</td>\n      <td>0.189391</td>\n      <td>0.771745</td>\n      <td>0.793280</td>\n      <td>0.751348</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.005600</td>\n      <td>0.152396</td>\n      <td>0.754386</td>\n      <td>0.832574</td>\n      <td>0.689623</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.003400</td>\n      <td>0.145428</td>\n      <td>0.777492</td>\n      <td>0.861617</td>\n      <td>0.708333</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.001600</td>\n      <td>0.159448</td>\n      <td>0.801539</td>\n      <td>0.830296</td>\n      <td>0.774708</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.001800</td>\n      <td>0.159331</td>\n      <td>0.793496</td>\n      <td>0.833713</td>\n      <td>0.756980</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.72 \t\t 0.88\t\t 0.79\t\n Eiongelmia                \t 0.53 \t\t 0.48\t\t 0.50\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.80 \t\t 0.83\t\t 0.81\t\n Eiongelmia                \t 0.38 \t\t 0.47\t\t 0.42\t\nStrictness: partial\nCheckpointU F1-Score: 0.77 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.71 \t\t 0.87\t\t 0.78\t\n Eiongelmia                \t 0.47 \t\t 0.52\t\t 0.50\t\nStrictness: partial\nCheckpointU F1-Score: 0.75 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.73 \t\t 0.90\t\t 0.81\t\n Eiongelmia                \t 0.47 \t\t 0.54\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.81 \t\t 0.87\t\t 0.84\t\n Eiongelmia                \t 0.47 \t\t 0.48\t\t 0.48\t\nStrictness: partial\nCheckpointU F1-Score: 0.80 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.78 \t\t 0.87\t\t 0.82\t\n Eiongelmia                \t 0.53 \t\t 0.48\t\t 0.50\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0062, 'grad_norm': 0.3986355662345886, 'learning_rate': 0.00015187500000000002, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.20986077189445496, 'eval_f1_score': 0.7701508060322413, 'eval_recall_score': 0.8433940774487472, 'eval_precision': 0.7086124401913876, 'eval_runtime': 1.2152, 'eval_samples_per_second': 211.479, 'eval_steps_per_second': 7.406, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0059, 'grad_norm': 0.12938998639583588, 'learning_rate': 0.00012375, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.1893908977508545, 'eval_f1_score': 0.7717451523545706, 'eval_recall_score': 0.7932801822323462, 'eval_precision': 0.7513484358144552, 'eval_runtime': 1.2212, 'eval_samples_per_second': 210.447, 'eval_steps_per_second': 7.37, 'epoch': 3.125, 'step': 200}, {'loss': 0.0056, 'grad_norm': 0.25731605291366577, 'learning_rate': 9.562500000000001e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.1523958146572113, 'eval_f1_score': 0.7543859649122807, 'eval_recall_score': 0.8325740318906606, 'eval_precision': 0.689622641509434, 'eval_runtime': 1.2203, 'eval_samples_per_second': 210.604, 'eval_steps_per_second': 7.375, 'epoch': 4.6875, 'step': 300}, {'loss': 0.0034, 'grad_norm': 0.016710316762328148, 'learning_rate': 6.75e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.1454276442527771, 'eval_f1_score': 0.7774922918807812, 'eval_recall_score': 0.8616173120728929, 'eval_precision': 0.7083333333333334, 'eval_runtime': 1.2283, 'eval_samples_per_second': 209.226, 'eval_steps_per_second': 7.327, 'epoch': 6.25, 'step': 400}, {'loss': 0.0016, 'grad_norm': 0.00979296863079071, 'learning_rate': 3.9375e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.15944801270961761, 'eval_f1_score': 0.8015393073117097, 'eval_recall_score': 0.8302961275626424, 'eval_precision': 0.7747077577045696, 'eval_runtime': 1.2337, 'eval_samples_per_second': 208.312, 'eval_steps_per_second': 7.295, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0018, 'grad_norm': 0.12853799760341644, 'learning_rate': 1.125e-05, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.15933088958263397, 'eval_f1_score': 0.7934959349593497, 'eval_recall_score': 0.8337129840546698, 'eval_precision': 0.7569803516028956, 'eval_runtime': 1.2265, 'eval_samples_per_second': 209.544, 'eval_steps_per_second': 7.338, 'epoch': 9.375, 'step': 600}, {'train_runtime': 244.3566, 'train_samples_per_second': 83.812, 'train_steps_per_second': 2.619, 'total_flos': 2687412044952960.0, 'train_loss': 0.00393843375495635, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.81 \t\t 0.87\t\t 0.84\t\n Eiongelmia                \t 0.47 \t\t 0.48\t\t 0.48\t\nStrictness: partial\nCheckpointU F1-Score: 0.80 \n\n\ni:6, done\ntraining is progressing as intended\nk 14\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [640/640 04:05, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n      <th>Recall Score</th>\n      <th>Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.006400</td>\n      <td>0.181635</td>\n      <td>0.733436</td>\n      <td>0.813212</td>\n      <td>0.667914</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.008900</td>\n      <td>0.153468</td>\n      <td>0.779180</td>\n      <td>0.843964</td>\n      <td>0.723633</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.007900</td>\n      <td>0.149866</td>\n      <td>0.782086</td>\n      <td>0.815490</td>\n      <td>0.751312</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.004000</td>\n      <td>0.169250</td>\n      <td>0.776033</td>\n      <td>0.844533</td>\n      <td>0.717812</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.002500</td>\n      <td>0.168213</td>\n      <td>0.789417</td>\n      <td>0.832574</td>\n      <td>0.750513</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.001500</td>\n      <td>0.167581</td>\n      <td>0.793287</td>\n      <td>0.847950</td>\n      <td>0.745245</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.71 \t\t 0.85\t\t 0.77\t\n Eiongelmia                \t 0.34 \t\t 0.50\t\t 0.40\t\nStrictness: partial\nCheckpointU F1-Score: 0.73 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.74 \t\t 0.88\t\t 0.80\t\n Eiongelmia                \t 0.51 \t\t 0.52\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.78 \t\t 0.85\t\t 0.82\t\n Eiongelmia                \t 0.45 \t\t 0.49\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.75 \t\t 0.88\t\t 0.81\t\n Eiongelmia                \t 0.44 \t\t 0.48\t\t 0.46\t\nStrictness: partial\nCheckpointU F1-Score: 0.78 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.87\t\t 0.82\t\n Eiongelmia                \t 0.49 \t\t 0.45\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.88\t\t 0.82\t\n Eiongelmia                \t 0.49 \t\t 0.54\t\t 0.51\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\nTraining callback called!!\nstate['log_history'] [{'loss': 0.0064, 'grad_norm': 0.4110603928565979, 'learning_rate': 0.00015187500000000002, 'epoch': 1.5625, 'step': 100}, {'eval_loss': 0.181634783744812, 'eval_f1_score': 0.7334360554699537, 'eval_recall_score': 0.8132118451025057, 'eval_precision': 0.6679139382600561, 'eval_runtime': 1.2242, 'eval_samples_per_second': 209.926, 'eval_steps_per_second': 7.352, 'epoch': 1.5625, 'step': 100}, {'loss': 0.0089, 'grad_norm': 1.7694746255874634, 'learning_rate': 0.00012375, 'epoch': 3.125, 'step': 200}, {'eval_loss': 0.1534683257341385, 'eval_f1_score': 0.779179810725552, 'eval_recall_score': 0.8439635535307517, 'eval_precision': 0.7236328125, 'eval_runtime': 1.2322, 'eval_samples_per_second': 208.565, 'eval_steps_per_second': 7.304, 'epoch': 3.125, 'step': 200}, {'loss': 0.0079, 'grad_norm': 0.30680227279663086, 'learning_rate': 9.562500000000001e-05, 'epoch': 4.6875, 'step': 300}, {'eval_loss': 0.14986580610275269, 'eval_f1_score': 0.7820862916439104, 'eval_recall_score': 0.8154897494305239, 'eval_precision': 0.751311647429171, 'eval_runtime': 1.2245, 'eval_samples_per_second': 209.878, 'eval_steps_per_second': 7.35, 'epoch': 4.6875, 'step': 300}, {'loss': 0.004, 'grad_norm': 0.19581632316112518, 'learning_rate': 6.75e-05, 'epoch': 6.25, 'step': 400}, {'eval_loss': 0.1692504584789276, 'eval_f1_score': 0.7760334903192047, 'eval_recall_score': 0.8445330296127562, 'eval_precision': 0.717812197483059, 'eval_runtime': 1.2296, 'eval_samples_per_second': 209.015, 'eval_steps_per_second': 7.32, 'epoch': 6.25, 'step': 400}, {'loss': 0.0025, 'grad_norm': 0.07728473842144012, 'learning_rate': 3.9375e-05, 'epoch': 7.8125, 'step': 500}, {'eval_loss': 0.1682133674621582, 'eval_f1_score': 0.7894168466522679, 'eval_recall_score': 0.8325740318906606, 'eval_precision': 0.7505133470225873, 'eval_runtime': 1.2206, 'eval_samples_per_second': 210.552, 'eval_steps_per_second': 7.373, 'epoch': 7.8125, 'step': 500}, {'loss': 0.0015, 'grad_norm': 0.04427191987633705, 'learning_rate': 1.125e-05, 'epoch': 9.375, 'step': 600}, {'eval_loss': 0.1675814688205719, 'eval_f1_score': 0.7932871603622803, 'eval_recall_score': 0.8479498861047836, 'eval_precision': 0.7452452452452453, 'eval_runtime': 1.2283, 'eval_samples_per_second': 209.232, 'eval_steps_per_second': 7.327, 'epoch': 9.375, 'step': 600}, {'train_runtime': 245.9335, 'train_samples_per_second': 83.275, 'train_steps_per_second': 2.602, 'total_flos': 2687412044952960.0, 'train_loss': 0.004956461291294545, 'epoch': 10.0, 'step': 640}]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\t\t\t\t Precision \t Recall \t F1 score\n                           \t 0.00 \t\t 0.00\t\t 0.00\t\n Ongelmia                  \t 0.77 \t\t 0.87\t\t 0.82\t\n Eiongelmia                \t 0.49 \t\t 0.45\t\t 0.47\t\nStrictness: partial\nCheckpointU F1-Score: 0.79 \n\n\ni:7, done\nlen(history): 15\npositive_training is False and prev_f1_score > current_f1_score\nhistorical_settings {'f1_score': 0.8051450465243567, 'eval_f1_score': 0.8051450465243567, 'target': 'weight_decay', 'parameters': {'evaluation_strategy': 'steps', 'learning_rate': 0.00018, 'load_best_model_at_end': True, 'logging_steps': 100, 'num_train_epochs': 10, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 16, 'save_strategy': 'steps', 'seed': 42, 'weight_decay': 0.20004000000000005}, 'eval_results': {'eval_loss': 0.20555704832077026, 'eval_f1_score': 0.8051450465243567, 'eval_recall_score': 0.8376993166287016, 'eval_precision': 0.7750263435194942, 'eval_runtime': 1.4259, 'eval_samples_per_second': 180.234, 'eval_steps_per_second': 6.312, 'epoch': 10.0, 'experiment': 'Incontinence_NER_v5_20231208_orig_par_opt', 'run_id': 'cbe7f211', 'uuid': '2ebfb94c-0889-47f7-b909-8ab704b9ce71', 'today': '2024_07_18', 'timestamp': 1721317609.528402, 'learning_rate': 0.00018, 'weight_decay': 0.20003000000000004}, 'strictness': 'partial', 'k': 5}\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721319871859
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "historical_settings['eval_results']"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "{'eval_loss': 0.20555704832077026,\n 'eval_f1_score': 0.8051450465243567,\n 'eval_recall_score': 0.8376993166287016,\n 'eval_precision': 0.7750263435194942,\n 'eval_runtime': 1.4259,\n 'eval_samples_per_second': 180.234,\n 'eval_steps_per_second': 6.312,\n 'epoch': 10.0,\n 'experiment': 'Incontinence_NER_v5_20231208_orig_par_opt',\n 'run_id': 'cbe7f211',\n 'uuid': '2ebfb94c-0889-47f7-b909-8ab704b9ce71',\n 'today': '2024_07_18',\n 'timestamp': 1721317609.528402,\n 'learning_rate': 0.00018,\n 'weight_decay': 0.20003000000000004}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721319872050
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721319872212
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(historical_settings['eval_results'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'epoch': 10.0,\n 'eval_f1_score': 0.8051450465243567,\n 'eval_loss': 0.20555704832077026,\n 'eval_precision': 0.7750263435194942,\n 'eval_recall_score': 0.8376993166287016,\n 'eval_runtime': 1.4259,\n 'eval_samples_per_second': 180.234,\n 'eval_steps_per_second': 6.312,\n 'experiment': 'Incontinence_NER_v5_20231208_orig_par_opt',\n 'learning_rate': 0.00018,\n 'run_id': 'cbe7f211',\n 'timestamp': 1721317609.528402,\n 'today': '2024_07_18',\n 'uuid': '2ebfb94c-0889-47f7-b909-8ab704b9ce71',\n 'weight_decay': 0.20003000000000004}\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721319872363
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "historical_settings"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "{'f1_score': 0.8051450465243567,\n 'eval_f1_score': 0.8051450465243567,\n 'target': 'weight_decay',\n 'parameters': {'evaluation_strategy': 'steps',\n  'learning_rate': 0.00018,\n  'load_best_model_at_end': True,\n  'logging_steps': 100,\n  'num_train_epochs': 10,\n  'per_device_train_batch_size': 16,\n  'per_device_eval_batch_size': 16,\n  'save_strategy': 'steps',\n  'seed': 42,\n  'weight_decay': 0.20003000000000004},\n 'eval_results': {'eval_loss': 0.20555704832077026,\n  'eval_f1_score': 0.8051450465243567,\n  'eval_recall_score': 0.8376993166287016,\n  'eval_precision': 0.7750263435194942,\n  'eval_runtime': 1.4259,\n  'eval_samples_per_second': 180.234,\n  'eval_steps_per_second': 6.312,\n  'epoch': 10.0,\n  'experiment': 'Incontinence_NER_v5_20231208_orig_par_opt',\n  'run_id': 'cbe7f211',\n  'uuid': '2ebfb94c-0889-47f7-b909-8ab704b9ce71',\n  'today': '2024_07_18',\n  'timestamp': 1721317609.528402,\n  'learning_rate': 0.00018,\n  'weight_decay': 0.20003000000000004},\n 'strictness': 'partial',\n 'k': 5}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721319872526
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "current_parameters"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": "{'evaluation_strategy': 'steps',\n 'learning_rate': 0.00018,\n 'load_best_model_at_end': True,\n 'logging_steps': 100,\n 'num_train_epochs': 10,\n 'per_device_train_batch_size': 16,\n 'per_device_eval_batch_size': 16,\n 'save_strategy': 'steps',\n 'seed': 42,\n 'weight_decay': 0.20003000000000004}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721319872696
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read .csv file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(os.path.join(model_save_folder_path, 'history.csv'), sep=';', encoding='utf-8')\n",
        "#df = pd.read_csv(\"../_trained_models/loneliness/C/2024_07_15_218b682e/history.csv\", sep=';', encoding='utf-8')"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721319872859
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_hyperparameter_table(df):\n",
        "    \"\"\"Shows hyperparameter training history as pd.DataFrame\"\"\"\n",
        "    \n",
        "    import ast\n",
        "    # both hyperparameters\n",
        "    # LR\n",
        "    lr_res = ast.literal_eval(df.loc[0, '0'])\n",
        "    # WD\n",
        "    wd_res = ast.literal_eval(df.loc[0, '1'])\n",
        "\n",
        "    def _show_hyperparameter_table(table):\n",
        "        df = pd.DataFrame(table)\n",
        "        # wide to long\n",
        "        df = df.transpose()\n",
        "        # contains columns: f1_score, target, parameters, eval_results, k\n",
        "        # parameters is a dict that contains parameter values\n",
        "        df['learning_rate'] = list(map(lambda x: x['learning_rate'], df['eval_results']))\n",
        "        df['weight_decay'] = list(map(lambda x: x['weight_decay'], df['eval_results']))\n",
        "        df['precision'] = list(map(lambda x: x['eval_precision'], df['eval_results']))\n",
        "        df['recall'] = list(map(lambda x: x['eval_recall_score'], df['eval_results']))\n",
        "        return df\n",
        "\n",
        "    df_lr = _show_hyperparameter_table(lr_res)\n",
        "    df_wd = _show_hyperparameter_table(wd_res)\n",
        "\n",
        "    # concatanate\n",
        "    df = pd.concat([df_lr, df_wd])\n",
        "    \n",
        "    # manage indeces\n",
        "    df['original_index'] = df.index\n",
        "    df.index = range(0, df.shape[0])\n",
        "    return df\n",
        "\n",
        "df = show_hyperparameter_table(df)"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721319873075
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(experiment)\n",
        "sub_df = df[['learning_rate', 'weight_decay','f1_score', 'precision', 'recall', 'k', 'target']]\n",
        "sub_df"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Incontinence_NER_v5_20231208_orig_par_opt\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 29,
          "data": {
            "text/plain": "    learning_rate  weight_decay  f1_score  precision    recall   k  \\\n0         0.00009       0.19998  0.695581   0.667539  0.726082   0   \n1         0.00010       0.19998  0.729412   0.687500  0.776765   1   \n2         0.00011       0.19998  0.763458   0.730489  0.799544   2   \n3         0.00012       0.19998  0.757867   0.712638  0.809226   3   \n4         0.00013       0.19998    0.7708   0.728934  0.817768   4   \n5         0.00014       0.19998  0.760824   0.709154  0.820615   5   \n6         0.00015       0.19998   0.76877   0.735938  0.804670   6   \n7         0.00016       0.19998  0.787017   0.743915  0.835421   7   \n8         0.00017       0.19998  0.791101   0.755440  0.830296   8   \n9         0.00018       0.19998   0.79225   0.751020  0.838269   9   \n10        0.00018       0.19998  0.778075   0.733367  0.828588   0   \n11        0.00018       0.19999  0.775107   0.729879  0.826310   1   \n12        0.00018       0.20000  0.789809   0.739563  0.847380   2   \n13        0.00018       0.20001   0.79021   0.748726  0.836560   3   \n14        0.00018       0.20002  0.788357   0.754687  0.825171   4   \n15        0.00018       0.20003  0.805145   0.775026  0.837699   5   \n16        0.00018       0.20004  0.778361   0.722222  0.843964   6   \n17        0.00018       0.19997  0.784293   0.743119  0.830296   7   \n18        0.00018       0.19996   0.79648   0.770213  0.824601   8   \n19        0.00018       0.19995   0.78403   0.740385  0.833144   9   \n20        0.00018       0.19994  0.787121   0.738761  0.842255  10   \n21        0.00018       0.19993  0.784681   0.740647  0.834282  11   \n22        0.00018       0.19992  0.788058   0.746687  0.834282  12   \n23        0.00018       0.19991  0.801539   0.774708  0.830296  13   \n24        0.00018       0.19990  0.789417   0.750513  0.832574  14   \n\n           target  \n0   learning_rate  \n1   learning_rate  \n2   learning_rate  \n3   learning_rate  \n4   learning_rate  \n5   learning_rate  \n6   learning_rate  \n7   learning_rate  \n8   learning_rate  \n9   learning_rate  \n10   weight_decay  \n11   weight_decay  \n12   weight_decay  \n13   weight_decay  \n14   weight_decay  \n15   weight_decay  \n16   weight_decay  \n17   weight_decay  \n18   weight_decay  \n19   weight_decay  \n20   weight_decay  \n21   weight_decay  \n22   weight_decay  \n23   weight_decay  \n24   weight_decay  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>learning_rate</th>\n      <th>weight_decay</th>\n      <th>f1_score</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>k</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00009</td>\n      <td>0.19998</td>\n      <td>0.695581</td>\n      <td>0.667539</td>\n      <td>0.726082</td>\n      <td>0</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00010</td>\n      <td>0.19998</td>\n      <td>0.729412</td>\n      <td>0.687500</td>\n      <td>0.776765</td>\n      <td>1</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00011</td>\n      <td>0.19998</td>\n      <td>0.763458</td>\n      <td>0.730489</td>\n      <td>0.799544</td>\n      <td>2</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00012</td>\n      <td>0.19998</td>\n      <td>0.757867</td>\n      <td>0.712638</td>\n      <td>0.809226</td>\n      <td>3</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00013</td>\n      <td>0.19998</td>\n      <td>0.7708</td>\n      <td>0.728934</td>\n      <td>0.817768</td>\n      <td>4</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.00014</td>\n      <td>0.19998</td>\n      <td>0.760824</td>\n      <td>0.709154</td>\n      <td>0.820615</td>\n      <td>5</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.00015</td>\n      <td>0.19998</td>\n      <td>0.76877</td>\n      <td>0.735938</td>\n      <td>0.804670</td>\n      <td>6</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.00016</td>\n      <td>0.19998</td>\n      <td>0.787017</td>\n      <td>0.743915</td>\n      <td>0.835421</td>\n      <td>7</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.00017</td>\n      <td>0.19998</td>\n      <td>0.791101</td>\n      <td>0.755440</td>\n      <td>0.830296</td>\n      <td>8</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.00018</td>\n      <td>0.19998</td>\n      <td>0.79225</td>\n      <td>0.751020</td>\n      <td>0.838269</td>\n      <td>9</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.00018</td>\n      <td>0.19998</td>\n      <td>0.778075</td>\n      <td>0.733367</td>\n      <td>0.828588</td>\n      <td>0</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.00018</td>\n      <td>0.19999</td>\n      <td>0.775107</td>\n      <td>0.729879</td>\n      <td>0.826310</td>\n      <td>1</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.00018</td>\n      <td>0.20000</td>\n      <td>0.789809</td>\n      <td>0.739563</td>\n      <td>0.847380</td>\n      <td>2</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.00018</td>\n      <td>0.20001</td>\n      <td>0.79021</td>\n      <td>0.748726</td>\n      <td>0.836560</td>\n      <td>3</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.00018</td>\n      <td>0.20002</td>\n      <td>0.788357</td>\n      <td>0.754687</td>\n      <td>0.825171</td>\n      <td>4</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.00018</td>\n      <td>0.20003</td>\n      <td>0.805145</td>\n      <td>0.775026</td>\n      <td>0.837699</td>\n      <td>5</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.00018</td>\n      <td>0.20004</td>\n      <td>0.778361</td>\n      <td>0.722222</td>\n      <td>0.843964</td>\n      <td>6</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.00018</td>\n      <td>0.19997</td>\n      <td>0.784293</td>\n      <td>0.743119</td>\n      <td>0.830296</td>\n      <td>7</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.00018</td>\n      <td>0.19996</td>\n      <td>0.79648</td>\n      <td>0.770213</td>\n      <td>0.824601</td>\n      <td>8</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.00018</td>\n      <td>0.19995</td>\n      <td>0.78403</td>\n      <td>0.740385</td>\n      <td>0.833144</td>\n      <td>9</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.00018</td>\n      <td>0.19994</td>\n      <td>0.787121</td>\n      <td>0.738761</td>\n      <td>0.842255</td>\n      <td>10</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.00018</td>\n      <td>0.19993</td>\n      <td>0.784681</td>\n      <td>0.740647</td>\n      <td>0.834282</td>\n      <td>11</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.00018</td>\n      <td>0.19992</td>\n      <td>0.788058</td>\n      <td>0.746687</td>\n      <td>0.834282</td>\n      <td>12</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.00018</td>\n      <td>0.19991</td>\n      <td>0.801539</td>\n      <td>0.774708</td>\n      <td>0.830296</td>\n      <td>13</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.00018</td>\n      <td>0.19990</td>\n      <td>0.789417</td>\n      <td>0.750513</td>\n      <td>0.832574</td>\n      <td>14</td>\n      <td>weight_decay</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721319873277
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_df = df.sort_values(by=['f1_score'])\n",
        "print(experiment)\n",
        "sorted_df[['learning_rate', 'weight_decay','f1_score', 'precision', 'recall', 'k', 'target']]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Incontinence_NER_v5_20231208_orig_par_opt\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 30,
          "data": {
            "text/plain": "    learning_rate  weight_decay  f1_score  precision    recall   k  \\\n0         0.00009       0.19998  0.695581   0.667539  0.726082   0   \n1         0.00010       0.19998  0.729412   0.687500  0.776765   1   \n3         0.00012       0.19998  0.757867   0.712638  0.809226   3   \n5         0.00014       0.19998  0.760824   0.709154  0.820615   5   \n2         0.00011       0.19998  0.763458   0.730489  0.799544   2   \n6         0.00015       0.19998   0.76877   0.735938  0.804670   6   \n4         0.00013       0.19998    0.7708   0.728934  0.817768   4   \n11        0.00018       0.19999  0.775107   0.729879  0.826310   1   \n10        0.00018       0.19998  0.778075   0.733367  0.828588   0   \n16        0.00018       0.20004  0.778361   0.722222  0.843964   6   \n19        0.00018       0.19995   0.78403   0.740385  0.833144   9   \n17        0.00018       0.19997  0.784293   0.743119  0.830296   7   \n21        0.00018       0.19993  0.784681   0.740647  0.834282  11   \n7         0.00016       0.19998  0.787017   0.743915  0.835421   7   \n20        0.00018       0.19994  0.787121   0.738761  0.842255  10   \n22        0.00018       0.19992  0.788058   0.746687  0.834282  12   \n14        0.00018       0.20002  0.788357   0.754687  0.825171   4   \n24        0.00018       0.19990  0.789417   0.750513  0.832574  14   \n12        0.00018       0.20000  0.789809   0.739563  0.847380   2   \n13        0.00018       0.20001   0.79021   0.748726  0.836560   3   \n8         0.00017       0.19998  0.791101   0.755440  0.830296   8   \n9         0.00018       0.19998   0.79225   0.751020  0.838269   9   \n18        0.00018       0.19996   0.79648   0.770213  0.824601   8   \n23        0.00018       0.19991  0.801539   0.774708  0.830296  13   \n15        0.00018       0.20003  0.805145   0.775026  0.837699   5   \n\n           target  \n0   learning_rate  \n1   learning_rate  \n3   learning_rate  \n5   learning_rate  \n2   learning_rate  \n6   learning_rate  \n4   learning_rate  \n11   weight_decay  \n10   weight_decay  \n16   weight_decay  \n19   weight_decay  \n17   weight_decay  \n21   weight_decay  \n7   learning_rate  \n20   weight_decay  \n22   weight_decay  \n14   weight_decay  \n24   weight_decay  \n12   weight_decay  \n13   weight_decay  \n8   learning_rate  \n9   learning_rate  \n18   weight_decay  \n23   weight_decay  \n15   weight_decay  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>learning_rate</th>\n      <th>weight_decay</th>\n      <th>f1_score</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>k</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00009</td>\n      <td>0.19998</td>\n      <td>0.695581</td>\n      <td>0.667539</td>\n      <td>0.726082</td>\n      <td>0</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00010</td>\n      <td>0.19998</td>\n      <td>0.729412</td>\n      <td>0.687500</td>\n      <td>0.776765</td>\n      <td>1</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00012</td>\n      <td>0.19998</td>\n      <td>0.757867</td>\n      <td>0.712638</td>\n      <td>0.809226</td>\n      <td>3</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.00014</td>\n      <td>0.19998</td>\n      <td>0.760824</td>\n      <td>0.709154</td>\n      <td>0.820615</td>\n      <td>5</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00011</td>\n      <td>0.19998</td>\n      <td>0.763458</td>\n      <td>0.730489</td>\n      <td>0.799544</td>\n      <td>2</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.00015</td>\n      <td>0.19998</td>\n      <td>0.76877</td>\n      <td>0.735938</td>\n      <td>0.804670</td>\n      <td>6</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00013</td>\n      <td>0.19998</td>\n      <td>0.7708</td>\n      <td>0.728934</td>\n      <td>0.817768</td>\n      <td>4</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.00018</td>\n      <td>0.19999</td>\n      <td>0.775107</td>\n      <td>0.729879</td>\n      <td>0.826310</td>\n      <td>1</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.00018</td>\n      <td>0.19998</td>\n      <td>0.778075</td>\n      <td>0.733367</td>\n      <td>0.828588</td>\n      <td>0</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.00018</td>\n      <td>0.20004</td>\n      <td>0.778361</td>\n      <td>0.722222</td>\n      <td>0.843964</td>\n      <td>6</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.00018</td>\n      <td>0.19995</td>\n      <td>0.78403</td>\n      <td>0.740385</td>\n      <td>0.833144</td>\n      <td>9</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.00018</td>\n      <td>0.19997</td>\n      <td>0.784293</td>\n      <td>0.743119</td>\n      <td>0.830296</td>\n      <td>7</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.00018</td>\n      <td>0.19993</td>\n      <td>0.784681</td>\n      <td>0.740647</td>\n      <td>0.834282</td>\n      <td>11</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.00016</td>\n      <td>0.19998</td>\n      <td>0.787017</td>\n      <td>0.743915</td>\n      <td>0.835421</td>\n      <td>7</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.00018</td>\n      <td>0.19994</td>\n      <td>0.787121</td>\n      <td>0.738761</td>\n      <td>0.842255</td>\n      <td>10</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.00018</td>\n      <td>0.19992</td>\n      <td>0.788058</td>\n      <td>0.746687</td>\n      <td>0.834282</td>\n      <td>12</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.00018</td>\n      <td>0.20002</td>\n      <td>0.788357</td>\n      <td>0.754687</td>\n      <td>0.825171</td>\n      <td>4</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.00018</td>\n      <td>0.19990</td>\n      <td>0.789417</td>\n      <td>0.750513</td>\n      <td>0.832574</td>\n      <td>14</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.00018</td>\n      <td>0.20000</td>\n      <td>0.789809</td>\n      <td>0.739563</td>\n      <td>0.847380</td>\n      <td>2</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.00018</td>\n      <td>0.20001</td>\n      <td>0.79021</td>\n      <td>0.748726</td>\n      <td>0.836560</td>\n      <td>3</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.00017</td>\n      <td>0.19998</td>\n      <td>0.791101</td>\n      <td>0.755440</td>\n      <td>0.830296</td>\n      <td>8</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.00018</td>\n      <td>0.19998</td>\n      <td>0.79225</td>\n      <td>0.751020</td>\n      <td>0.838269</td>\n      <td>9</td>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.00018</td>\n      <td>0.19996</td>\n      <td>0.79648</td>\n      <td>0.770213</td>\n      <td>0.824601</td>\n      <td>8</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.00018</td>\n      <td>0.19991</td>\n      <td>0.801539</td>\n      <td>0.774708</td>\n      <td>0.830296</td>\n      <td>13</td>\n      <td>weight_decay</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.00018</td>\n      <td>0.20003</td>\n      <td>0.805145</td>\n      <td>0.775026</td>\n      <td>0.837699</td>\n      <td>5</td>\n      <td>weight_decay</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721319873444
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_evaluation_results(df):\n",
        "    \"\"\"Function extracts the evaluation results from dataframe.\"\"\"\n",
        "    # manage indeces\n",
        "    df['original_index'] = df.index\n",
        "    df.index = range(0, df.shape[0])\n",
        "\n",
        "    # get evaluation results for the model\n",
        "    objs = {}\n",
        "    for i in range(0, df.shape[0]):\n",
        "        obj_str = df.loc[i, 'eval_results']\n",
        "        tuples = list(obj_str.items())\n",
        "        # transform the list of tuples to a dictionary\n",
        "        obj = {}\n",
        "        for t in tuples:\n",
        "            _key = t[0]\n",
        "            _value = t[1]\n",
        "            if _key == 'epoch':\n",
        "                # make epoch an int\n",
        "                _value = int(_value)\n",
        "            else: pass\n",
        "            \n",
        "            obj[_key] = _value\n",
        "        # make main dict object with integer key\n",
        "        objs[i] = obj\n",
        "\n",
        "    return pd.DataFrame(objs).transpose()\n",
        "\n",
        "df_evals = show_evaluation_results(df)\n",
        "df_evals"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/plain": "   eval_loss eval_f1_score eval_recall_score eval_precision eval_runtime  \\\n0   0.108071      0.695581          0.726082       0.667539       1.4318   \n1   0.152233      0.729412          0.776765         0.6875       1.4582   \n2   0.177246      0.763458          0.799544       0.730489       1.2491   \n3    0.19418      0.757867          0.809226       0.712638       1.4185   \n4   0.204267        0.7708          0.817768       0.728934       1.4165   \n5   0.209674      0.760824          0.820615       0.709154        1.283   \n6   0.204853       0.76877           0.80467       0.735938       1.2744   \n7   0.193269      0.787017          0.835421       0.743915       1.2675   \n8   0.204188      0.791101          0.830296        0.75544       1.2846   \n9   0.198575       0.79225          0.838269        0.75102       1.4279   \n10  0.219091      0.778075          0.828588       0.733367        1.408   \n11  0.219248      0.775107           0.82631       0.729879       1.2749   \n12  0.208347      0.789809           0.84738       0.739563       1.4234   \n13  0.192461       0.79021           0.83656       0.748726       1.2656   \n14  0.202494      0.788357          0.825171       0.754687       1.2779   \n15  0.205557      0.805145          0.837699       0.775026       1.4259   \n16  0.208706      0.778361          0.843964       0.722222       1.2465   \n17  0.199008      0.784293          0.830296       0.743119       1.2713   \n18  0.188994       0.79648          0.824601       0.770213       1.2309   \n19  0.192549       0.78403          0.833144       0.740385       1.4365   \n20  0.194591      0.787121          0.842255       0.738761       1.2727   \n21  0.200471      0.784681          0.834282       0.740647       1.2483   \n22   0.16333      0.788058          0.834282       0.746687       1.2577   \n23  0.159448      0.801539          0.830296       0.774708       1.2498   \n24  0.168213      0.789417          0.832574       0.750513       1.4232   \n\n   eval_samples_per_second eval_steps_per_second epoch  \\\n0                  179.494                 6.286    10   \n1                   176.24                 6.172    10   \n2                  205.753                 7.205    10   \n3                  181.178                 6.345    10   \n4                  181.427                 6.353    10   \n5                  200.313                 7.015    10   \n6                  201.671                 7.062    10   \n7                  202.768                 7.101    10   \n8                  200.065                 7.006    10   \n9                  179.991                 6.303    10   \n10                  182.53                 6.392    10   \n11                  201.58                 7.059    10   \n12                 180.549                 6.323    10   \n13                 203.062                 7.111    10   \n14                 201.109                 7.043    10   \n15                 180.234                 6.312    10   \n16                 206.173                  7.22    10   \n17                 202.155                 7.079    10   \n18                 208.784                 7.311    10   \n19                 178.904                 6.265    10   \n20                  201.93                 7.071    10   \n21                 205.876                  7.21    10   \n22                 204.345                 7.156    10   \n23                 205.637                 7.201    10   \n24                 180.585                 6.324    10   \n\n                                   experiment    run_id  \\\n0   Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n1   Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n2   Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n3   Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n4   Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n5   Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n6   Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n7   Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n8   Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n9   Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n10  Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n11  Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n12  Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n13  Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n14  Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n15  Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n16  Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n17  Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n18  Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n19  Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n20  Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n21  Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n22  Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n23  Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n24  Incontinence_NER_v5_20231208_orig_par_opt  cbe7f211   \n\n                                    uuid       today          timestamp  \\\n0   00dd5b50-42e0-42e4-980f-86d70dc61df7  2024_07_18  1721313853.740526   \n1   206ea6e6-8518-47cf-b913-7861893bb690  2024_07_18  1721314104.607059   \n2   393a12fb-ca50-4edd-96bb-683a4a7c4a10  2024_07_18  1721314354.825245   \n3   27a0e827-cadc-4602-8160-321eebeda90d  2024_07_18  1721314604.809916   \n4   394e9a02-78b6-413c-a403-616d31a94cd4  2024_07_18  1721314853.795578   \n5   ba145f1c-68b7-4774-b57e-1cc8e6f65262  2024_07_18   1721315104.81541   \n6   b3e8cbff-5b95-4838-bcbd-7fcb776c1327  2024_07_18  1721315355.264024   \n7   862fd055-c358-4452-ab64-a8afa96e9896  2024_07_18  1721315605.088203   \n8   01140405-d62e-4861-867d-f04effcd9e89  2024_07_18  1721315856.143144   \n9   59820cdf-8e40-4560-bb84-38e116d5268a  2024_07_18   1721316106.43603   \n10  61e6b8ae-1623-421d-87eb-7fa30fe347f4  2024_07_18  1721316358.633999   \n11  264f6518-3567-400e-848f-ae5b8c29e398  2024_07_18  1721316607.842135   \n12  fe5c29ed-e940-4752-a2c6-64573ea71bda  2024_07_18   1721316857.37038   \n13  695982f0-142d-4305-99ca-f7fd0f8ee230  2024_07_18  1721317108.109712   \n14  9db50161-9665-4169-9725-811e4b16f8f3  2024_07_18  1721317359.367088   \n15  2ebfb94c-0889-47f7-b909-8ab704b9ce71  2024_07_18  1721317609.528402   \n16  4206f24b-79b9-45a2-891a-fefeadbdd94e  2024_07_18   1721317859.50838   \n17  a054fa7b-6f37-4c81-90ef-1f3429f4b58f  2024_07_18  1721318109.104351   \n18  c2219556-8a5d-4b2d-b5e8-c52c9206488c  2024_07_18  1721318362.783945   \n19  0db94082-ed00-473e-9fe0-48b746f07e19  2024_07_18  1721318612.110691   \n20  131f6ccc-864c-4f18-aeb7-e0b13a061d7d  2024_07_18  1721318862.000815   \n21  fa2e7181-51dd-4b6f-a56d-6e484dd1015e  2024_07_18  1721319113.563996   \n22  f742159d-c96b-4c65-a41d-9e1479c27ffd  2024_07_18  1721319363.801952   \n23  a80f4487-7986-4c26-88d3-6e13f3e1fcfe  2024_07_18  1721319614.613219   \n24  3fcef11c-4c35-4cb7-a4e4-b9533c2f56c9  2024_07_18  1721319866.599191   \n\n   learning_rate weight_decay  \n0        0.00009      0.19998  \n1         0.0001      0.19998  \n2        0.00011      0.19998  \n3        0.00012      0.19998  \n4        0.00013      0.19998  \n5        0.00014      0.19998  \n6        0.00015      0.19998  \n7        0.00016      0.19998  \n8        0.00017      0.19998  \n9        0.00018      0.19998  \n10       0.00018      0.19998  \n11       0.00018      0.19999  \n12       0.00018          0.2  \n13       0.00018      0.20001  \n14       0.00018      0.20002  \n15       0.00018      0.20003  \n16       0.00018      0.20004  \n17       0.00018      0.19997  \n18       0.00018      0.19996  \n19       0.00018      0.19995  \n20       0.00018      0.19994  \n21       0.00018      0.19993  \n22       0.00018      0.19992  \n23       0.00018      0.19991  \n24       0.00018       0.1999  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eval_loss</th>\n      <th>eval_f1_score</th>\n      <th>eval_recall_score</th>\n      <th>eval_precision</th>\n      <th>eval_runtime</th>\n      <th>eval_samples_per_second</th>\n      <th>eval_steps_per_second</th>\n      <th>epoch</th>\n      <th>experiment</th>\n      <th>run_id</th>\n      <th>uuid</th>\n      <th>today</th>\n      <th>timestamp</th>\n      <th>learning_rate</th>\n      <th>weight_decay</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.108071</td>\n      <td>0.695581</td>\n      <td>0.726082</td>\n      <td>0.667539</td>\n      <td>1.4318</td>\n      <td>179.494</td>\n      <td>6.286</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>00dd5b50-42e0-42e4-980f-86d70dc61df7</td>\n      <td>2024_07_18</td>\n      <td>1721313853.740526</td>\n      <td>0.00009</td>\n      <td>0.19998</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.152233</td>\n      <td>0.729412</td>\n      <td>0.776765</td>\n      <td>0.6875</td>\n      <td>1.4582</td>\n      <td>176.24</td>\n      <td>6.172</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>206ea6e6-8518-47cf-b913-7861893bb690</td>\n      <td>2024_07_18</td>\n      <td>1721314104.607059</td>\n      <td>0.0001</td>\n      <td>0.19998</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.177246</td>\n      <td>0.763458</td>\n      <td>0.799544</td>\n      <td>0.730489</td>\n      <td>1.2491</td>\n      <td>205.753</td>\n      <td>7.205</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>393a12fb-ca50-4edd-96bb-683a4a7c4a10</td>\n      <td>2024_07_18</td>\n      <td>1721314354.825245</td>\n      <td>0.00011</td>\n      <td>0.19998</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.19418</td>\n      <td>0.757867</td>\n      <td>0.809226</td>\n      <td>0.712638</td>\n      <td>1.4185</td>\n      <td>181.178</td>\n      <td>6.345</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>27a0e827-cadc-4602-8160-321eebeda90d</td>\n      <td>2024_07_18</td>\n      <td>1721314604.809916</td>\n      <td>0.00012</td>\n      <td>0.19998</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.204267</td>\n      <td>0.7708</td>\n      <td>0.817768</td>\n      <td>0.728934</td>\n      <td>1.4165</td>\n      <td>181.427</td>\n      <td>6.353</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>394e9a02-78b6-413c-a403-616d31a94cd4</td>\n      <td>2024_07_18</td>\n      <td>1721314853.795578</td>\n      <td>0.00013</td>\n      <td>0.19998</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.209674</td>\n      <td>0.760824</td>\n      <td>0.820615</td>\n      <td>0.709154</td>\n      <td>1.283</td>\n      <td>200.313</td>\n      <td>7.015</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>ba145f1c-68b7-4774-b57e-1cc8e6f65262</td>\n      <td>2024_07_18</td>\n      <td>1721315104.81541</td>\n      <td>0.00014</td>\n      <td>0.19998</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.204853</td>\n      <td>0.76877</td>\n      <td>0.80467</td>\n      <td>0.735938</td>\n      <td>1.2744</td>\n      <td>201.671</td>\n      <td>7.062</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>b3e8cbff-5b95-4838-bcbd-7fcb776c1327</td>\n      <td>2024_07_18</td>\n      <td>1721315355.264024</td>\n      <td>0.00015</td>\n      <td>0.19998</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.193269</td>\n      <td>0.787017</td>\n      <td>0.835421</td>\n      <td>0.743915</td>\n      <td>1.2675</td>\n      <td>202.768</td>\n      <td>7.101</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>862fd055-c358-4452-ab64-a8afa96e9896</td>\n      <td>2024_07_18</td>\n      <td>1721315605.088203</td>\n      <td>0.00016</td>\n      <td>0.19998</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.204188</td>\n      <td>0.791101</td>\n      <td>0.830296</td>\n      <td>0.75544</td>\n      <td>1.2846</td>\n      <td>200.065</td>\n      <td>7.006</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>01140405-d62e-4861-867d-f04effcd9e89</td>\n      <td>2024_07_18</td>\n      <td>1721315856.143144</td>\n      <td>0.00017</td>\n      <td>0.19998</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.198575</td>\n      <td>0.79225</td>\n      <td>0.838269</td>\n      <td>0.75102</td>\n      <td>1.4279</td>\n      <td>179.991</td>\n      <td>6.303</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>59820cdf-8e40-4560-bb84-38e116d5268a</td>\n      <td>2024_07_18</td>\n      <td>1721316106.43603</td>\n      <td>0.00018</td>\n      <td>0.19998</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.219091</td>\n      <td>0.778075</td>\n      <td>0.828588</td>\n      <td>0.733367</td>\n      <td>1.408</td>\n      <td>182.53</td>\n      <td>6.392</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>61e6b8ae-1623-421d-87eb-7fa30fe347f4</td>\n      <td>2024_07_18</td>\n      <td>1721316358.633999</td>\n      <td>0.00018</td>\n      <td>0.19998</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.219248</td>\n      <td>0.775107</td>\n      <td>0.82631</td>\n      <td>0.729879</td>\n      <td>1.2749</td>\n      <td>201.58</td>\n      <td>7.059</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>264f6518-3567-400e-848f-ae5b8c29e398</td>\n      <td>2024_07_18</td>\n      <td>1721316607.842135</td>\n      <td>0.00018</td>\n      <td>0.19999</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.208347</td>\n      <td>0.789809</td>\n      <td>0.84738</td>\n      <td>0.739563</td>\n      <td>1.4234</td>\n      <td>180.549</td>\n      <td>6.323</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>fe5c29ed-e940-4752-a2c6-64573ea71bda</td>\n      <td>2024_07_18</td>\n      <td>1721316857.37038</td>\n      <td>0.00018</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.192461</td>\n      <td>0.79021</td>\n      <td>0.83656</td>\n      <td>0.748726</td>\n      <td>1.2656</td>\n      <td>203.062</td>\n      <td>7.111</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>695982f0-142d-4305-99ca-f7fd0f8ee230</td>\n      <td>2024_07_18</td>\n      <td>1721317108.109712</td>\n      <td>0.00018</td>\n      <td>0.20001</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.202494</td>\n      <td>0.788357</td>\n      <td>0.825171</td>\n      <td>0.754687</td>\n      <td>1.2779</td>\n      <td>201.109</td>\n      <td>7.043</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>9db50161-9665-4169-9725-811e4b16f8f3</td>\n      <td>2024_07_18</td>\n      <td>1721317359.367088</td>\n      <td>0.00018</td>\n      <td>0.20002</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.205557</td>\n      <td>0.805145</td>\n      <td>0.837699</td>\n      <td>0.775026</td>\n      <td>1.4259</td>\n      <td>180.234</td>\n      <td>6.312</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>2ebfb94c-0889-47f7-b909-8ab704b9ce71</td>\n      <td>2024_07_18</td>\n      <td>1721317609.528402</td>\n      <td>0.00018</td>\n      <td>0.20003</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.208706</td>\n      <td>0.778361</td>\n      <td>0.843964</td>\n      <td>0.722222</td>\n      <td>1.2465</td>\n      <td>206.173</td>\n      <td>7.22</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>4206f24b-79b9-45a2-891a-fefeadbdd94e</td>\n      <td>2024_07_18</td>\n      <td>1721317859.50838</td>\n      <td>0.00018</td>\n      <td>0.20004</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.199008</td>\n      <td>0.784293</td>\n      <td>0.830296</td>\n      <td>0.743119</td>\n      <td>1.2713</td>\n      <td>202.155</td>\n      <td>7.079</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>a054fa7b-6f37-4c81-90ef-1f3429f4b58f</td>\n      <td>2024_07_18</td>\n      <td>1721318109.104351</td>\n      <td>0.00018</td>\n      <td>0.19997</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.188994</td>\n      <td>0.79648</td>\n      <td>0.824601</td>\n      <td>0.770213</td>\n      <td>1.2309</td>\n      <td>208.784</td>\n      <td>7.311</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>c2219556-8a5d-4b2d-b5e8-c52c9206488c</td>\n      <td>2024_07_18</td>\n      <td>1721318362.783945</td>\n      <td>0.00018</td>\n      <td>0.19996</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.192549</td>\n      <td>0.78403</td>\n      <td>0.833144</td>\n      <td>0.740385</td>\n      <td>1.4365</td>\n      <td>178.904</td>\n      <td>6.265</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>0db94082-ed00-473e-9fe0-48b746f07e19</td>\n      <td>2024_07_18</td>\n      <td>1721318612.110691</td>\n      <td>0.00018</td>\n      <td>0.19995</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.194591</td>\n      <td>0.787121</td>\n      <td>0.842255</td>\n      <td>0.738761</td>\n      <td>1.2727</td>\n      <td>201.93</td>\n      <td>7.071</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>131f6ccc-864c-4f18-aeb7-e0b13a061d7d</td>\n      <td>2024_07_18</td>\n      <td>1721318862.000815</td>\n      <td>0.00018</td>\n      <td>0.19994</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.200471</td>\n      <td>0.784681</td>\n      <td>0.834282</td>\n      <td>0.740647</td>\n      <td>1.2483</td>\n      <td>205.876</td>\n      <td>7.21</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>fa2e7181-51dd-4b6f-a56d-6e484dd1015e</td>\n      <td>2024_07_18</td>\n      <td>1721319113.563996</td>\n      <td>0.00018</td>\n      <td>0.19993</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.16333</td>\n      <td>0.788058</td>\n      <td>0.834282</td>\n      <td>0.746687</td>\n      <td>1.2577</td>\n      <td>204.345</td>\n      <td>7.156</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>f742159d-c96b-4c65-a41d-9e1479c27ffd</td>\n      <td>2024_07_18</td>\n      <td>1721319363.801952</td>\n      <td>0.00018</td>\n      <td>0.19992</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.159448</td>\n      <td>0.801539</td>\n      <td>0.830296</td>\n      <td>0.774708</td>\n      <td>1.2498</td>\n      <td>205.637</td>\n      <td>7.201</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>a80f4487-7986-4c26-88d3-6e13f3e1fcfe</td>\n      <td>2024_07_18</td>\n      <td>1721319614.613219</td>\n      <td>0.00018</td>\n      <td>0.19991</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.168213</td>\n      <td>0.789417</td>\n      <td>0.832574</td>\n      <td>0.750513</td>\n      <td>1.4232</td>\n      <td>180.585</td>\n      <td>6.324</td>\n      <td>10</td>\n      <td>Incontinence_NER_v5_20231208_orig_par_opt</td>\n      <td>cbe7f211</td>\n      <td>3fcef11c-4c35-4cb7-a4e4-b9533c2f56c9</td>\n      <td>2024_07_18</td>\n      <td>1721319866.599191</td>\n      <td>0.00018</td>\n      <td>0.1999</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721319873600
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_parameters(df):\n",
        "    \"\"\"Shows the parameters columns of data frame. Remember that learning rate and \n",
        "    weight decay should not be checked using this function. Instead LR and WD are \n",
        "    correctly showed via show_evaluation_results function.\n",
        "    \"\"\"\n",
        "    # manage indeces\n",
        "    df['original_index'] = df.index\n",
        "    df.index = range(0, df.shape[0])\n",
        "\n",
        "    # get evaluation results for the model\n",
        "    objs = {}\n",
        "    for i in range(0, df.shape[0]):\n",
        "        obj_str = df.loc[i, 'parameters']\n",
        "        tuples = list(obj_str.items())\n",
        "        # transform the list of tuples to a dictionary\n",
        "        obj = {}\n",
        "        for t in tuples:\n",
        "            _key = t[0]\n",
        "            _value = t[1]\n",
        "            if _key == 'epoch':\n",
        "                # make epoch an int\n",
        "                _value = int(_value)\n",
        "            else: pass\n",
        "            \n",
        "            obj[_key] = _value\n",
        "        # make main dict object with integer key\n",
        "        objs[i] = obj\n",
        "\n",
        "    return pd.DataFrame(objs).transpose()\n",
        "\n",
        "df_param = show_parameters(df)\n",
        "df_param"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 32,
          "data": {
            "text/plain": "   evaluation_strategy learning_rate load_best_model_at_end logging_steps  \\\n0                steps       0.00018                   True           100   \n1                steps       0.00018                   True           100   \n2                steps       0.00018                   True           100   \n3                steps       0.00018                   True           100   \n4                steps       0.00018                   True           100   \n5                steps       0.00018                   True           100   \n6                steps       0.00018                   True           100   \n7                steps       0.00018                   True           100   \n8                steps       0.00018                   True           100   \n9                steps       0.00018                   True           100   \n10               steps       0.00018                   True           100   \n11               steps       0.00018                   True           100   \n12               steps       0.00018                   True           100   \n13               steps       0.00018                   True           100   \n14               steps       0.00018                   True           100   \n15               steps       0.00018                   True           100   \n16               steps       0.00018                   True           100   \n17               steps       0.00018                   True           100   \n18               steps       0.00018                   True           100   \n19               steps       0.00018                   True           100   \n20               steps       0.00018                   True           100   \n21               steps       0.00018                   True           100   \n22               steps       0.00018                   True           100   \n23               steps       0.00018                   True           100   \n24               steps       0.00018                   True           100   \n\n   num_train_epochs per_device_train_batch_size per_device_eval_batch_size  \\\n0                10                          16                         16   \n1                10                          16                         16   \n2                10                          16                         16   \n3                10                          16                         16   \n4                10                          16                         16   \n5                10                          16                         16   \n6                10                          16                         16   \n7                10                          16                         16   \n8                10                          16                         16   \n9                10                          16                         16   \n10               10                          16                         16   \n11               10                          16                         16   \n12               10                          16                         16   \n13               10                          16                         16   \n14               10                          16                         16   \n15               10                          16                         16   \n16               10                          16                         16   \n17               10                          16                         16   \n18               10                          16                         16   \n19               10                          16                         16   \n20               10                          16                         16   \n21               10                          16                         16   \n22               10                          16                         16   \n23               10                          16                         16   \n24               10                          16                         16   \n\n   save_strategy seed weight_decay  \n0          steps   42      0.20003  \n1          steps   42      0.20003  \n2          steps   42      0.20003  \n3          steps   42      0.20003  \n4          steps   42      0.20003  \n5          steps   42      0.20003  \n6          steps   42      0.20003  \n7          steps   42      0.20003  \n8          steps   42      0.20003  \n9          steps   42      0.20003  \n10         steps   42      0.20003  \n11         steps   42      0.20003  \n12         steps   42      0.20003  \n13         steps   42      0.20003  \n14         steps   42      0.20003  \n15         steps   42      0.20003  \n16         steps   42      0.20003  \n17         steps   42       0.1999  \n18         steps   42       0.1999  \n19         steps   42       0.1999  \n20         steps   42       0.1999  \n21         steps   42       0.1999  \n22         steps   42       0.1999  \n23         steps   42       0.1999  \n24         steps   42       0.1999  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>evaluation_strategy</th>\n      <th>learning_rate</th>\n      <th>load_best_model_at_end</th>\n      <th>logging_steps</th>\n      <th>num_train_epochs</th>\n      <th>per_device_train_batch_size</th>\n      <th>per_device_eval_batch_size</th>\n      <th>save_strategy</th>\n      <th>seed</th>\n      <th>weight_decay</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.20003</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.20003</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.20003</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.20003</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.20003</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.20003</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.20003</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.20003</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.20003</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.20003</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.20003</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.20003</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.20003</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.20003</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.20003</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.20003</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.20003</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.1999</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.1999</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.1999</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.1999</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.1999</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.1999</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.1999</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>steps</td>\n      <td>0.00018</td>\n      <td>True</td>\n      <td>100</td>\n      <td>10</td>\n      <td>16</td>\n      <td>16</td>\n      <td>steps</td>\n      <td>42</td>\n      <td>0.1999</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721319873765
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we're done!"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}